"""
Store Assistant RAG - Setup Script
Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡: Chunking + Embedding + Vector DB
"""

import json
import shutil
from pathlib import Path
from typing import List, Dict
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# Import configs
try:
    from config import *
except ImportError:
    from src.config import *


# ============================================
# Ø¨Ø®Ø´ 1: Chunking (Ø¨Ø¯ÙˆÙ† JSON Ù…ÛŒØ§Ù†ÛŒ)
# ============================================

class UniversalProductChunker:
    """Ú†Ø§Ù†Ú©Ø± Ø¬Ù‡Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª"""
    
    KNOWN_CATEGORIES = {
        "mobile": ["Ù…ÙˆØ¨Ø§ÛŒÙ„", "Ú¯ÙˆØ´ÛŒ", "phone"],
        "clothing": ["Ù„Ø¨Ø§Ø³", "Ù¾ÙˆØ´Ø§Ú©"],
        "electronics": ["Ù„Ù¾ØªØ§Ù¾", "ØªØ¨Ù„Øª"],
    }
    
    def chunk_products(self, products: List[Dict]) -> List[Document]:
        """ØªØ¨Ø¯ÛŒÙ„ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ LangChain Documents"""
        documents = []
        
        for product in products:
            try:
                doc = self._create_document(product)
                documents.append(doc)
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø­ØµÙˆÙ„: {e}")
                continue
        
        return documents
    
    def _create_document(self, product: Dict) -> Document:
        """Ø³Ø§Ø®Øª ÛŒÚ© Document"""
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ØµÙ„ÛŒ
        title = product.get('title', product.get('name', 'Ù…Ø­ØµÙˆÙ„'))
        brand = product.get('brand', 'Ù†Ø§Ù…Ø´Ø®Øµ')
        price = product.get('price', 0)
        category = self._detect_category(product)
        is_available = product.get('is_available', True)
        rating = product.get('rating', 0)
        
        # Ø³Ø§Ø®Øª Ù…ØªÙ† ØºÙ†ÛŒ
        text_parts = [
            f"Ø¹Ù†ÙˆØ§Ù† Ù…Ø­ØµÙˆÙ„: {title}",
            f"Ø¨Ø±Ù†Ø¯: {brand}",
            f"Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {category}",
            f"Ù‚ÛŒÙ…Øª: {price:,} Ø±ÛŒØ§Ù„",
            f"ÙˆØ¶Ø¹ÛŒØª Ù…ÙˆØ¬ÙˆØ¯ÛŒ: {'âœ… Ù…ÙˆØ¬ÙˆØ¯' if is_available else 'âŒ Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯'}",
        ]
        
        if rating > 0:
            text_parts.append(f"Ø§Ù…ØªÛŒØ§Ø²: {rating} Ø§Ø² 5")
        
        # Ø§ÙØ²ÙˆØ¯Ù† Ù…Ø´Ø®ØµØ§Øª
        specs = self._extract_specs(product)
        if specs:
            text_parts.append("\nÙ…Ø´Ø®ØµØ§Øª:")
            text_parts.extend([f"â€¢ {k}: {v}" for k, v in specs.items()])
        
        # Ø§ÙØ²ÙˆØ¯Ù† ØªÙˆØ¶ÛŒØ­Ø§Øª
        if 'description' in product:
            text_parts.append(f"\nØªÙˆØ¶ÛŒØ­Ø§Øª: {product['description'][:300]}")
        
        # Metadata Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯
        metadata = {
            "type": "product",
            "product_id": str(product.get('id', '')),
            "title": title,
            "brand": brand,
            "category": category,
            "price": price,
            "is_available": is_available,
        }
        
        return Document(
            page_content="\n".join(text_parts),
            metadata=metadata
        )
    
    def _detect_category(self, product: Dict) -> str:
        """ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ"""
        if 'category' in product:
            return product['category']
        
        title = str(product.get('title', '')).lower()
        for category, keywords in self.KNOWN_CATEGORIES.items():
            if any(kw in title for kw in keywords):
                return category
        return "general"
    
    def _extract_specs(self, product: Dict) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø´Ø®ØµØ§Øª"""
        specs = {}
        ignore = {'id', 'title', 'name', 'price', 'brand', 'category', 
                  'description', 'is_available', 'rating', 'url'}
        
        for key, value in product.items():
            if key not in ignore and value and not isinstance(value, (dict, list)):
                specs[key] = value
        
        return specs


class ArticleChunker:
    """Ú†Ø§Ù†Ú©Ø± Ù…Ù‚Ø§Ù„Ø§Øª"""
    
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", "ØŒ ", " "]
        )
    
    def chunk_articles(self, articles: List[Dict]) -> List[Document]:
        """Ú†Ø§Ù†Ú© Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ù‡ Documents"""
        all_docs = []
        
        for article in articles:
            chunks = self.splitter.split_text(article['content'])
            
            for i, chunk_text in enumerate(chunks):
                metadata = {
                    "type": "article",
                    "article_id": article['id'],
                    "article_title": article['title'],
                    "chunk_index": i,
                    "total_chunks": len(chunks)
                }
                
                doc = Document(page_content=chunk_text, metadata=metadata)
                all_docs.append(doc)
        
        return all_docs


# ============================================
# Ø¨Ø®Ø´ 2: Data Loading
# ============================================

def load_products() -> List[Dict]:
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª"""
    try:
        with open(PRODUCTS_JSON, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        products = data if isinstance(data, list) else data.get('products', [])
        print(f"âœ… {len(products)} Ù…Ø­ØµÙˆÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        return products
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª: {e}")
        return []


def load_articles() -> List[Dict]:
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª"""
    articles = []
    
    if not ARTICLES_DIR.exists():
        print(f"âš ï¸ Ù¾ÙˆØ´Ù‡ Ù…Ù‚Ø§Ù„Ø§Øª ÛŒØ§ÙØª Ù†Ø´Ø¯")
        return []
    
    for i, file_path in enumerate(ARTICLES_DIR.glob("*.txt"), 1):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if content:
                articles.append({
                    "id": f"article_{i}",
                    "title": file_path.stem,
                    "content": content
                })
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± {file_path.name}: {e}")
    
    print(f"âœ… {len(articles)} Ù…Ù‚Ø§Ù„Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    return articles


# ============================================
# Ø¨Ø®Ø´ 3: Vector DB Creation
# ============================================

def create_vector_db(documents: List[Document], 
                     persist_dir: Path,
                     collection_name: str) -> Chroma:
    """Ø³Ø§Ø®Øª Vector Database"""
    
    # Ø­Ø°Ù Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù‚Ø¯ÛŒÙ…ÛŒ
    if persist_dir.exists():
        shutil.rmtree(persist_dir)
        print(f"ğŸ—‘ï¸  Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù‚Ø¯ÛŒÙ…ÛŒ Ø­Ø°Ù Ø´Ø¯")
    
    # Embeddings
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        api_key=OPENAI_API_KEY,
        base_url=OPENAI_BASE_URL
    )
    
    # Ø³Ø§Ø®Øª Chroma
    print(f"â³ Ø¯Ø± Ø­Ø§Ù„ embedding {len(documents)} document...")
    
    vector_store = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=str(persist_dir)
    )
    
    print(f"âœ… Vector DB Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯: {persist_dir}")
    return vector_store


# ============================================
# Ø¨Ø®Ø´ 4: Main Setup
# ============================================

def setup_products():
    """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Vector DB Ù…Ø­ØµÙˆÙ„Ø§Øª"""
    print("\n" + "="*60)
    print("ğŸ“¦ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª")
    print("="*60)
    
    # 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
    products = load_products()
    if not products:
        print("âŒ Ù‡ÛŒÚ† Ù…Ø­ØµÙˆÙ„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
        return None
    
    # 2. Ú†Ø§Ù†Ú© Ú©Ø±Ø¯Ù† (Ø¨Ø¯ÙˆÙ† JSON Ù…ÛŒØ§Ù†ÛŒ)
    chunker = UniversalProductChunker()
    documents = chunker.chunk_products(products)
    print(f"âœ… {len(documents)} document Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯")
    
    # 3. Ø³Ø§Ø®Øª Vector DB
    vector_store = create_vector_db(
        documents=documents,
        persist_dir=PRODUCTS_CHROMA_DIR,
        collection_name=PRODUCTS_COLLECTION
    )
    
    return vector_store


def setup_articles():
    """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Vector DB Ù…Ù‚Ø§Ù„Ø§Øª"""
    print("\n" + "="*60)
    print("ğŸ“° Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª")
    print("="*60)
    
    # 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
    articles = load_articles()
    if not articles:
        print("âŒ Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
        return None
    
    # 2. Ú†Ø§Ù†Ú© Ú©Ø±Ø¯Ù†
    chunker = ArticleChunker(
        chunk_size=ARTICLE_CHUNK_SIZE,
        chunk_overlap=ARTICLE_CHUNK_OVERLAP
    )
    documents = chunker.chunk_articles(articles)
    print(f"âœ… {len(documents)} chunk Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯")
    
    # 3. Ø³Ø§Ø®Øª Vector DB
    vector_store = create_vector_db(
        documents=documents,
        persist_dir=ARTICLES_CHROMA_DIR,
        collection_name=ARTICLES_COLLECTION
    )
    
    return vector_store


def main():
    """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Setup"""
    print("\n" + "="*60)
    print("ğŸš€ Store Assistant RAG - Setup")
    print("="*60)
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    if not validate_config():
        print("\nâŒ Ù„Ø·ÙØ§Ù‹ Ù…Ø´Ú©Ù„Ø§Øª Ø±Ø§ Ø±ÙØ¹ Ú©Ù†ÛŒØ¯")
        return
    
    # Ø³Ø§Ø®Øª Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
    create_directories()
    
    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
    products_db = setup_products()
    
    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª
    articles_db = setup_articles()
    
    # Ø®Ù„Ø§ØµÙ‡
    print("\n" + "="*60)
    print("âœ… Setup Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
    print("="*60)
    print(f"ğŸ“ Ù…Ø­ØµÙˆÙ„Ø§Øª: {PRODUCTS_CHROMA_DIR}")
    print(f"ğŸ“ Ù…Ù‚Ø§Ù„Ø§Øª: {ARTICLES_CHROMA_DIR}")
    print("\nğŸ¯ Ø§Ú©Ù†ÙˆÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ rag_agent.py Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯")


if __name__ == "__main__":
    main()