"""
Store Assistant RAG Agent - Core Logic
Optimized for reduced token consumption and loop prevention.
"""

import os
import base64
from typing import Literal, Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
    trim_messages,
    BaseMessage,
)
from langchain_core.tools import tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field

try:
    from config import *
    from tts_handler import text_to_speech
except ImportError:
    from src.config import *
    from src.tts_handler import text_to_speech


# ============================================
# Global Variables
# ============================================
products_tool = None
articles_tool = None


# ============================================
# State Definition
# ============================================
class AgentState(MessagesState):
    """State with audio input/output support and retry counter"""
    audio_path: Optional[str] = None
    audio_output_path: Optional[str] = None
    enable_tts: bool = False
    retry_count: int = 0
     # [ØªØºÛŒÛŒØ± Ø¬Ø¯ÛŒØ¯]: Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† Ù…ØªØºÛŒØ± Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…ØªÙ† Ù…Ø®ØµÙˆØµ ÙˆÛŒØ³
    audio_script: Optional[str] = None 


# ============================================
# Helper Functions
# ============================================
def _extract_store_context(store_name: str) -> str:
    """
    Extract store type/context from store name.
    Examples:
        "Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø§Ø³ØªÙ‚Ù„Ø§Ù„" -> "mobile phone and electronics"
        "Ù„Ø¨Ø§Ø³ Ù¾Ø±Ø³Ù¾ÙˆÙ„ÛŒØ³" -> "clothing and fashion"
        "Ú©ØªØ§Ø¨ Ø¢Ø²Ø§Ø¯ÛŒ" -> "book"
    """
    store_lower = store_name.lower()
    
    # Define keywords for different store types
    mobile_keywords = ["Ù…ÙˆØ¨Ø§ÛŒÙ„", "mobile", "Ú¯ÙˆØ´ÛŒ", "phone", "Ù„Ù¾ØªØ§Ù¾", "laptop", "ØªØ¨Ù„Øª", "tablet"]
    clothing_keywords = ["Ù„Ø¨Ø§Ø³", "Ù¾ÙˆØ´Ø§Ú©", "clothing", "fashion", "Ù…Ø¯"]
    book_keywords = ["Ú©ØªØ§Ø¨", "book", "Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡"]
    electronics_keywords = ["Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©", "electronic", "Ø¯ÛŒØ¬ÛŒØªØ§Ù„", "digital"]
    
    # Check for mobile/electronics store
    if any(keyword in store_lower for keyword in mobile_keywords):
        return "mobile phone, laptop, tablet and electronics"
    
    # Check for clothing store
    if any(keyword in store_lower for keyword in clothing_keywords):
        return "clothing and fashion"
    
    # Check for book store
    if any(keyword in store_lower for keyword in book_keywords):
        return "book and publication"
    
    # Check for general electronics
    if any(keyword in store_lower for keyword in electronics_keywords):
        return "electronics and technology"
    
    # Default: try to use the store name itself as context
    return f"{store_name} products"


def get_trimmed_history(messages: list[BaseMessage], max_tokens=2000):
    """
    Aggressively trim message history to save costs.
    Keeps only system prompt + last few messages.
    """
    return trim_messages(
        messages,
        max_tokens=max_tokens,
        strategy="last",
        token_counter=len,
        include_system=True,
        start_on="human",
    )


def custom_router(state):
    """
    Updated Router:
    - If tools called -> go to 'retrieve'
    - If NO tools -> go to 'generate_answer' (to handle greetings intelligently)
    """
    # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø§Ø¨Ø²Ø§Ø± Ø®ÙˆØ§Ø³ØªÙ‡ ÛŒØ§ Ù†Ù‡
    last_message = state["messages"][-1]
    
    if hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0:
        return "retrieve"
    
    # [ØªØºÛŒÛŒØ± Ù…Ù‡Ù…]: Ù‚Ø¨Ù„Ø§Ù‹ Ù…ÛŒâ€ŒØ±ÙØª END ÛŒØ§ audio_outputØŒ Ø§Ù„Ø§Ù† Ù…ÛŒâ€ŒÙØ±Ø³ØªÛŒÙ…Ø´ Ù¾ÛŒØ´ Ù…ØºØ² Ù…ØªÙÚ©Ø±
    return "generate_answer"


def route_after_answer(state):
    """Route to audio output if TTS is enabled AND script exists"""
    # [ØªØºÛŒÛŒØ±]: ÙÙ‚Ø· Ø§Ú¯Ø± Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØµÙˆØªÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª Ø¨Ø±Ùˆ Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§
    if state.get("enable_tts", False) and state.get("audio_script"):
        return "audio_output"
    return END


# ============================================
# Vector Store Initialization
# ============================================
def load_vector_stores():
    """Load and initialize Chroma vector stores for products and articles"""
    log_step("LOAD", "Loading vector stores...")
    
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL, 
        api_key=API_KEY, 
        base_url=OPENAI_BASE_URL
    )
    
    products_store = Chroma(
        collection_name=PRODUCTS_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(PRODUCTS_CHROMA_DIR),
    )
    
    articles_store = Chroma(
        collection_name=ARTICLES_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(ARTICLES_CHROMA_DIR),
    )
    
    log_success("Vector stores loaded successfully")
    return products_store, articles_store


# ============================================
# Retriever Tools
# ============================================
def create_retriever_tools(products_store, articles_store):
    """
    Create retriever tools for products and articles.
    k=2 for lower token consumption (previously k=3)
    """
    products_retriever = products_store.as_retriever(search_kwargs={"k": 4})
    articles_retriever = articles_store.as_retriever(search_kwargs={"k": 2})

    @tool
    def products_retriever_tool(query: str):
        """Search products database (mobile, laptop, etc.). Returns price and availability."""
        return products_retriever.invoke(query)

    @tool
    def articles_retriever_tool(query: str):
        """Search articles and buying guides."""
        return articles_retriever.invoke(query)

    products_retriever_tool.name = "products_retriever"
    articles_retriever_tool.name = "articles_retriever"

    return products_retriever_tool, articles_retriever_tool


# ============================================
# Language Models
# ============================================

# [Ø§Ø¶Ø§ÙÙ‡ Ø´ÙˆØ¯]: Ù…Ø¯Ù„ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¬ÛŒØ³ÙˆÙ†
class ResponseDecision(BaseModel):
    """Ø³Ø§Ø®ØªØ§Ø± ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„"""
    visual_text: str = Field(description="The text to be displayed in the chat bubble. Can be markdown tables, lists, etc. Leave empty if Voice Only.")
    spoken_text: str = Field(description="The text to be converted to speech. Should be conversational and short. Leave empty if Text Only.")
    action: Literal["text_only", "voice_only", "hybrid"] = Field(description="The display mode selected.")


class GradeDocuments(BaseModel):
    """Schema for document grading"""
    binary_score: str = Field(description="'yes' or 'no'")


def gpt_4o_mini():
    """Initialize GPT-4o-mini model"""
    return ChatOpenAI(
        model=CHAT_GPT_MODEL, 
        api_key=API_KEY, 
        base_url=OPENAI_BASE_URL, 
        temperature=0
    )


def gemini_2_flash():
    """Initialize Gemini 2 Flash model"""
    return ChatGoogleGenerativeAI(
        model=CHAT_GEMINI_MODEL,
        google_api_key=API_KEY,
        transport="rest",
        client_options={"api_endpoint": GOOGLE_BASE_URL},
        temperature=0.7,
    )


# ============================================
# Audio Processing
# ============================================
def transcribe_audio_file(file_path: str) -> str:
    """
    Transcribe audio file using Gemini with vision/audio capabilities.
    Supports: mp3, ogg, wav, webm
    """
    if not file_path or not os.path.exists(file_path):
        return ""
    
    try:
        llm = gemini_2_flash()
        
        # Determine MIME type
        mime_type = "audio/mp3"
        if file_path.endswith(".ogg"):
            mime_type = "audio/ogg"
        elif file_path.endswith(".wav"):
            mime_type = "audio/wav"
        elif file_path.endswith(".webm"):
            mime_type = "audio/webm"

        # Read and encode audio
        with open(file_path, "rb") as audio_file:
            audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

        # Concise prompt to reduce input tokens
        strict_prompt = "Transcribe this audio to text only. No additional explanation."

        message = HumanMessage(
            content=[
                {"type": "text", "text": strict_prompt},
                {"type": "media", "mime_type": mime_type, "data": audio_b64},
            ]
        )
        
        logger.info(f"{Colors.CYAN}ðŸŽ¤ Transcribing audio...{Colors.END}")
        response = llm.invoke([message])
        return response.content.strip()
        
    except Exception as e:
        log_error(f"Audio transcription error: {e}")
        return ""


def check_audio_input(state: AgentState):
    """
    Check for audio input and transcribe if present.
    First node in the graph.
    """
    audio_path = state.get("audio_path")
    
    # [ØªØºÛŒÛŒØ± Ø¬Ø¯ÛŒØ¯]: Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ø¨Ø§ÙØ±Ù‡Ø§ Ø´Ø§Ù…Ù„ audio_script
    reset_dict = {
        "audio_path": None, 
        "audio_output_path": None, 
        "audio_script": None 
    }
    
    if audio_path and os.path.exists(audio_path):
        transcribed_text = transcribe_audio_file(audio_path)
        
        if transcribed_text:
            return {
                "messages": [HumanMessage(content=transcribed_text)],
                **reset_dict
            }
        else:
            return {
                "messages": [HumanMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ ØµØ¯Ø§ ÙˆØ§Ø¶Ø­ Ù†Ø¨ÙˆØ¯.")],
                **reset_dict
            }
    
    return reset_dict


# ============================================
# Agent Nodes
# ============================================
def generate_query_or_respond(state: AgentState):
    """
    Main decision node: Determine whether to search or respond directly.
    Uses tools (products/articles retrievers) if needed.
    """
    log_step("QUERY", "Analyzing request...")

    # Check for user message
    has_user = any(isinstance(msg, HumanMessage) for msg in state["messages"])
    if not has_user:
        return {"messages": [AIMessage(content="No message received.")]}

    llm = gpt_4o_mini()

    # Extract store context from name (e.g., "Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø§Ø³ØªÙ‚Ù„Ø§Ù„" -> mobile store)
    store_context = _extract_store_context(STORE_NAME)

    # Enhanced system prompt with store context
    system_prompt = f"""You are an assistant for "{STORE_NAME}" - {store_context}.

IMPORTANT: Your store name is "{STORE_NAME}" and you should freely share this name when customers ask about it. This is public information and there's no reason to hide it.

Your role:
- Answer questions about our {store_context} products, prices, and availability
- Use products_retriever for product searches
- Use articles_retriever for guides and articles
- Always introduce yourself with the store name when appropriate
- You should be able to write a good query for semantic search and retrieve information based on the request the user makes

Sample chunks are saved

    "id": 20110012,
    "title_fa": "Ú¯ÙˆØ´ÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯ Ù…Ø¯Ù„ Galaxy A07 Ø¯Ùˆ Ø³ÛŒÙ… Ú©Ø§Ø±Øª Ø¸Ø±ÙÛŒØª 128 Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª Ùˆ Ø±Ù… 4 Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª ",
    "title_en": "Samsung Galaxy A07 Dual SIM Storage 128GB And 4GB RAM Mobile Phone",
    "brand": "Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯",
    "price": 114862000,
    "price_formatted": "114,862,000 Ø±ÛŒØ§Ù„",
    "rating": 87.31,
    "rating_count": 219,
    "url": "https://www.digikala.com/product/dkp-20110012",
    "image": "https://dkstatics-public.digikala.com/digikala-products/69c8ee8dcb6d825fdb6de8a8515b2a45b4fb7a79_1763385430.jpg?x-oss-process=image/resize,m_lfit,h_300,w_300/quality,q_80",
    "colors": ["Ù…Ø´Ú©ÛŒ", "Ø³Ø¨Ø²", "ÛŒØ§Ø³ÛŒ"],
    "specifications": ,
    "is_available": true

- You should be able to convert the user's request into a query that performs a semantic search among hundreds of json like above 
"""
# Rules:
# 1. ONLY use information from the retrieval tools for specific product details
# 2. If information is not in the tools, say "We don't have that information currently"
# 3. Never make up prices, availability, or product details
# 4. For general questions about {store_context} or the store itself, answer directly
# 5. Always maintain context that you work for "{STORE_NAME}" - a {store_context} store

    # Aggressive history trimming (only last 4-5 messages)
    trimmed_msgs = get_trimmed_history(state["messages"], max_tokens=2000)
    messages = [SystemMessage(content=system_prompt)] + trimmed_msgs

    # Prevent infinite loops: disable tools after 2 retries
    if state.get("retry_count", 0) >= 2:
        log_warning("Retry limit reached. Direct response without tools.")
        response = llm.invoke(messages)
    else:
        if products_tool and articles_tool:
            response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)
        else:
            response = llm.invoke(messages)

    if hasattr(response, "tool_calls") and len(response.tool_calls) > 0:
        return {"messages": [response]}
    
    # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø§Ø¨Ø²Ø§Ø±ÛŒ ØµØ¯Ø§ Ù†Ø²Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø¬ÙˆØ§Ø¨ Ø³Ù„Ø§Ù… Ø¯Ø§Ø¯)ØŒ Ù…Ø§ Ù¾ÛŒØ§Ù…Ø´ Ø±Ø§ Ø¯ÙˆØ± Ù…ÛŒâ€ŒØ±ÛŒØ²ÛŒÙ…!
    # Ú†Ø±Ø§ØŸ Ú†ÙˆÙ† Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… 'generate_answer' Ø¨Ø§ ÙØ±Ù…Øª Ø¬ÛŒØ³ÙˆÙ† Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¬ÙˆØ§Ø¨ Ø³Ù„Ø§Ù… Ø±Ø§ Ø¨Ø¯Ù‡Ø¯.
    # Ù¾Ø³ Ù„ÛŒØ³Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù¾Ø¯ÛŒØª Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø®Ø§Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…).
    return {"messages": []}

def grade_documents(
    state: AgentState,
) -> Literal["generate_answer", "rewrite_question"]:
    """
    Grade document relevance with loop protection.
    After 1 retry, proceed to answer even if documents aren't perfect.
    """
    log_step("GRADE", "Grading documents...")

    # Loop protection: after 1 retry, proceed to answer
    current_retry = state.get("retry_count", 0)
    if current_retry >= 1:
        log_warning(f"Retry {current_retry}: Skipping strict grading.")
        return "generate_answer"

    # Extract tool messages
    tool_msgs = [
        msg for msg in state["messages"] if hasattr(msg, "type") and msg.type == "tool"
    ]
    
    if not tool_msgs:
        return "rewrite_question"

    llm = gpt_4o_mini()
    
    # Find original question
    question = state["messages"][0].content

    # Preview first 1000 chars of context for grading (cost savings)
    context_preview = "\n".join([msg.content[:1000] for msg in tool_msgs])

    grade_prompt = f"""Question: {question}
Context: {context_preview}
Are these documents relevant to the question? (yes/no)"""

    response = llm.invoke([{"role": "user", "content": grade_prompt}])

    if "yes" in response.content.lower():
        return "generate_answer"
    else:
        return "rewrite_question"


def rewrite_question(state: AgentState):
    """
    Rewrite question for better retrieval.
    Increments retry counter to prevent loops.
    """
    log_step("REWRITE", "Rewriting query...")

    # Increment retry counter
    new_count = state.get("retry_count", 0) + 1

    llm = gpt_4o_mini()

    # Find last human message (original question)
    messages = state["messages"]
    last_human_message = next(
        (m for m in reversed(messages) if isinstance(m, HumanMessage)), None
    )

    if last_human_message:
        original_q = last_human_message.content
    else:
        original_q = messages[-1].content

    logger.info(f"Original Question: {original_q}")

    # Concise rewrite prompt
    msg = (
        f"Improve this question for better product database search. "
        f"Write only the improved question, no explanation.\n"
        f"Original: {original_q}"
    )

    response = llm.invoke(msg)

    logger.info(
        f"{Colors.GREEN}Rewritten question ({new_count}): {response.content}{Colors.END}"
    )

    return {
        "messages": [HumanMessage(content=response.content)],
        "retry_count": new_count,
    }


def generate_answer(state: AgentState):
    """
    Generate answer using Structured Output to decide Text/Voice strategy.
    """
    log_step("ANSWER", "Generating smart response...")
    
    # [ØªØºÛŒÛŒØ±]: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² with_structured_output Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¬ÛŒØ³ÙˆÙ†
    llm = gpt_4o_mini().with_structured_output(ResponseDecision)

    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±
    question = "Unknown"
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            question = msg.content
            break

    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ØªÚ©Ø³Øª (Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„)
    tool_contents = [msg.content[:800] for msg in state["messages"] if hasattr(msg, "type") and msg.type == "tool"]
    full_context = "\n\n".join(tool_contents)[:3000]

    answer_prompt = f"""ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ (Multimodal) Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ "{STORE_NAME}" Ù‡Ø³ØªÛŒ.
ØªÙˆ Ø¨Ù‡ Ù…ÙˆØªÙˆØ± ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ ØµØ¯Ø§ (TTS) Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±ÛŒ. **Ù‡Ø±Ú¯Ø² Ù†Ú¯Ùˆ 'Ù…Ù† Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… ÙˆÛŒØ³ Ø¨ÙØ±Ø³ØªÙ…'**.

Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {question}
Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ (Context): {full_context}

Ù†Ú©ØªÙ‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ú©Ø§Ù†ØªÚ©Ø³Øª: 
- Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ ØªÙ…Ø§Ù…Ø§ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ù…Ø­ØµÙˆÙ„Ø§Øª Ú©Ù‡ Ø¬Ø²Ùˆ Ø¯Ø§Ù†Ø´ Ø¯Ø§Ø®Ù„ÛŒ Ø´Ù…Ø§ Ù‡Ø³Øª Ø¬Ù„Ùˆ Ø¨Ø±ÙˆÛŒØ¯ØŒ Ø§Ù…Ø§ Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø§Ø±ÛŒØ¯ Ù…Ù†Ø·Ø¨Ù‚ Ø¨Ø± Ø¯Ø§Ù†Ø´ Ú©Ù„ÛŒ Ø®ÙˆØ¯ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± Ø­ÙˆØ²Ù‡  ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯
- Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø­ÙˆØ²Ù‡ ØªØ®ØµØµÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù†Ø¨ÙˆØ¯ Ø³Ø¹ÛŒ Ú©Ù† Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø³Ù…Øª Ø­ÙˆØ²Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø³ÙˆÙ‚ Ø¯Ù‡ÛŒ Ùˆ Ø¯Ø± Ù…ÙˆØ±Ø¯ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ ØµØ­Ø¨Øª Ú©Ù†ÛŒ
- Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ùˆ Ø¨Ù‡ ÙØ±Ù…Øª Markdown Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ØŒ Ø³Ø¹ÛŒ Ú©Ù† Ø§Ø² Ø³Ø§Ø®Øª Ø¬Ø¯ÙˆÙ„ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒ
Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ (DECISION RULES):
1. **visual_text**: Ù…ØªÙ†ÛŒ Ú©Ù‡ Ø¯Ø± Ø­Ø¨Ø§Ø¨ Ú†Øª Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø´Ø§Ù…Ù„ Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ù„ÛŒØ³Øª Ù‚ÛŒÙ…Øª Ùˆ Ø¬Ø²Ø¦ÛŒØ§Øª).
2. **spoken_text**: Ù…ØªÙ†ÛŒ Ú©Ù‡ Ø¨Ø§ ØµØ¯Ø§ÛŒ Ø¨Ù„Ù†Ø¯ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø¨Ø§ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ØŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ø¯ Ùˆ ØªØ§Ø­Ø¯ Ø§Ù…Ú©Ø§Ù† Ø§Ø¹Ø¯Ø§Ø¯ ØªÙˆØ´ Ù†Ø¨Ø§Ø´Ù†).
3. **action**: Ù†ÙˆØ¹ Ù†Ù…Ø§ÛŒØ´ Ú©Ù‡ Ø´Ø§Ù…Ù„ 'text_only', 'voice_only', 'hybrid' Ø§Ø³Øª.

Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ (STRATEGY):
- **Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ (Ø³Ù„Ø§Ù…/Ø§Ø­ÙˆØ§Ù„â€ŒÙ¾Ø±Ø³ÛŒ):** 
  -> Action: 'voice_only' (ÛŒØ§ hybrid Ø¨Ø§ Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡).
  -> Spoken: ÛŒÚ© Ù¾Ø§Ø³Ø® Ú¯Ø±Ù… Ùˆ ØµÙ…ÛŒÙ…ÛŒ.
  
- **Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„/Ù‚ÛŒÙ…Øª:** 
  -> Action: 'hybrid'.
  -> Visual: Ù„ÛŒØ³Øª Ú©Ø§Ù…Ù„ Ù…Ø´Ø®ØµØ§Øª Ùˆ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§.
  -> Spoken: ÙÙ‚Ø· ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ Ú©ÙˆØªØ§Ù‡ (Ù…Ø«Ù„Ø§Ù‹: "Ù„ÛŒØ³Øª Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ Ø±Ùˆ Ø¨Ø±Ø§Øª ÙØ±Ø³ØªØ§Ø¯Ù…ØŒ Ù…Ø¯Ù„ Ù¾Ø±Ùˆ Ù‡Ù… Ù…ÙˆØ¬ÙˆØ¯Ù‡"). **Ù‡Ø±Ú¯Ø² Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ø¯Ø± ÙˆÛŒØ³ Ù†Ø®ÙˆØ§Ù†.**

- **Ø¯Ø±Ø®ÙˆØ§Ø³Øª ÙˆÛŒØ³ (Ú©Ø§Ø±Ø¨Ø± Ø¨Ú¯ÙˆÛŒØ¯ "ÙˆÛŒØ³ Ø¨Ø¯Ù‡"):** 
  -> Action: 'voice_only'.

Ø²Ø¨Ø§Ù† Ù¾Ø§Ø³Ø®: ÙØ§Ø±Ø³ÛŒ.
"""

    # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
    response: ResponseDecision = llm.invoke([{"role": "user", "content": answer_prompt}])
    
    visual = response.visual_text
    spoken = response.spoken_text
    
    # --- [LOGIC MATRIX: Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú©Ø§Ø±Ø¨Ø±] ---
    user_allows_tts = state.get("enable_tts", False)
    final_audio_script = None
    
    if not user_allows_tts:
        # Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± ØµØ¯Ø§ Ø±Ø§ Ø¨Ø³ØªÙ‡ØŒ Ù‡ÛŒÚ† ØµØ¯Ø§ÛŒÛŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ú©Ù†
        final_audio_script = None 
        # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª ÙÙ‚Ø· ØµØ¯Ø§ Ø¨Ø¯Ù‡Ø¯ØŒ Ù…ØªÙ† Ø¢Ù† Ø±Ø§ Ù†Ø´Ø§Ù† Ø¨Ø¯Ù‡ Ú©Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø§Ø² Ø¯Ø³Øª Ù†Ø¯Ù‡Ø¯
        if not visual and spoken:
            visual = spoken 
    else:
        # Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± ØµØ¯Ø§ Ø±Ø§ Ø¨Ø§Ø² Ú¯Ø°Ø§Ø´ØªÙ‡
        final_audio_script = spoken
        # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø®ÙˆØ§Ø³ØªÙ‡ ÙÙ‚Ø· ØµØ¯Ø§ Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© Ù…ØªÙ† Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ú¯Ø°Ø§Ø± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        if not visual:
            visual = "ðŸ”Š (Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ)"

    return {
        "messages": [AIMessage(content=visual)], # Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ú†Øª
        "audio_script": final_audio_script,       # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ù†ÙˆØ¯ ØµØ¯Ø§
        "retry_count": 0
    }


def generate_audio_output(state: AgentState):
    """
    Converts 'audio_script' to speech (instead of last message content).
    """
    # [ØªØºÛŒÛŒØ±]: Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø² Ù…ØªØºÛŒØ± Ø¬Ø¯ÛŒØ¯ audio_script
    script = state.get("audio_script")
    
    if not script:
        log_step("TTS", "No audio script to generate.")
        return {"audio_output_path": None}

    log_step("TTS", f"ðŸ”Š Generating audio ({len(script)} chars)...")

    audio_path = text_to_speech(
        text=script,
        model="gemini-2.5-flash-preview-tts",
        add_emotion=True,
    )

    if audio_path:
        return {"audio_output_path": audio_path}

    return {"audio_output_path": None}


# ============================================
# Graph Construction
# ============================================
def create_agent_graph(p_tool, a_tool):
    """
    Construct the LangGraph workflow.
    
    Flow:
    START -> check_audio -> generate_query_or_respond -> [retrieve OR audio_output OR END]
    retrieve -> grade_documents -> [generate_answer OR rewrite_question]
    generate_answer -> [audio_output OR END]
    rewrite_question -> generate_query_or_respond
    """
    global products_tool, articles_tool
    products_tool = p_tool
    articles_tool = a_tool

    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("check_audio", check_audio_input)
    workflow.add_node("generate_query_or_respond", generate_query_or_respond)
    workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
    workflow.add_node("rewrite_question", rewrite_question)
    workflow.add_node("generate_answer", generate_answer)
    workflow.add_node("audio_output", generate_audio_output)

    # Add edges
    workflow.add_edge(START, "check_audio")
    workflow.add_edge("check_audio", "generate_query_or_respond")

    # Custom router after query generation
    # Ø¯Ø± create_agent_graph:
    workflow.add_conditional_edges(
        "generate_query_or_respond",
        custom_router,
        {
            "retrieve": "retrieve",
            "generate_answer": "generate_answer", # <--- Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        }
    )

    workflow.add_conditional_edges("retrieve", grade_documents)

    # Conditional routing after answer
    workflow.add_conditional_edges(
        "generate_answer",
        route_after_answer,
        {"audio_output": "audio_output", END: END},
    )

    workflow.add_edge("audio_output", END)
    workflow.add_edge("rewrite_question", "generate_query_or_respond")

    # Compile with memory
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)
