# """
# Store Assistant RAG Agent
# Agent Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Voice Input, Checkpointer Ùˆ Gradio UI
# """

# import gradio as gr
# import base64
# import os
# from typing import Literal, Optional
# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_chroma import Chroma
# from langchain_core.messages import (
#     SystemMessage,
#     HumanMessage,
#     AIMessage,
#     trim_messages,
# )
# from langchain_core.tools import create_retriever_tool
# from langgraph.graph import StateGraph, MessagesState, START, END
# from langgraph.prebuilt import ToolNode, tools_condition
# from langgraph.checkpoint.memory import MemorySaver
# from pydantic import BaseModel, Field

# try:
#     from config import *
# except ImportError:
#     from src.config import *


# # ============================================
# # ØªØ¹Ø±ÛŒÙ State Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ ØµÙˆØª
# # ============================================


# class AgentState(MessagesState):
#     """State Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¯Ø±ÛŒØ§ÙØª ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ"""

#     audio_path: Optional[str] = None


# # ============================================
# # Ø¨Ø®Ø´ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores
# # ============================================


# def load_vector_stores():
#     """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector DBÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù‡"""

#     log_step("LOAD", "Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores...")

#     embeddings = OpenAIEmbeddings(
#         model=EMBEDDING_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL
#     )

#     products_store = Chroma(
#         collection_name=PRODUCTS_COLLECTION,
#         embedding_function=embeddings,
#         persist_directory=str(PRODUCTS_CHROMA_DIR),
#     )

#     articles_store = Chroma(
#         collection_name=ARTICLES_COLLECTION,
#         embedding_function=embeddings,
#         persist_directory=str(ARTICLES_CHROMA_DIR),
#     )

#     log_success("Vector stores Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
#     return products_store, articles_store


# # ============================================
# # Ø¨Ø®Ø´ 2: Ø³Ø§Ø®Øª Retriever Tools
# # ============================================


# def create_retriever_tools(products_store, articles_store):
#     """Ø³Ø§Ø®Øª Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""

#     products_retriever = products_store.as_retriever(search_kwargs={"k": RETRIEVAL_K})

#     articles_retriever = articles_store.as_retriever(search_kwargs={"k": RETRIEVAL_K})

#     products_tool = create_retriever_tool(
#         retriever=products_retriever,
#         name="products_retriever",
#         description="Ø§Ø¨Ø²Ø§Ø± Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡. Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„Ø§ØªØŒ Ù‚ÛŒÙ…ØªØŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ùˆ Ù…Ø´Ø®ØµØ§Øª ÙÙ†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.",
#     )

#     articles_tool = create_retriever_tool(
#         retriever=articles_retriever,
#         name="articles_retriever",
#         description="Ø§Ø¨Ø²Ø§Ø± Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ø±Ø§Ù‡Ù†Ù…Ø§. Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø®Ø±ÛŒØ¯ØŒ Ù†Ú©Ø§Øª Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.",
#     )

#     return products_tool, articles_tool


# # ============================================
# # Ø¨Ø®Ø´ 3: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ
# # ============================================


# class GradeDocuments(BaseModel):
#     """Ù…Ø¯Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø³ØªÙ†Ø¯Ø§Øª"""

#     binary_score: str = Field(description="Ø§Ù…ØªÛŒØ§Ø² Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†: 'yes' ÛŒØ§ 'no'")


# def gpt_4o_mini():
#     """Ø³Ø§Ø®Øª Ù…Ø¯Ù„ OpenAI"""
#     return ChatOpenAI(
#         model=CHAT_GPT_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL, temperature=0
#     )


# def gemini_2_flash():
#     """Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Gemini"""
#     return ChatGoogleGenerativeAI(
#         model=CHAT_GEMINI_MODEL,
#         google_api_key=API_KEY,
#         transport="rest",
#         client_options={"api_endpoint": GOOGLE_BASE_URL},
#         temperature=0.7,
#     )


# # ============================================
# # Ø¨Ø®Ø´ 4: Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª (Voice Input)
# # ============================================


# def transcribe_audio_file(file_path: str) -> str:
#     """ØªØ¨Ø¯ÛŒÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Gemini"""

#     if not file_path or not os.path.exists(file_path):
#         return ""

#     try:
#         llm = gemini_2_flash()

#         # ØªØ´Ø®ÛŒØµ Mime Type
#         mime_type = "audio/mp3"
#         if file_path.endswith(".ogg"):
#             mime_type = "audio/ogg"
#         elif file_path.endswith(".wav"):
#             mime_type = "audio/wav"

#         # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Base64
#         with open(file_path, "rb") as audio_file:
#             audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

#         # Prompt Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±Ø§Ù†Ù‡
#         strict_prompt = """
#         ÙˆØ¸ÛŒÙÙ‡ ØªÙˆ ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· "Transcription" Ø§Ø³Øª.
#         1. Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ø± Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ´Ù†ÙˆÛŒ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³.
#         2. Ù‡ÛŒÚ† Ø¹Ø¨Ø§Ø±Øª Ø§Ø¶Ø§ÙÙ‡â€ŒØ§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù†.
#         3. Ù„Ø­Ù† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø±Ø§ Ø­ÙØ¸ Ú©Ù†.
#         4. ÙÙ‚Ø· Ù…ØªÙ† Ø®Ø§Ù„Øµ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.
#         """

#         # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù… Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ
#         message = HumanMessage(
#             content=[
#                 {"type": "text", "text": strict_prompt},
#                 {"type": "media", "mime_type": mime_type, "data": audio_b64},
#             ]
#         )

#         logger.info(f"{Colors.CYAN}ğŸ¤ Ø¯Ø± Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§ Ø¨Ù‡ Ù…ØªÙ†...{Colors.END}")
#         response = llm.invoke([message])

#         text = response.content.strip()
#         logger.info(f"{Colors.GREEN}âœ… Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {text}{Colors.END}")
#         return text

#     except Exception as e:
#         log_error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§: {e}")
#         return ""


# def check_audio_input(state: AgentState):
#     """Ù†ÙˆØ¯ ÙˆØ±ÙˆØ¯ÛŒ: Ø¨Ø±Ø±Ø³ÛŒ ØµØ¯Ø§ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ØªÙ†"""

#     audio_path = state.get("audio_path")

#     if audio_path and os.path.exists(audio_path):
#         log_step("AUDIO", "ğŸ¤ Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ...")

#         transcribed_text = transcribe_audio_file(audio_path)

#         if transcribed_text:
#             new_message = HumanMessage(content=transcribed_text)
#             return {"messages": [new_message], "audio_path": None}
#         else:
#             # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ Ù…ØªÙ†ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯
#             return {
#                 "messages": [HumanMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… ØµØ¯Ø§ÛŒ Ø´Ù…Ø§ Ø±Ø§ Ø¨Ø´Ù†ÙˆÙ…. Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")],
#                 "audio_path": None
#             }    

#     log_step("AUDIO", "Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ø§Ø³Øª (Ø¨Ø¯ÙˆÙ† ØµØ¯Ø§)")
#     return {}


# # ============================================
# # Ø¨Ø®Ø´ 5: Agent Nodes
# # ============================================


# def generate_query_or_respond(state: AgentState):
#     """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ: Ù†ÛŒØ§Ø² Ø¨Ù‡ RAG ÛŒØ§ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…"""

#     log_step("QUERY", "ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±...")

#     # 1. Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ: Ø¢ÛŒØ§ Ø§ØµÙ„Ø§Ù‹ Ø³ÙˆØ§Ù„ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
#     # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§Ú¯ "Ø§ÙˆÙ„ÛŒÙ† Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ" Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
#     has_user_message = any(isinstance(msg, HumanMessage) for msg in state["messages"])
    
#     if not has_user_message:
#         log_warning("Ù‡ÛŒÚ† Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ø§Ø² Ú©Ø§Ø±Ø¨Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ø´Ø§ÛŒØ¯ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯).")
#         return {
#             "messages": [
#                 AIMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ ØµØ¯Ø§ÛŒØªØ§Ù† Ø±Ø§ Ù†Ø´Ù†ÛŒØ¯Ù… ÛŒØ§ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ù…ØªÙ† Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.")
#             ]
#         }

#     # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆØ§Ù„ Ø¹Ø§Ø¯ÛŒ...
#     # llm = gemini_2_flash()
#     llm = gpt_4o_mini() # ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Tool Calling Ø§Ø² GPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

#     system_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù‡Ø³ØªÛŒ.

# ÙˆØ¸Ø§ÛŒÙ ØªÙˆ:
# - Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª (Ù‚ÛŒÙ…ØªØŒ Ù…Ø´Ø®ØµØ§ØªØŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ)
# - Ø§Ø±Ø§Ø¦Ù‡ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡ Ø®Ø±ÛŒØ¯
# - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…Ø®ØªÙ„Ù

# Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³:
# - products_retriever: Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
# - articles_retriever: Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§Ù‡Ø§ Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡

# Ù…Ù‡Ù…:
# - Ù‡ÛŒÚ†â€ŒÙˆÙ‚Øª Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¯Ø§Ø®Ù„ÛŒ Ø®ÙˆØ¯Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†
# - ÙÙ‚Ø· Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ú¯ÛŒØ±
# - Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†Ø¯Ø§Ø±ÛŒØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯Ùˆ
# - Ø¯Ø± Ø³ÙˆØ§Ù„Ø§Øª ØºÛŒØ±Ù…Ø±ØªØ¨Ø·ØŒ Ù…Ø³ÛŒØ± Ú¯ÙØªÚ¯Ùˆ Ø±Ø§ Ø¨Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ùˆ Ø®Ø¯Ù…Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù‡Ø¯Ø§ÛŒØª Ú©Ù†"""

#     # Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡
#     trimmed_messages = trim_messages(
#         state["messages"],
#         max_tokens=1000,
#         strategy="last",
#         token_counter=len,
#         include_system=True,
#     )

#     # Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ (Ø¨Ø§ SystemMessage Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø§ØµÙ„Ø§Ø­ Ú©Ø±Ø¯ÛŒÙ…)
#     messages = [SystemMessage(content=system_prompt)] + trimmed_messages

#     log_step("QUERY", "Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ RAG...")
    
#     # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø¯Ù„
#     response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)

#     # Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† ØªØµÙ…ÛŒÙ… Ù…Ø¯Ù„
#     if hasattr(response, "tool_calls") and response.tool_calls:
#         tool_names = [tc["name"] for tc in response.tool_calls]
#         log_step("QUERY", f"Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø¨Ø²Ø§Ø±: {', '.join(tool_names)}")
#     else:
#         log_step("QUERY", "Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø¯ÙˆÙ† RAG")

#     return {"messages": [response]}


# def grade_documents(
#     state: AgentState,
# ) -> Literal["generate_answer", "rewrite_question"]:
#     """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡"""

#     log_step("GRADE", "Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ù…Ø³ØªÙ†Ø¯Ø§Øª...")

#     llm = gpt_4o_mini()

#     question = None
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, HumanMessage):
#             question = msg.content
#             break

#     tool_contents = []
#     for msg in state["messages"]:
#         if hasattr(msg, "content") and hasattr(msg, "type"):
#             if msg.type == "tool":
#                 tool_contents.append(msg.content)

#     context = "\n\n".join(tool_contents) if tool_contents else ""

#     logger.info(f"{Colors.BLUE}ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len(tool_contents)}{Colors.END}")
#     logger.info(f"{Colors.BLUE}ğŸ“ Ø·ÙˆÙ„ context: {len(context)} Ú©Ø§Ø±Ø§Ú©ØªØ±{Colors.END}")

#     grade_prompt = f"""Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†.

# Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {question}

# Ù…Ø³ØªÙ†Ø¯Ø§Øª: {context}

# Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù†Ø¯?
# - Ø§Ú¯Ø± Ù…Ø±ØªØ¨Ø· Ùˆ Ù…ÙÛŒØ¯ Ù‡Ø³ØªÙ†Ø¯: yes
# - Ø§Ú¯Ø± Ù†Ø§Ù…Ø±ØªØ¨Ø· ÛŒØ§ Ù†Ø§Ú©Ø§ÙÛŒ Ù‡Ø³ØªÙ†Ø¯: no"""

#     response = llm.with_structured_output(GradeDocuments).invoke(
#         [{"role": "user", "content": grade_prompt}]
#     )

#     decision = response.binary_score

#     if decision == "yes":
#         log_success("Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±ØªØ¨Ø· Ø§Ø³Øª â†’ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®")
#         return "generate_answer"
#     else:
#         log_warning("Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù†Ø§Ù…Ø±ØªØ¨Ø· â†’ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„")
#         return "rewrite_question"


# def rewrite_question(state: AgentState):
#     """Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ±"""

#     log_step("REWRITE", "Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„...")

#     llm = gpt_4o_mini()

#     question = None
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, HumanMessage):
#             question = msg.content
#             break

#     logger.info(f"{Colors.YELLOW}â“ Ø³ÙˆØ§Ù„ Ù‚Ø¨Ù„ÛŒ: {question}{Colors.END}")

#     prompt = f"""Ø³ÙˆØ§Ù„ Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‡ ØªØ§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø¨Ø§Ø´Ø¯:

# Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ: {question}

# ÙÙ‚Ø· Ø³ÙˆØ§Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÛŒ."""

#     response = llm.invoke([{"role": "user", "content": prompt}])
#     new_question = response.content

#     logger.info(f"{Colors.GREEN}âœï¸  Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯: {new_question}{Colors.END}")

#     return {"messages": [HumanMessage(content=new_question)]}


# def generate_answer(state: AgentState):
#     """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ"""

#     log_step("ANSWER", "ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ...")

#     llm = gpt_4o_mini()

#     question = None
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, HumanMessage):
#             question = msg.content
#             break

#     tool_contents = []
#     for msg in state["messages"]:
#         if hasattr(msg, "type") and msg.type == "tool":
#             tool_contents.append(msg.content)

#     context = "\n\n".join(tool_contents)

#     logger.info(
#         f"{Colors.CYAN}ğŸ’¬ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§Ø³Ø§Ø³ {len(tool_contents)} Ù…Ø³ØªÙ†Ø¯{Colors.END}"
#     )

#     answer_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù‡Ø³ØªÛŒ.

# Ø¨Ø±Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡:

# Ø³ÙˆØ§Ù„: {question}

# Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯:
# {context}

# Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„:
# - ÙÙ‚Ø· Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
# - Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†ÛŒØ³ØªØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯Ùˆ
# - Ù¾Ø§Ø³Ø® Ø±Ø§ ÙˆØ§Ø¶Ø­ Ùˆ Ù…Ø®ØªØµØ± Ø¨Ù†ÙˆÛŒØ³ (3-5 Ø¬Ù…Ù„Ù‡)"""

#     response = llm.invoke([{"role": "user", "content": answer_prompt}])

#     answer_length = len(response.content)
#     logger.info(f"{Colors.GREEN}âœ… Ù¾Ø§Ø³Ø® ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯ ({answer_length} Ú©Ø§Ø±Ø§Ú©ØªØ±){Colors.END}")

#     return {"messages": [response]}


# # ============================================
# # Ø¨Ø®Ø´ 6: Ø³Ø§Ø®Øª Graph
# # ============================================


# def create_agent_graph(products_tool, articles_tool):
#     """Ø³Ø§Ø®Øª Ú¯Ø±Ø§Ù agent Ø¨Ø§ Voice + Checkpointer"""

#     workflow = StateGraph(AgentState)

#     # Nodes
#     workflow.add_node("check_audio", check_audio_input)
#     workflow.add_node("generate_query_or_respond", generate_query_or_respond)
#     workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
#     workflow.add_node("rewrite_question", rewrite_question)
#     workflow.add_node("generate_answer", generate_answer)

#     # Edges
#     workflow.add_edge(START, "check_audio")
#     workflow.add_edge("check_audio", "generate_query_or_respond")

#     workflow.add_conditional_edges(
#         "generate_query_or_respond", tools_condition, {"tools": "retrieve", END: END}
#     )

#     workflow.add_conditional_edges("retrieve", grade_documents)
#     workflow.add_edge("generate_answer", END)
#     workflow.add_edge("rewrite_question", "generate_query_or_respond")

#     memory = MemorySaver()

#     return workflow.compile(checkpointer=memory)


# # ============================================
# # Ø¨Ø®Ø´ 7: Gradio UI
# # ============================================


# def chat_with_agent(message, history):
#     """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù… (Ù…ØªÙ† + ØµÙˆØª)"""

#     user_text = ""
#     audio_path = None

#     # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ùˆ ÙØ§ÛŒÙ„
#     if isinstance(message, dict):
#         user_text = message.get("text", "")
#         files = message.get("files", [])
#         if files:
#             audio_path = files[0]
#     else:
#         user_text = str(message)

#     logger.info(f"\n{Colors.PURPLE}{'='*60}{Colors.END}")
#     logger.info(f"{Colors.PURPLE}ğŸ†• Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø¯ÛŒØ¯{Colors.END}")
#     logger.info(f"{Colors.PURPLE}{'='*60}{Colors.END}")

#     # Ø³Ø§Ø®Øª ÙˆØ±ÙˆØ¯ÛŒ Ú¯Ø±Ø§Ù
#     graph_input = {"messages": []}

#     if audio_path:
#         graph_input["audio_path"] = audio_path
#         # Ø§Ú¯Ø± Ù…ØªÙ†ÛŒ Ù‡Ù… Ú©Ù†Ø§Ø± ÙØ§ÛŒÙ„ Ø¨ÙˆØ¯ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
#         if user_text:
#             graph_input["messages"].append(HumanMessage(content=user_text))
            
#     elif user_text:
#         # ÙÙ‚Ø· Ù…ØªÙ†
#         graph_input["messages"].append(HumanMessage(content=user_text))
#         graph_input["audio_path"] = None
#     else:
#         return "Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† ÛŒØ§ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯."

#     config = {"configurable": {"thread_id": "user_session"}}
#     response_text = ""
#     step_count = 0

#     try:
#         # --- Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³ØªØ±ÛŒÙ… ---
#         for event in agent.stream(graph_input, config=config, stream_mode="values"):
            
#             # Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø§Ø² Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¬Ø§Ø±ÛŒ
#             current_messages = event.get("messages", [])
            
#             # --- Ø§ØµÙ„Ø§Ø­ Ø§ØµÙ„ÛŒ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª ---
#             # Ø§Ú¯Ø± Ù„ÛŒØ³Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ (Ù‡Ù†ÙˆØ² Ù¾ÛŒØ§Ù…ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯Ù‡)ØŒ Ø¨Ø±Ùˆ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¹Ø¯
#             if not current_messages:
#                 continue

#             step_count += 1
            
#             # Ø­Ø§Ù„Ø§ Ú©Ù‡ Ù…Ø·Ù…Ø¦Ù†ÛŒÙ… Ù„ÛŒØ³Øª Ø®Ø§Ù„ÛŒ Ù†ÛŒØ³ØªØŒ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
#             last_message = current_messages[-1]

#             if isinstance(last_message, AIMessage):
#                 response_text = last_message.content
#             elif hasattr(last_message, "content"):
#                 # Ú¯Ø§Ù‡ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… HumanMessage Ø§Ø³Øª (Ù‚Ø¨Ù„ Ø§Ø² Ø¬ÙˆØ§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ)ØŒ Ø¢Ù† Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… ØªØ§ Ø¬ÙˆØ§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ Ø¨ÛŒØ§ÛŒØ¯
#                 # Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ø®ÙˆØ§Ù‡ÛŒØ¯ Ù„Ø­Ø¸Ù‡ Ø¨Ù‡ Ù„Ø­Ø¸Ù‡ Ø¢Ù¾Ø¯ÛŒØª Ú©Ù†ÛŒØ¯.
#                 # Ø§ÛŒÙ†Ø¬Ø§ ÙÙ‚Ø· Ø¬ÙˆØ§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ (AIMessage) Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ… ØªØ§ UI ØªÙ…ÛŒØ² Ø¨Ø§Ø´Ø¯
#                 pass

#         logger.info(f"{Colors.GREEN}ğŸ“Š Ú©Ù„ Ù…Ø±Ø§Ø­Ù„: {step_count}{Colors.END}")
#         logger.info(f"{Colors.PURPLE}{'='*60}{Colors.END}\n")

#         if not response_text:
#             return "Ø¯Ø±Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµØ¯Ø§... (Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®ÛŒ Ù†ÛŒØ§Ù…Ø¯ØŒ Ú©ÛŒÙÛŒØª ØµØ¯Ø§ Ø±Ø§ Ú†Ú© Ú©Ù†ÛŒØ¯)"
            
#         return response_text or "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡Ù…."

#     except Exception as e:
#         log_error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {str(e)}")
#         import traceback
#         traceback.print_exc() # Ú†Ø§Ù¾ Ø¯Ù‚ÛŒÙ‚ Ø®Ø·Ø§ Ø¯Ø± Ú©Ù†Ø³ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ù‡ØªØ±
#         return f"âŒ Ø®Ø·Ø§: {str(e)}"


# # ============================================
# # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
# # ============================================

# if __name__ == "__main__":
#     logger.info(f"{Colors.CYAN}{'='*60}{Colors.END}")
#     logger.info(f"{Colors.CYAN}ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Store Assistant Ø¨Ø§ Voice{Colors.END}")
#     logger.info(f"{Colors.CYAN}{'='*60}{Colors.END}")

#     products_store, articles_store = load_vector_stores()
#     products_tool, articles_tool = create_retriever_tools(
#         products_store, articles_store
#     )
#     agent = create_agent_graph(products_tool, articles_tool)

#     log_success("Agent Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª")
#     logger.info(f"{Colors.CYAN}{'='*60}{Colors.END}\n")

#     # --- Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ ---
#     demo = gr.ChatInterface(
#         fn=chat_with_agent,
#         title=f"ğŸ¤– {STORE_NAME} - Ø¯Ø³ØªÛŒØ§Ø± ØµÙˆØªÛŒ Ùˆ Ù…ØªÙ†ÛŒ",
#         description="Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ØªØ§ÛŒÙ¾ Ú©Ù†ÛŒØ¯ ÛŒØ§ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ (ÙˆÛŒØ³) Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.",
#         multimodal=True,
#         # theme=gr.themes.Soft(), # <--- Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ Ø­Ø°Ù Ú©Ø±Ø¯ÛŒÙ… Ú†ÙˆÙ† Ø¨Ø§Ø¹Ø« Ø§Ø±ÙˆØ± Ø´Ø¯
#     )

#     logger.info(f"{Colors.GREEN}ğŸŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Gradio UI...{Colors.END}")
#     demo.launch(share=False, server_name="127.0.0.1", server_port=7860)

"""
Store Assistant RAG Agent - Logic Core
Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ù…Ù†Ø·Ù‚ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ Ú¯Ø±Ø§Ù Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§Ø³Øª.
Ù‡ÛŒÚ† UI ÛŒØ§ Ø³Ø±ÙˆØ±ÛŒ Ø¯Ø± Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø§Ø¬Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.
"""

import os
import base64
from typing import Literal, Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
    trim_messages,
)
from langchain_core.tools import create_retriever_tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field

try:
    from config import *
except ImportError:
    from src.config import *


products_tool = None
articles_tool = None
# ============================================
# ØªØ¹Ø±ÛŒÙ State
# ============================================
class AgentState(MessagesState):
    audio_path: Optional[str] = None

# ============================================
# Ø¨Ø®Ø´ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores
# ============================================
def load_vector_stores():
    log_step("LOAD", "Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores...")
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL
    )
    products_store = Chroma(
        collection_name=PRODUCTS_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(PRODUCTS_CHROMA_DIR),
    )
    articles_store = Chroma(
        collection_name=ARTICLES_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(ARTICLES_CHROMA_DIR),
    )
    log_success("Vector stores Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    return products_store, articles_store

# ============================================
# Ø¨Ø®Ø´ 2: Ø³Ø§Ø®Øª Retriever Tools
# ============================================
def create_retriever_tools(products_store, articles_store):
    products_retriever = products_store.as_retriever(search_kwargs={"k": RETRIEVAL_K})
    articles_retriever = articles_store.as_retriever(search_kwargs={"k": RETRIEVAL_K})

    products_tool = create_retriever_tool(
        retriever=products_retriever,
        name="products_retriever",
        description="Ø§Ø¨Ø²Ø§Ø± Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡...",
    )
    articles_tool = create_retriever_tool(
        retriever=articles_retriever,
        name="articles_retriever",
        description="Ø§Ø¨Ø²Ø§Ø± Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ø±Ø§Ù‡Ù†Ù…Ø§...",
    )
    return products_tool, articles_tool

# ============================================
# Ø¨Ø®Ø´ 3: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ
# ============================================
class GradeDocuments(BaseModel):
    binary_score: str = Field(description="Ø§Ù…ØªÛŒØ§Ø² Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†: 'yes' ÛŒØ§ 'no'")

def gpt_4o_mini():
    return ChatOpenAI(
        model=CHAT_GPT_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL, temperature=0
    )

def gemini_2_flash():
    return ChatGoogleGenerativeAI(
        model=CHAT_GEMINI_MODEL,
        google_api_key=API_KEY,
        transport="rest",
        client_options={"api_endpoint": GOOGLE_BASE_URL},
        temperature=0.7,
    )

# ============================================
# Ø¨Ø®Ø´ 4: Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª (Voice Input)
# ============================================


def transcribe_audio_file(file_path: str) -> str:
    """ØªØ¨Ø¯ÛŒÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Gemini"""

    if not file_path or not os.path.exists(file_path):
        return ""

    try:
        llm = gemini_2_flash()

        # ØªØ´Ø®ÛŒØµ Mime Type
        mime_type = "audio/mp3"
        if file_path.endswith(".ogg"):
            mime_type = "audio/ogg"
        elif file_path.endswith(".wav"):
            mime_type = "audio/wav"

        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Base64
        with open(file_path, "rb") as audio_file:
            audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

        # Prompt Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±Ø§Ù†Ù‡
        strict_prompt = """
        ÙˆØ¸ÛŒÙÙ‡ ØªÙˆ ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· "Transcription" Ø§Ø³Øª.
        1. Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ø± Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ´Ù†ÙˆÛŒ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³.
        2. Ù‡ÛŒÚ† Ø¹Ø¨Ø§Ø±Øª Ø§Ø¶Ø§ÙÙ‡â€ŒØ§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù†.
        3. Ù„Ø­Ù† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø±Ø§ Ø­ÙØ¸ Ú©Ù†.
        4. ÙÙ‚Ø· Ù…ØªÙ† Ø®Ø§Ù„Øµ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.
        """

        # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù… Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ
        message = HumanMessage(
            content=[
                {"type": "text", "text": strict_prompt},
                {"type": "media", "mime_type": mime_type, "data": audio_b64},
            ]
        )

        logger.info(f"{Colors.CYAN}ğŸ¤ Ø¯Ø± Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§ Ø¨Ù‡ Ù…ØªÙ†...{Colors.END}")
        response = llm.invoke([message])

        text = response.content.strip()
        logger.info(f"{Colors.GREEN}âœ… Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {text}{Colors.END}")
        return text

    except Exception as e:
        log_error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§: {e}")
        return ""


def check_audio_input(state: AgentState):
    """Ù†ÙˆØ¯ ÙˆØ±ÙˆØ¯ÛŒ: Ø¨Ø±Ø±Ø³ÛŒ ØµØ¯Ø§ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ØªÙ†"""

    audio_path = state.get("audio_path")

    if audio_path and os.path.exists(audio_path):
        log_step("AUDIO", "ğŸ¤ Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ...")

        transcribed_text = transcribe_audio_file(audio_path)

        if transcribed_text:
            new_message = HumanMessage(content=transcribed_text)
            return {"messages": [new_message], "audio_path": None}
        else:
            # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ Ù…ØªÙ†ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯
            return {
                "messages": [HumanMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… ØµØ¯Ø§ÛŒ Ø´Ù…Ø§ Ø±Ø§ Ø¨Ø´Ù†ÙˆÙ…. Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")],
                "audio_path": None
            }    

    log_step("AUDIO", "Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ø§Ø³Øª (Ø¨Ø¯ÙˆÙ† ØµØ¯Ø§)")
    return {}


# ============================================
# Ø¨Ø®Ø´ 5: Agent Nodes
# ============================================


def generate_query_or_respond(state: AgentState):
    """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ: Ù†ÛŒØ§Ø² Ø¨Ù‡ RAG ÛŒØ§ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…"""

    log_step("QUERY", "ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±...")

    # 1. Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ: Ø¢ÛŒØ§ Ø§ØµÙ„Ø§Ù‹ Ø³ÙˆØ§Ù„ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
    # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§Ú¯ "Ø§ÙˆÙ„ÛŒÙ† Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ" Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    has_user_message = any(isinstance(msg, HumanMessage) for msg in state["messages"])
    
    if not has_user_message:
        log_warning("Ù‡ÛŒÚ† Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ø§Ø² Ú©Ø§Ø±Ø¨Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ø´Ø§ÛŒØ¯ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯).")
        return {
            "messages": [
                AIMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ ØµØ¯Ø§ÛŒØªØ§Ù† Ø±Ø§ Ù†Ø´Ù†ÛŒØ¯Ù… ÛŒØ§ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ù…ØªÙ† Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.")
            ]
        }

    # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆØ§Ù„ Ø¹Ø§Ø¯ÛŒ...
    # llm = gemini_2_flash()
    llm = gpt_4o_mini() # ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Tool Calling Ø§Ø² GPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

    system_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù‡Ø³ØªÛŒ.

ÙˆØ¸Ø§ÛŒÙ ØªÙˆ:
- Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª (Ù‚ÛŒÙ…ØªØŒ Ù…Ø´Ø®ØµØ§ØªØŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ)
- Ø§Ø±Ø§Ø¦Ù‡ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡ Ø®Ø±ÛŒØ¯
- Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…Ø®ØªÙ„Ù

Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³:
- products_retriever: Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
- articles_retriever: Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§Ù‡Ø§ Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡

Ù…Ù‡Ù…:
- Ù‡ÛŒÚ†â€ŒÙˆÙ‚Øª Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¯Ø§Ø®Ù„ÛŒ Ø®ÙˆØ¯Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†
- ÙÙ‚Ø· Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ú¯ÛŒØ±
- Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†Ø¯Ø§Ø±ÛŒØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯Ùˆ
- Ø¯Ø± Ø³ÙˆØ§Ù„Ø§Øª ØºÛŒØ±Ù…Ø±ØªØ¨Ø·ØŒ Ù…Ø³ÛŒØ± Ú¯ÙØªÚ¯Ùˆ Ø±Ø§ Ø¨Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ùˆ Ø®Ø¯Ù…Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù‡Ø¯Ø§ÛŒØª Ú©Ù†"""

    # Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡
    trimmed_messages = trim_messages(
        state["messages"],
        max_tokens=1000,
        strategy="last",
        token_counter=len,
        include_system=True,
    )

    # Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ (Ø¨Ø§ SystemMessage Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø§ØµÙ„Ø§Ø­ Ú©Ø±Ø¯ÛŒÙ…)
    messages = [SystemMessage(content=system_prompt)] + trimmed_messages

    log_step("QUERY", "Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ RAG...")
    
    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø¯Ù„
    response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)

    # Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† ØªØµÙ…ÛŒÙ… Ù…Ø¯Ù„
    if hasattr(response, "tool_calls") and response.tool_calls:
        tool_names = [tc["name"] for tc in response.tool_calls]
        log_step("QUERY", f"Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø¨Ø²Ø§Ø±: {', '.join(tool_names)}")
    else:
        log_step("QUERY", "Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø¯ÙˆÙ† RAG")

    return {"messages": [response]}


def grade_documents(
    state: AgentState,
) -> Literal["generate_answer", "rewrite_question"]:
    """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡"""

    log_step("GRADE", "Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ù…Ø³ØªÙ†Ø¯Ø§Øª...")

    llm = gpt_4o_mini()

    question = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            question = msg.content
            break

    tool_contents = []
    for msg in state["messages"]:
        if hasattr(msg, "content") and hasattr(msg, "type"):
            if msg.type == "tool":
                tool_contents.append(msg.content)

    context = "\n\n".join(tool_contents) if tool_contents else ""

    logger.info(f"{Colors.BLUE}ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len(tool_contents)}{Colors.END}")
    logger.info(f"{Colors.BLUE}ğŸ“ Ø·ÙˆÙ„ context: {len(context)} Ú©Ø§Ø±Ø§Ú©ØªØ±{Colors.END}")

    grade_prompt = f"""Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†.

Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {question}

Ù…Ø³ØªÙ†Ø¯Ø§Øª: {context}

Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù†Ø¯?
- Ø§Ú¯Ø± Ù…Ø±ØªØ¨Ø· Ùˆ Ù…ÙÛŒØ¯ Ù‡Ø³ØªÙ†Ø¯: yes
- Ø§Ú¯Ø± Ù†Ø§Ù…Ø±ØªØ¨Ø· ÛŒØ§ Ù†Ø§Ú©Ø§ÙÛŒ Ù‡Ø³ØªÙ†Ø¯: no"""

    response = llm.with_structured_output(GradeDocuments).invoke(
        [{"role": "user", "content": grade_prompt}]
    )

    decision = response.binary_score

    if decision == "yes":
        log_success("Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±ØªØ¨Ø· Ø§Ø³Øª â†’ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®")
        return "generate_answer"
    else:
        log_warning("Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù†Ø§Ù…Ø±ØªØ¨Ø· â†’ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„")
        return "rewrite_question"


def rewrite_question(state: AgentState):
    """Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ±"""

    log_step("REWRITE", "Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„...")

    llm = gpt_4o_mini()

    question = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            question = msg.content
            break

    logger.info(f"{Colors.YELLOW}â“ Ø³ÙˆØ§Ù„ Ù‚Ø¨Ù„ÛŒ: {question}{Colors.END}")

    prompt = f"""Ø³ÙˆØ§Ù„ Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‡ ØªØ§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø¨Ø§Ø´Ø¯:

Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ: {question}

ÙÙ‚Ø· Ø³ÙˆØ§Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÛŒ."""

    response = llm.invoke([{"role": "user", "content": prompt}])
    new_question = response.content

    logger.info(f"{Colors.GREEN}âœï¸  Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯: {new_question}{Colors.END}")

    return {"messages": [HumanMessage(content=new_question)]}


def generate_answer(state: AgentState):
    """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ"""

    log_step("ANSWER", "ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ...")

    llm = gpt_4o_mini()

    question = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            question = msg.content
            break

    tool_contents = []
    for msg in state["messages"]:
        if hasattr(msg, "type") and msg.type == "tool":
            tool_contents.append(msg.content)

    context = "\n\n".join(tool_contents)

    logger.info(
        f"{Colors.CYAN}ğŸ’¬ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§Ø³Ø§Ø³ {len(tool_contents)} Ù…Ø³ØªÙ†Ø¯{Colors.END}"
    )

    answer_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù‡Ø³ØªÛŒ.

Ø¨Ø±Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡:

Ø³ÙˆØ§Ù„: {question}

Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯:
{context}

Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„:
- ÙÙ‚Ø· Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
- Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†ÛŒØ³ØªØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯Ùˆ
- Ù¾Ø§Ø³Ø® Ø±Ø§ ÙˆØ§Ø¶Ø­ Ùˆ Ù…Ø®ØªØµØ± Ø¨Ù†ÙˆÛŒØ³ (3-5 Ø¬Ù…Ù„Ù‡)"""

    response = llm.invoke([{"role": "user", "content": answer_prompt}])

    answer_length = len(response.content)
    logger.info(f"{Colors.GREEN}âœ… Ù¾Ø§Ø³Ø® ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯ ({answer_length} Ú©Ø§Ø±Ø§Ú©ØªØ±){Colors.END}")

    return {"messages": [response]}
# ============================================
# Ø¨Ø®Ø´ 6: Ø³Ø§Ø®Øª Graph
# ============================================
def create_agent_graph(p_tool, a_tool):

    # Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ
    global products_tool, articles_tool
    
    # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù†ÙˆØ¯Ù‡Ø§
    products_tool = p_tool
    articles_tool = a_tool

    workflow = StateGraph(AgentState)
    
    workflow.add_node("check_audio", check_audio_input)
    workflow.add_node("generate_query_or_respond", generate_query_or_respond)
    workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
    workflow.add_node("rewrite_question", rewrite_question)
    workflow.add_node("generate_answer", generate_answer)

    workflow.add_edge(START, "check_audio")
    workflow.add_edge("check_audio", "generate_query_or_respond")
    
    workflow.add_conditional_edges(
        "generate_query_or_respond", tools_condition, {"tools": "retrieve", END: END}
    )
    workflow.add_conditional_edges("retrieve", grade_documents)
    workflow.add_edge("generate_answer", END)
    workflow.add_edge("rewrite_question", "generate_query_or_respond")

    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# ============================================
# Ø­Ø°Ù Ú©Ø§Ù…Ù„ Ø¨Ø®Ø´ Main Ùˆ Gradio
# ============================================
# Ù‚Ø¨Ù„Ø§Ù‹ Ø§ÛŒÙ†Ø¬Ø§ if __name__ == "__main__" Ø¯Ø§Ø´ØªÛŒÙ… Ú©Ù‡ gradio Ø±Ø§ Ù„Ø§Ù†Ú† Ù…ÛŒâ€ŒÚ©Ø±Ø¯.
# Ø§Ù„Ø§Ù† Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ÙÙ‚Ø· ØªÙˆØ§Ø¨Ø¹ Ø¨Ø§Ù„Ø§ Ø±Ø§ "ØªØ¹Ø±ÛŒÙ" Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ server.py Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ "ØµØ¯Ø§" Ù…ÛŒâ€ŒØ²Ù†Ø¯.