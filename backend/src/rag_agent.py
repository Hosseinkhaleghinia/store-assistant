# """
# Store Assistant RAG Agent - Core Logic
# Optimized for reduced token consumption and loop prevention.
# """

# import os
# import base64
# from typing import Literal, Optional
# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_chroma import Chroma
# from langchain_core.messages import (
#     SystemMessage,
#     HumanMessage,
#     AIMessage,
#     trim_messages,
#     BaseMessage,
#     RemoveMessage,
# )
# from langchain_core.tools import tool
# from langgraph.graph import StateGraph, MessagesState, START, END
# from langgraph.prebuilt import ToolNode, tools_condition
# from langgraph.checkpoint.memory import MemorySaver
# from pydantic import BaseModel, Field

# try:
#     from config import *
#     from tts_handler import text_to_speech
# except ImportError:
#     from src.config import *
#     from src.tts_handler import text_to_speech


# # ============================================
# # Global Variables
# # ============================================
# products_tool = None
# articles_tool = None


# # ============================================
# # State Definition
# # ============================================
# class AgentState(MessagesState):
#     """State with audio input/output support and retry counter"""
#     audio_path: Optional[str] = None
#     audio_output_path: Optional[str] = None
#     enable_tts: bool = False
#     retry_count: int = 0
#      # [ØªØºÛŒÛŒØ± Ø¬Ø¯ÛŒØ¯]: Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† Ù…ØªØºÛŒØ± Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…ØªÙ† Ù…Ø®ØµÙˆØµ ÙˆÛŒØ³
#     audio_script: Optional[str] = None 


# # ============================================
# # Helper Functions
# # ============================================
# def _extract_store_context(store_name: str) -> str:
#     """
#     Extract store type/context from store name.
#     Examples:
#         "Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø§Ø³ØªÙ‚Ù„Ø§Ù„" -> "mobile phone and electronics"
#         "Ù„Ø¨Ø§Ø³ Ù¾Ø±Ø³Ù¾ÙˆÙ„ÛŒØ³" -> "clothing and fashion"
#         "Ú©ØªØ§Ø¨ Ø¢Ø²Ø§Ø¯ÛŒ" -> "book"
#     """
#     store_lower = store_name.lower()
    
#     # Define keywords for different store types
#     mobile_keywords = ["Ù…ÙˆØ¨Ø§ÛŒÙ„", "mobile", "Ú¯ÙˆØ´ÛŒ", "phone", "Ù„Ù¾ØªØ§Ù¾", "laptop", "ØªØ¨Ù„Øª", "tablet"]
#     clothing_keywords = ["Ù„Ø¨Ø§Ø³", "Ù¾ÙˆØ´Ø§Ú©", "clothing", "fashion", "Ù…Ø¯"]
#     book_keywords = ["Ú©ØªØ§Ø¨", "book", "Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡"]
#     electronics_keywords = ["Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©", "electronic", "Ø¯ÛŒØ¬ÛŒØªØ§Ù„", "digital"]
    
#     # Check for mobile/electronics store
#     if any(keyword in store_lower for keyword in mobile_keywords):
#         return "mobile phone, laptop, tablet and electronics"
    
#     # Check for clothing store
#     if any(keyword in store_lower for keyword in clothing_keywords):
#         return "clothing and fashion"
    
#     # Check for book store
#     if any(keyword in store_lower for keyword in book_keywords):
#         return "book and publication"
    
#     # Check for general electronics
#     if any(keyword in store_lower for keyword in electronics_keywords):
#         return "electronics and technology"
    
#     # Default: try to use the store name itself as context
#     return f"{store_name} products"


# def get_trimmed_history(messages: list[BaseMessage], max_tokens=2000):
#     """
#     Aggressively trim message history to save costs.
#     Keeps only system prompt + last few messages.
#     """
#     return trim_messages(
#         messages,
#         max_tokens=max_tokens,
#         strategy="last",
#         token_counter=len,
#         include_system=True,
#         start_on="human",
#     )


# def custom_router(state):
#     """
#     Updated Router:
#     - If tools called -> go to 'retrieve'
#     - If NO tools -> go to 'generate_answer' (to handle greetings intelligently)
#     """
#     # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø§Ø¨Ø²Ø§Ø± Ø®ÙˆØ§Ø³ØªÙ‡ ÛŒØ§ Ù†Ù‡
#     last_message = state["messages"][-1]
    
#     if hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0:
#         return "retrieve"
    
#     # [ØªØºÛŒÛŒØ± Ù…Ù‡Ù…]: Ù‚Ø¨Ù„Ø§Ù‹ Ù…ÛŒâ€ŒØ±ÙØª END ÛŒØ§ audio_outputØŒ Ø§Ù„Ø§Ù† Ù…ÛŒâ€ŒÙØ±Ø³ØªÛŒÙ…Ø´ Ù¾ÛŒØ´ Ù…ØºØ² Ù…ØªÙÚ©Ø±
#     return "generate_answer"


# def route_after_answer(state):
#     """Route to audio output if TTS is enabled AND script exists"""
#     # [ØªØºÛŒÛŒØ±]: ÙÙ‚Ø· Ø§Ú¯Ø± Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØµÙˆØªÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª Ø¨Ø±Ùˆ Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§
#     if state.get("enable_tts", False) and state.get("audio_script"):
#         return "audio_output"
#     return END


# # ============================================
# # Vector Store Initialization
# # ============================================
# def load_vector_stores():
#     """Load and initialize Chroma vector stores for products and articles"""
#     log_step("LOAD", "Loading vector stores...")
    
#     embeddings = OpenAIEmbeddings(
#         model=EMBEDDING_MODEL, 
#         api_key=API_KEY, 
#         base_url=OPENAI_BASE_URL
#     )
    
#     products_store = Chroma(
#         collection_name=PRODUCTS_COLLECTION,
#         embedding_function=embeddings,
#         persist_directory=str(PRODUCTS_CHROMA_DIR),
#     )
    
#     articles_store = Chroma(
#         collection_name=ARTICLES_COLLECTION,
#         embedding_function=embeddings,
#         persist_directory=str(ARTICLES_CHROMA_DIR),
#     )
    
#     log_success("Vector stores loaded successfully")
#     return products_store, articles_store


# # ============================================
# # Retriever Tools
# # ============================================
# # def create_retriever_tools(products_store, articles_store):
# #     """
# #     Create retriever tools for products and articles.
# #     k=2 for lower token consumption (previously k=3)
# #     """
# #     products_retriever = products_store.as_retriever(search_kwargs={"k": 4})
# #     articles_retriever = articles_store.as_retriever(search_kwargs={"k": 2})

# #     @tool
# #     def products_retriever_tool(query: str):
# #         """Search products database (mobile, laptop, etc.). Returns price and availability."""
# #         return products_retriever.invoke(query)

# #     @tool
# #     def articles_retriever_tool(query: str):
# #         """Search articles and buying guides."""
# #         return articles_retriever.invoke(query)

# #     products_retriever_tool.name = "products_retriever"
# #     articles_retriever_tool.name = "articles_retriever"

# #     return products_retriever_tool, articles_retriever_tool
# def create_retriever_tools(products_store, articles_store):
#     """
#     Create retriever tools with DYNAMIC 'k' support.
#     Allows the LLM to request 1 result for specific details, or up to 4 for lists.
#     """
    
#     # 1. ØªØ¹Ø±ÛŒÙ Ø³Ø§Ø®ØªØ§Ø± ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø¯Ù„ Ø¨Ø¯ÙˆÙ†Ù‡ Ù…ÛŒØªÙˆÙ†Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø±Ùˆ ØªØ¹ÛŒÛŒÙ† Ú©Ù†Ù‡
#     class SearchInput(BaseModel):
#         query: str = Field(description="Ù…ØªÙ† Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø­ØµÙˆÙ„Ø§Øª ÛŒØ§ Ù…Ù‚Ø§Ù„Ø§Øª")
#         k: int = Field(
#             default=2,
#             description="ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø². Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ø¬Ø²Ø¦ÛŒØ§Øª ÛŒÚ© Ù…Ø­ØµÙˆÙ„ Ø®Ø§Øµ Ø±Ø§ Ø®ÙˆØ§Ø³Øª Ø¹Ø¯Ø¯ 1 Ùˆ Ø§Ú¯Ø± Ù„ÛŒØ³Øª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø®ÙˆØ§Ø³Øª Ø¹Ø¯Ø¯ 4 Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†."
#         )

#     # 2. Ø§Ø¨Ø²Ø§Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª (Ù‡ÙˆØ´Ù…Ù†Ø¯)
#     @tool(args_schema=SearchInput)
#     def products_retriever_tool(query: str, k: int = 2):
#         """Search products database. Returns price and availability."""
        
#         # [Ù…Ù‡Ù…] Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø³Ø®Øª: Ù‡ÛŒÚ†ÙˆÙ‚Øª Ø¨ÛŒØ´ØªØ± Ø§Ø² 4 ØªØ§ Ù†Ø¯Ù‡ØŒ Ø­ØªÛŒ Ø§Ú¯Ù‡ Ù…Ø¯Ù„ Ø®ÙˆØ§Ø³Øª
#         safe_k = min(k, 4) 
#         # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ú©Ù…ØªØ± Ø§Ø² 1 Ù‡Ù… Ù†Ø´Ù‡
#         safe_k = max(safe_k, 1)
        
#         # Ø³Ø§Ø®Øª Ø±ÛŒØªØ±ÛŒÙˆØ± Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ©
#         retriever = products_store.as_retriever(search_kwargs={"k": safe_k})
#         return retriever.invoke(query)

#     # 3. Ø§Ø¨Ø²Ø§Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ù‚Ø§Ù„Ø§Øª (Ù‡ÙˆØ´Ù…Ù†Ø¯)
#     @tool(args_schema=SearchInput)
#     def articles_retriever_tool(query: str, k: int = 2):
#         """Search articles and buying guides."""
        
#         # Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø¹Ù…ÙˆÙ„Ø§ 2 ØªØ§ Ú©Ø§ÙÛŒÙ‡ØŒ ÙˆÙ„ÛŒ Ø³Ù‚Ù Ø±Ùˆ Ù‡Ù…ÙˆÙ† 4 Ù…ÛŒØ°Ø§Ø±ÛŒÙ…
#         safe_k = min(k, 4)
#         safe_k = max(safe_k, 1)
        
#         retriever = articles_store.as_retriever(search_kwargs={"k": safe_k})
#         return retriever.invoke(query)

#     products_retriever_tool.name = "products_retriever"
#     articles_retriever_tool.name = "articles_retriever"

#     return products_retriever_tool, articles_retriever_tool


# # ============================================
# # Language Models
# # ============================================

# # [Ø§Ø¶Ø§ÙÙ‡ Ø´ÙˆØ¯]: Ù…Ø¯Ù„ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¬ÛŒØ³ÙˆÙ†
# class ResponseDecision(BaseModel):
#     """Ø³Ø§Ø®ØªØ§Ø± ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„"""
#     visual_text: str = Field(description="The text to be displayed in the chat bubble. Can be markdown tables, lists, etc. Leave empty if Voice Only.")
#     spoken_text: str = Field(description="The text to be converted to speech. Should be conversational and short. Leave empty if Text Only.")
#     action: Literal["text_only", "voice_only", "hybrid"] = Field(description="The display mode selected.")


# class GradeDocuments(BaseModel):
#     """Schema for document grading"""
#     binary_score: str = Field(description="'yes' or 'no'")


# def gpt_4o_mini():
#     """Initialize GPT-4o-mini model"""
#     return ChatOpenAI(
#         model=CHAT_GPT_MODEL, 
#         api_key=API_KEY, 
#         base_url=OPENAI_BASE_URL, 
#         temperature=0
#     )


# def gemini_2_flash():
#     """Initialize Gemini 2 Flash model"""
#     return ChatGoogleGenerativeAI(
#         model=CHAT_GEMINI_MODEL,
#         google_api_key=API_KEY,
#         transport="rest",
#         client_options={"api_endpoint": GOOGLE_BASE_URL},
#         temperature=0.7,
#     )


# # ============================================
# # Audio Processing
# # ============================================
# def transcribe_audio_file(file_path: str) -> str:
#     """
#     Transcribe audio file using Gemini with vision/audio capabilities.
#     Supports: mp3, ogg, wav, webm
#     """
#     if not file_path or not os.path.exists(file_path):
#         return ""
    
#     try:
#         llm = gemini_2_flash()
        
#         # Determine MIME type
#         mime_type = "audio/mp3"
#         if file_path.endswith(".ogg"):
#             mime_type = "audio/ogg"
#         elif file_path.endswith(".wav"):
#             mime_type = "audio/wav"
#         elif file_path.endswith(".webm"):
#             mime_type = "audio/webm"

#         # Read and encode audio
#         with open(file_path, "rb") as audio_file:
#             audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

#         # Concise prompt to reduce input tokens
#         strict_prompt = "Transcribe this audio to text only. No additional explanation."

#         message = HumanMessage(
#             content=[
#                 {"type": "text", "text": strict_prompt},
#                 {"type": "media", "mime_type": mime_type, "data": audio_b64},
#             ]
#         )
        
#         logger.info(f"{Colors.CYAN}ğŸ¤ Transcribing audio...{Colors.END}")
#         response = llm.invoke([message])
#         return response.content.strip()
        
#     except Exception as e:
#         log_error(f"Audio transcription error: {e}")
#         return ""


# def check_audio_input(state: AgentState):
#     """
#     Check for audio input and transcribe if present.
#     First node in the graph.
#     """
#     audio_path = state.get("audio_path")
    
#     # [ØªØºÛŒÛŒØ± Ø¬Ø¯ÛŒØ¯]: Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ø¨Ø§ÙØ±Ù‡Ø§ Ø´Ø§Ù…Ù„ audio_script
#     reset_dict = {
#         "audio_path": None, 
#         "audio_output_path": None, 
#         "audio_script": None 
#     }
    
#     if audio_path and os.path.exists(audio_path):
#         transcribed_text = transcribe_audio_file(audio_path)
        
#         if transcribed_text:
#             return {
#                 "messages": [HumanMessage(content=transcribed_text)],
#                 **reset_dict
#             }
#         else:
#             return {
#                 "messages": [HumanMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ ØµØ¯Ø§ ÙˆØ§Ø¶Ø­ Ù†Ø¨ÙˆØ¯.")],
#                 **reset_dict
#             }
    
#     return reset_dict


# # ============================================
# # Agent Nodes
# # ============================================
# def generate_query_or_respond(state: AgentState):
#     """
#     Main decision node: Determine whether to search or respond directly.
#     Uses tools (products/articles retrievers) if needed.
#     """
#     log_step("QUERY", "Analyzing request...")

#     # Check for user message
#     has_user = any(isinstance(msg, HumanMessage) for msg in state["messages"])
#     if not has_user:
#         return {"messages": [AIMessage(content="No message received.")]}

#     llm = gpt_4o_mini()

#     # Extract store context from name (e.g., "Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø§Ø³ØªÙ‚Ù„Ø§Ù„" -> mobile store)
#     store_context = _extract_store_context(STORE_NAME)

#     # Enhanced system prompt with store context
#     system_prompt = f"""You are an assistant for "{STORE_NAME}" - {store_context}.

# IMPORTANT: Your store name is "{STORE_NAME}" and you should freely share this name when customers ask about it. This is public information and there's no reason to hide it.

# Your role:
# - Answer questions about our {store_context} products, prices, and availability
# - Use products_retriever for product searches
# - Use articles_retriever for guides and articles
# - Always introduce yourself with the store name when appropriate
# - You should be able to write a good query for semantic search and retrieve information based on the request the user makes

# Sample chunks are saved

#     "id": 20110012,
#     "title_fa": "Ú¯ÙˆØ´ÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯ Ù…Ø¯Ù„ Galaxy A07 Ø¯Ùˆ Ø³ÛŒÙ… Ú©Ø§Ø±Øª Ø¸Ø±ÙÛŒØª 128 Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª Ùˆ Ø±Ù… 4 Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª ",
#     "title_en": "Samsung Galaxy A07 Dual SIM Storage 128GB And 4GB RAM Mobile Phone",
#     "brand": "Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯",
#     "price": 114862000,
#     "price_formatted": "114,862,000 Ø±ÛŒØ§Ù„",
#     "rating": 87.31,
#     "rating_count": 219,
#     "url": "https://www.digikala.com/product/dkp-20110012",
#     "image": "https://dkstatics-public.digikala.com/digikala-products/69c8ee8dcb6d825fdb6de8a8515b2a45b4fb7a79_1763385430.jpg?x-oss-process=image/resize,m_lfit,h_300,w_300/quality,q_80",
#     "colors": ["Ù…Ø´Ú©ÛŒ", "Ø³Ø¨Ø²", "ÛŒØ§Ø³ÛŒ"],
#     "specifications": ,
#     "is_available": true

# - You should be able to convert the user's request into a query that performs a semantic search among hundreds of json like above 
# """
# # Rules:
# # 1. ONLY use information from the retrieval tools for specific product details
# # 2. If information is not in the tools, say "We don't have that information currently"
# # 3. Never make up prices, availability, or product details
# # 4. For general questions about {store_context} or the store itself, answer directly
# # 5. Always maintain context that you work for "{STORE_NAME}" - a {store_context} store

#     # Aggressive history trimming (only last 4-5 messages)
#     trimmed_msgs = get_trimmed_history(state["messages"], max_tokens=2000)
#     messages = [SystemMessage(content=system_prompt)] + trimmed_msgs

#     # Prevent infinite loops: disable tools after 2 retries
#     if state.get("retry_count", 0) >= 2:
#         log_warning("Retry limit reached. Direct response without tools.")
#         response = llm.invoke(messages)
#     else:
#         if products_tool and articles_tool:
#             response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)
#         else:
#             response = llm.invoke(messages)

#     if hasattr(response, "tool_calls") and len(response.tool_calls) > 0:
#         return {"messages": [response]}
    
#     # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø§Ø¨Ø²Ø§Ø±ÛŒ ØµØ¯Ø§ Ù†Ø²Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø¬ÙˆØ§Ø¨ Ø³Ù„Ø§Ù… Ø¯Ø§Ø¯)ØŒ Ù…Ø§ Ù¾ÛŒØ§Ù…Ø´ Ø±Ø§ Ø¯ÙˆØ± Ù…ÛŒâ€ŒØ±ÛŒØ²ÛŒÙ…!
#     # Ú†Ø±Ø§ØŸ Ú†ÙˆÙ† Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… 'generate_answer' Ø¨Ø§ ÙØ±Ù…Øª Ø¬ÛŒØ³ÙˆÙ† Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¬ÙˆØ§Ø¨ Ø³Ù„Ø§Ù… Ø±Ø§ Ø¨Ø¯Ù‡Ø¯.
#     # Ù¾Ø³ Ù„ÛŒØ³Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù¾Ø¯ÛŒØª Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø®Ø§Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…).
#     return {"messages": []}

# def grade_documents(
#     state: AgentState,
# ) -> Literal["generate_answer", "rewrite_question"]:
#     """
#     Grade document relevance with loop protection.
#     After 1 retry, proceed to answer even if documents aren't perfect.
#     """
#     log_step("GRADE", "Grading documents...")

#     # Loop protection: after 1 retry, proceed to answer
#     current_retry = state.get("retry_count", 0)
#     if current_retry >= 1:
#         log_warning(f"Retry {current_retry}: Skipping strict grading.")
#         return "generate_answer"

#     # Extract tool messages
#     tool_msgs = [
#         msg for msg in state["messages"] if hasattr(msg, "type") and msg.type == "tool"
#     ]
    
#     if not tool_msgs:
#         return "rewrite_question"

#     llm = gpt_4o_mini()
    
#     # Find original question
#     question = state["messages"][0].content

#     # Preview first 1000 chars of context for grading (cost savings)
#     #! Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ Ø­Ø¯ÙˆØ¯ 5000 Ú©Ø§Ø±Ø§Ú©ØªØ± ÙˆØ±ÙˆØ¯ÛŒ
#     context_preview = "\n".join([msg.content[:5000] for msg in tool_msgs]) 

#     grade_prompt = f"""Question: {question}
# Context: {context_preview}
# Are these documents relevant to the question? (yes/no)"""

#     response = llm.invoke([{"role": "user", "content": grade_prompt}])

#     if "yes" in response.content.lower():
#         return "generate_answer"
#     else:
#         return "rewrite_question"


# def rewrite_question(state: AgentState):
#     """
#     Rewrite question for better retrieval.
#     Increments retry counter to prevent loops.
#     """
#     log_step("REWRITE", "Rewriting query...")

#     # Increment retry counter
#     new_count = state.get("retry_count", 0) + 1

#     llm = gpt_4o_mini()

#     # Find last human message (original question)
#     messages = state["messages"]
#     last_human_message = next(
#         (m for m in reversed(messages) if isinstance(m, HumanMessage)), None
#     )

#     if last_human_message:
#         original_q = last_human_message.content
#     else:
#         original_q = messages[-1].content

#     logger.info(f"Original Question: {original_q}")

#     # Concise rewrite prompt
#     msg = (
#         f"Improve this question for better product database search. "
#         f"Write only the improved question, no explanation.\n"
#         f"Original: {original_q}"
#     )

#     response = llm.invoke(msg)

#     logger.info(
#         f"{Colors.GREEN}Rewritten question ({new_count}): {response.content}{Colors.END}"
#     )

#     return {
#         "messages": [HumanMessage(content=response.content)],
#         "retry_count": new_count,
#     }


# def generate_answer(state: AgentState):
#     """
#     Generate answer using Structured Output to decide Text/Voice strategy.
#     """
#     log_step("ANSWER", "Generating smart response...")
    
#     # [ØªØºÛŒÛŒØ±]: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² with_structured_output Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¬ÛŒØ³ÙˆÙ†
#     llm = gpt_4o_mini().with_structured_output(ResponseDecision)

#     # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±
#     question = "Unknown"
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, HumanMessage):
#             question = msg.content
#             break

#     # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ØªÚ©Ø³Øª (Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„)
#     #! 5000â€ŒÚ©Ø§Ø±Ø§Ú©ØªØ±
#     tool_contents = [msg.content[:5000] for msg in state["messages"] if hasattr(msg, "type") and msg.type == "tool"]
#     #! 10000 Ú©Ø§Ø±Ø§Ú©ØªØ±
#     full_context = "\n\n".join(tool_contents)[:10000]

#     answer_prompt = f"""ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ (Multimodal) Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ "{STORE_NAME}" Ù‡Ø³ØªÛŒ.
# ØªÙˆ Ø¨Ù‡ Ù…ÙˆØªÙˆØ± ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ ØµØ¯Ø§ (TTS) Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±ÛŒ. **Ù‡Ø±Ú¯Ø² Ù†Ú¯Ùˆ 'Ù…Ù† Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… ÙˆÛŒØ³ Ø¨ÙØ±Ø³ØªÙ…'**.

# Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {question}
# Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ (Context): {full_context}

# Ù†Ú©ØªÙ‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ú©Ø§Ù†ØªÚ©Ø³Øª: 
# - Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ ØªÙ…Ø§Ù…Ø§ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ù…Ø­ØµÙˆÙ„Ø§Øª Ú©Ù‡ Ø¬Ø²Ùˆ Ø¯Ø§Ù†Ø´ Ø¯Ø§Ø®Ù„ÛŒ Ø´Ù…Ø§ Ù‡Ø³Øª Ø¬Ù„Ùˆ Ø¨Ø±ÙˆÛŒØ¯ØŒ Ø§Ù…Ø§ Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø§Ø±ÛŒØ¯ Ù…Ù†Ø·Ø¨Ù‚ Ø¨Ø± Ø¯Ø§Ù†Ø´ Ú©Ù„ÛŒ Ø®ÙˆØ¯ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± Ø­ÙˆØ²Ù‡  ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯
# - Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø­ÙˆØ²Ù‡ ØªØ®ØµØµÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù†Ø¨ÙˆØ¯ Ø³Ø¹ÛŒ Ú©Ù† Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø³Ù…Øª Ø­ÙˆØ²Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø³ÙˆÙ‚ Ø¯Ù‡ÛŒ Ùˆ Ø¯Ø± Ù…ÙˆØ±Ø¯ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ ØµØ­Ø¨Øª Ú©Ù†ÛŒ
# - Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ùˆ Ø¨Ù‡ ÙØ±Ù…Øª Markdown Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ØŒ Ø³Ø¹ÛŒ Ú©Ù† Ø§Ø² Ø³Ø§Ø®Øª Ø¬Ø¯ÙˆÙ„ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒ
# Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ (DECISION RULES):
# 1. **visual_text**: Ù…ØªÙ†ÛŒ Ú©Ù‡ Ø¯Ø± Ø­Ø¨Ø§Ø¨ Ú†Øª Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø´Ø§Ù…Ù„ Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ù„ÛŒØ³Øª Ù‚ÛŒÙ…Øª Ùˆ Ø¬Ø²Ø¦ÛŒØ§Øª).
# 2. **spoken_text**: Ù…ØªÙ†ÛŒ Ú©Ù‡ Ø¨Ø§ ØµØ¯Ø§ÛŒ Ø¨Ù„Ù†Ø¯ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø¨Ø§ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ØŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ø¯ Ùˆ ØªØ§Ø­Ø¯ Ø§Ù…Ú©Ø§Ù† Ø§Ø¹Ø¯Ø§Ø¯ ØªÙˆØ´ Ù†Ø¨Ø§Ø´Ù†).
# 3. **action**: Ù†ÙˆØ¹ Ù†Ù…Ø§ÛŒØ´ Ú©Ù‡ Ø´Ø§Ù…Ù„ 'text_only', 'voice_only', 'hybrid' Ø§Ø³Øª.

# Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ (STRATEGY):
# - **Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ (Ø³Ù„Ø§Ù…/Ø§Ø­ÙˆØ§Ù„â€ŒÙ¾Ø±Ø³ÛŒ):** 
#   -> Action: 'voice_only' (ÛŒØ§ hybrid Ø¨Ø§ Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡).
#   -> Spoken: ÛŒÚ© Ù¾Ø§Ø³Ø® Ú¯Ø±Ù… Ùˆ ØµÙ…ÛŒÙ…ÛŒ.
  
# - **Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„/Ù‚ÛŒÙ…Øª:** 
#   -> Action: 'hybrid'.
#   -> Visual: Ù„ÛŒØ³Øª Ú©Ø§Ù…Ù„ Ù…Ø´Ø®ØµØ§Øª Ùˆ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§.
#   -> Spoken: ÙÙ‚Ø· ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ Ú©ÙˆØªØ§Ù‡ (Ù…Ø«Ù„Ø§Ù‹: "Ù„ÛŒØ³Øª Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ Ø±Ùˆ Ø¨Ø±Ø§Øª ÙØ±Ø³ØªØ§Ø¯Ù…ØŒ Ù…Ø¯Ù„ Ù¾Ø±Ùˆ Ù‡Ù… Ù…ÙˆØ¬ÙˆØ¯Ù‡"). **Ù‡Ø±Ú¯Ø² Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ø¯Ø± ÙˆÛŒØ³ Ù†Ø®ÙˆØ§Ù†.**

# - **Ø¯Ø±Ø®ÙˆØ§Ø³Øª ÙˆÛŒØ³ (Ú©Ø§Ø±Ø¨Ø± Ø¨Ú¯ÙˆÛŒØ¯ "ÙˆÛŒØ³ Ø¨Ø¯Ù‡"):** 
#   -> Action: 'voice_only'.

# Ø²Ø¨Ø§Ù† Ù¾Ø§Ø³Ø®: ÙØ§Ø±Ø³ÛŒ.
# """

#     # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
#     response: ResponseDecision = llm.invoke([{"role": "user", "content": answer_prompt}])
    
#     visual = response.visual_text
#     spoken = response.spoken_text
    
#     # --- [LOGIC MATRIX: Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú©Ø§Ø±Ø¨Ø±] ---
#     user_allows_tts = state.get("enable_tts", False)
#     final_audio_script = None
    
#     if not user_allows_tts:
#         # Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± ØµØ¯Ø§ Ø±Ø§ Ø¨Ø³ØªÙ‡ØŒ Ù‡ÛŒÚ† ØµØ¯Ø§ÛŒÛŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ú©Ù†
#         final_audio_script = None 
#         # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª ÙÙ‚Ø· ØµØ¯Ø§ Ø¨Ø¯Ù‡Ø¯ØŒ Ù…ØªÙ† Ø¢Ù† Ø±Ø§ Ù†Ø´Ø§Ù† Ø¨Ø¯Ù‡ Ú©Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø§Ø² Ø¯Ø³Øª Ù†Ø¯Ù‡Ø¯
#         if not visual and spoken:
#             visual = spoken 
#     else:
#         # Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± ØµØ¯Ø§ Ø±Ø§ Ø¨Ø§Ø² Ú¯Ø°Ø§Ø´ØªÙ‡
#         final_audio_script = spoken
#         # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø®ÙˆØ§Ø³ØªÙ‡ ÙÙ‚Ø· ØµØ¯Ø§ Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© Ù…ØªÙ† Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ú¯Ø°Ø§Ø± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
#         if not visual:
#             visual = "ğŸ”Š (Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ)"

#     return {
#         "messages": [AIMessage(content=visual)], # Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ú†Øª
#         "audio_script": final_audio_script,       # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ù†ÙˆØ¯ ØµØ¯Ø§
#         "retry_count": 0
#     }


# def generate_audio_output(state: AgentState):
#     """
#     Converts 'audio_script' to speech (instead of last message content).
#     """
#     # [ØªØºÛŒÛŒØ±]: Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø² Ù…ØªØºÛŒØ± Ø¬Ø¯ÛŒØ¯ audio_script
#     script = state.get("audio_script")
    
#     if not script:
#         log_step("TTS", "No audio script to generate.")
#         return {"audio_output_path": None}

#     log_step("TTS", f"ğŸ”Š Generating audio ({len(script)} chars)...")

#     audio_path = text_to_speech(
#         text=script,
#         model="gemini-2.5-flash-preview-tts",
#         add_emotion=True,
#     )

#     if audio_path:
#         return {"audio_output_path": audio_path}

#     return {"audio_output_path": None}

# # ---------------------------------------------------------
# # 1. ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Cleanup (Ø§ÛŒÙ† Ø±Ø§ Ù‚Ø¨Ù„ Ø§Ø² create_agent_graph Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯)
# # ---------------------------------------------------------
# def cleanup_node(state: AgentState):
#     """
#     Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡: Ø­Ø°Ù Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø¨Ø²Ø§Ø± Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¨Ø²Ø§Ø±
#     Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù ØªÙˆÚ©Ù† Ùˆ ØªÙ…ÛŒØ² Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† Ú©Ø§Ù†ØªÚ©Ø³Øª.
#     """
#     messages = state["messages"]
#     delete_ops = []
    
#     for msg in messages:
#         # Ø§Ù„Ù) Ø­Ø°Ù Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø¨Ø²Ø§Ø± (ToolMessage)
#         if hasattr(msg, "type") and msg.type == "tool":
#             delete_ops.append(RemoveMessage(id=msg.id))
            
#         # Ø¨) Ø­Ø°Ù Ù¾ÛŒØ§Ù… Ù…Ø¯Ù„ Ú©Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø¨Ø²Ø§Ø± Ú©Ø±Ø¯Ù‡ (AIMessage Ø¯Ø§Ø±Ø§ÛŒ tool_calls)
#         if isinstance(msg, AIMessage) and hasattr(msg, "tool_calls") and len(msg.tool_calls) > 0:
#             delete_ops.append(RemoveMessage(id=msg.id))
            
#     return {"messages": delete_ops}

# # ============================================
# # Graph Construction
# # ============================================
# def create_agent_graph(p_tool, a_tool):
#     """
#     Construct the LangGraph workflow.
    
#     Flow:
#     START -> check_audio -> generate_query_or_respond -> [retrieve OR audio_output OR END]
#     retrieve -> grade_documents -> [generate_answer OR rewrite_question]
#     generate_answer -> [audio_output OR END]
#     rewrite_question -> generate_query_or_respond
#     """
#     global products_tool, articles_tool
#     products_tool = p_tool
#     articles_tool = a_tool

#     workflow = StateGraph(AgentState)

#     # --- Add Nodes ---
#     workflow.add_node("check_audio", check_audio_input)
#     workflow.add_node("generate_query_or_respond", generate_query_or_respond)
#     workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
#     workflow.add_node("rewrite_question", rewrite_question)
#     workflow.add_node("generate_answer", generate_answer)
#     workflow.add_node("audio_output", generate_audio_output)
    
#     # [NEW] Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ÙˆØ¯ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
#     workflow.add_node("cleanup_node", cleanup_node)

#     # --- Add Edges ---
#     workflow.add_edge(START, "check_audio")
#     workflow.add_edge("check_audio", "generate_query_or_respond")

#     # ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
#     workflow.add_conditional_edges(
#         "generate_query_or_respond",
#         custom_router,
#         {
#             "retrieve": "retrieve",
#             "generate_answer": "generate_answer",
#         }
#     )

#     workflow.add_conditional_edges("retrieve", grade_documents)

#     # ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ
#     # [ØªØºÛŒÛŒØ± Ù…Ù‡Ù…]: Ø§Ú¯Ø± Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ ØªÙ…Ø§Ù… Ø´ÙˆØ¯ (END)ØŒ Ø­Ø§Ù„Ø§ Ù…ÛŒâ€ŒÙØ±Ø³ØªÛŒÙ…Ø´ Ø¨Ù‡ cleanup_node
#     workflow.add_conditional_edges(
#         "generate_answer",
#         route_after_answer,
#         {
#             "audio_output": "audio_output", 
#             END: "cleanup_node"  # <--- Ø¨Ù‡ Ø¬Ø§ÛŒ END Ù…ÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
#         }
#     )

#     # Ø¨Ø¹Ø¯ Ø§Ø² ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§ Ù‡Ù… Ø¨Ø§ÛŒØ¯ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯
#     workflow.add_edge("audio_output", "cleanup_node")
    
#     # Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„
#     workflow.add_edge("rewrite_question", "generate_query_or_respond")

#     # [NEW] Ù¾Ø§ÛŒØ§Ù† Ú¯Ø±Ø§Ù Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø§Ø³Øª
#     workflow.add_edge("cleanup_node", END)

#     # Compile with memory
#     memory = MemorySaver()
#     return workflow.compile(checkpointer=memory)

"""
Store Assistant RAG Agent - Core Logic
Optimized for performance, token efficiency, and adaptive UI.
Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÛŒØ§Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ·Ø¨ÛŒÙ‚ Ù‡ÙˆØ´Ù…Ù†Ø¯ (Ù…ØªÙ†/ØµØ¯Ø§).
"""

import os
import base64
from typing import Literal, Optional

# LangChain & AI Imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
    trim_messages,
    BaseMessage,
    RemoveMessage,
)
from langchain_core.tools import tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field

# Local Imports (Handle Docker vs Local paths)
try:
    from config import *
    from tts_handler import text_to_speech
except ImportError:
    from src.config import *
    from src.tts_handler import text_to_speech


# ============================================
# Global Variables
# ============================================
products_tool = None
articles_tool = None


# ============================================
# State Definition
# ============================================
class AgentState(MessagesState):
    """
    ÙˆØ¶Ø¹ÛŒØª (State) Ú¯Ø±Ø§Ù Ø´Ø§Ù…Ù„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ØŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµØ¯Ø§ Ùˆ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ ØªÙ„Ø§Ø´.
    """
    audio_path: Optional[str] = None          # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
    audio_output_path: Optional[str] = None   # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡
    enable_tts: bool = False                  # Ø¢ÛŒØ§ Ú©Ø§Ø±Ø¨Ø± Ø¯Ú©Ù…Ù‡ ØµØ¯Ø§ Ø±Ø§ Ø±ÙˆØ´Ù† Ú©Ø±Ø¯Ù‡ØŸ
    retry_count: int = 0                      # Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ù„Ù‚Ù‡
    audio_script: Optional[str] = None        # Ù…ØªÙ†ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´ÙˆØ¯ (Ø¬Ø¯Ø§ Ø§Ø² Ù…ØªÙ† Ù†Ù…Ø§ÛŒØ´)


# ============================================
# Output Schemas (Pydantic)
# ============================================
class ResponseDecision(BaseModel):
    """
    Ø³Ø§Ø®ØªØ§Ø± ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªÙÚ©ÛŒÚ© Ù…ØªÙ† Ù†Ù…Ø§ÛŒØ´ÛŒ Ùˆ Ú¯ÙØªØ§Ø±ÛŒ.
    """
    visual_text: str = Field(
        description="Text to display in chat bubble (Markdown allowed)."
    )
    spoken_text: str = Field(
        description="Text to be spoken by TTS (Short, conversational)."
    )
    action: Literal["text_only", "voice_only", "hybrid"] = Field(
        description="Display mode: 'text_only', 'voice_only', or 'hybrid'."
    )

class GradeDocuments(BaseModel):
    """Ø³Ø§Ø®ØªØ§Ø± Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ Ù…Ø¯Ø§Ø±Ú©"""
    binary_score: str = Field(description="'yes' or 'no'")


# ============================================
# Helper Functions
# ============================================
def _extract_store_context(store_name: str) -> str:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆØ¹ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø§Ø² Ø±ÙˆÛŒ Ù†Ø§Ù… Ø¢Ù† Ø¨Ø±Ø§ÛŒ Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø§Ù…Ù¾Øª.
    """
    store_lower = store_name.lower()
    
    if any(k in store_lower for k in ["Ù…ÙˆØ¨Ø§ÛŒÙ„", "mobile", "phone", "Ù„Ù¾ØªØ§Ù¾"]):
        return "mobile phone, laptop, and electronics"
    if any(k in store_lower for k in ["Ù„Ø¨Ø§Ø³", "clothing", "fashion"]):
        return "clothing and fashion"
    if any(k in store_lower for k in ["Ú©ØªØ§Ø¨", "book"]):
        return "books and publications"
        
    return f"{store_name} products"


def get_trimmed_history(messages: list[BaseMessage], max_tokens=2000):
    """
    Ú©ÙˆØªØ§Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù ØªÙˆÚ©Ù†.
    ÙÙ‚Ø· Ù¾ÛŒØ§Ù… Ø³ÛŒØ³ØªÙ… Ùˆ Ú†Ù†Ø¯ Ù¾ÛŒØ§Ù… Ø¢Ø®Ø± Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
    """
    return trim_messages(
        messages,
        max_tokens=max_tokens,
        strategy="last",
        token_counter=len,
        include_system=True,
        start_on="human",
    )


def custom_router(state):
    """
    Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ Ù‡ÙˆØ´Ù…Ù†Ø¯:
    1. Ø§Ú¯Ø± Ø§Ø¨Ø²Ø§Ø± ØµØ¯Ø§ Ø²Ø¯Ù‡ Ø´Ø¯Ù‡ -> Retrieve
    2. Ø§Ú¯Ø± Ø§Ø¨Ø²Ø§Ø± Ù†ÛŒØ³Øª (Ú†Øª Ø¹Ù…ÙˆÙ…ÛŒ) -> Generate Answer (Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯)
    """
    last_message = state["messages"][-1]
    
    if hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0:
        return "retrieve"
    
    return "generate_answer"


def route_after_answer(state):
    """
    Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ Ø¨Ø¹Ø¯ Ø§Ø² ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®:
    1. Ø§Ú¯Ø± Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØµÙˆØªÛŒ Ø¯Ø§Ø±ÛŒÙ… -> Audio Output
    2. Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª -> Cleanup Node
    """
    if state.get("enable_tts", False) and state.get("audio_script"):
        return "audio_output"
    return "cleanup_node"


# ============================================
# Setup & Tools
# ============================================
def load_vector_stores():
    """Load ChromaDB vector stores."""
    log_step("LOAD", "Loading vector stores...")
    
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL, 
        api_key=API_KEY, 
        base_url=OPENAI_BASE_URL
    )
    
    products_store = Chroma(
        collection_name=PRODUCTS_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(PRODUCTS_CHROMA_DIR),
    )
    
    articles_store = Chroma(
        collection_name=ARTICLES_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(ARTICLES_CHROMA_DIR),
    )
    
    log_success("Vector stores loaded successfully")
    return products_store, articles_store


def create_retriever_tools(products_store, articles_store):
    """
    Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ (Dynamic k).
    """
    
    # Ù…Ø¯Ù„ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¨Ø²Ø§Ø± Ú©Ù‡ Ø¨Ù‡ LLM Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªØ¹Ø¯Ø§Ø¯ Ø±Ø§ ØªØ¹ÛŒÛŒÙ† Ú©Ù†Ø¯
    class SearchInput(BaseModel):
        query: str = Field(description="Search query string")
        k: int = Field(
            default=2,
            description="Number of results. Use 1 for specific product details, 4 for lists."
        )

    @tool(args_schema=SearchInput)
    def products_retriever_tool(query: str, k: int = 2):
        """Search products database. Returns price, specs, and availability."""
        # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø³Ø®Øª: Ø­Ø¯Ø§Ù‚Ù„ 1 Ùˆ Ø­Ø¯Ø§Ú©Ø«Ø± 4
        safe_k = max(1, min(k, 4))
        retriever = products_store.as_retriever(search_kwargs={"k": safe_k})
        return retriever.invoke(query)

    @tool(args_schema=SearchInput)
    def articles_retriever_tool(query: str, k: int = 2):
        """Search articles and guides."""
        safe_k = max(1, min(k, 4))
        retriever = articles_store.as_retriever(search_kwargs={"k": safe_k})
        return retriever.invoke(query)

    products_retriever_tool.name = "products_retriever"
    articles_retriever_tool.name = "articles_retriever"

    return products_retriever_tool, articles_retriever_tool


# ============================================
# LLM Initialization
# ============================================
def gpt_4o_mini():
    return ChatOpenAI(
        model=CHAT_GPT_MODEL, 
        api_key=API_KEY, 
        base_url=OPENAI_BASE_URL, 
        temperature=0
    )

def gemini_2_flash():
    return ChatGoogleGenerativeAI(
        model=CHAT_GEMINI_MODEL,
        google_api_key=API_KEY,
        transport="rest",
        client_options={"api_endpoint": GOOGLE_BASE_URL},
        temperature=0.7,
    )


# ============================================
# Nodes Logic
# ============================================

def transcribe_audio_file(file_path: str) -> str:
    """
    ØªØ¨Ø¯ÛŒÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Gemini.
    """
    if not file_path or not os.path.exists(file_path):
        return ""
    
    try:
        llm = gemini_2_flash()
        
        # ØªØ´Ø®ÛŒØµ ÙØ±Ù…Øª ÙØ§ÛŒÙ„
        ext = file_path.split('.')[-1]
        mime_type = f"audio/{ext}" if ext in ["mp3", "ogg", "wav", "webm"] else "audio/mp3"

        with open(file_path, "rb") as audio_file:
            audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

        # English Prompt: "Transcribe the audio text only. No extra explanation."
        # ÙØ§Ø±Ø³ÛŒ: ÙÙ‚Ø· Ù…ØªÙ† Ø¯Ø§Ø®Ù„ Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³. Ù‡ÛŒÚ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÙ‡â€ŒØ§ÛŒ Ù†Ø¯Ù‡.
        strict_prompt = "Transcribe the audio text exactly. Do not add any explanations."

        message = HumanMessage(
            content=[
                {"type": "text", "text": strict_prompt},
                {"type": "media", "mime_type": mime_type, "data": audio_b64},
            ]
        )
        
        logger.info(f"{Colors.CYAN}ğŸ¤ Transcribing audio...{Colors.END}")
        response = llm.invoke([message])
        return response.content.strip()
        
    except Exception as e:
        log_error(f"Audio transcription error: {e}")
        return ""


def check_audio_input(state: AgentState):
    """
    Ù†ÙˆØ¯ Ø´Ø±ÙˆØ¹: Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ±ÙˆØ¯ÛŒ ØµÙˆØªÛŒ Ùˆ Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ø¨Ø§ÙØ±Ù‡Ø§.
    """
    audio_path = state.get("audio_path")
    
    # Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯ÙˆØ± Ø¬Ø¯ÛŒØ¯
    reset_dict = {
        "audio_path": None, 
        "audio_output_path": None, 
        "audio_script": None 
    }
    
    if audio_path and os.path.exists(audio_path):
        transcribed_text = transcribe_audio_file(audio_path)
        return {
            "messages": [HumanMessage(content=transcribed_text or "Audio was unclear.")],
            **reset_dict
        }
    
    return reset_dict


def generate_query_or_respond(state: AgentState):
    """
    Ù†ÙˆØ¯ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±Ù†Ø¯Ù‡: Ø¢ÛŒØ§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø¨Ø²Ø§Ø± Ø¯Ø§Ø±ÛŒÙ… ÛŒØ§ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…ØŸ
    """
    log_step("QUERY", "Analyzing request...")

    has_user = any(isinstance(msg, HumanMessage) for msg in state["messages"])
    if not has_user:
        return {"messages": [AIMessage(content="No message received.")]}

    llm = gpt_4o_mini()
    store_context = _extract_store_context(STORE_NAME)
    trimmed_msgs = get_trimmed_history(state["messages"], max_tokens=2000)

    # English Prompt with Persian Context
    # ØªØ±Ø¬Ù…Ù‡ Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡: ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù‡Ø³ØªÛŒ. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„/Ù‚ÛŒÙ…Øª Ø¨ÙˆØ¯ Ø§Ø¨Ø²Ø§Ø± Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†.
    # Ø§Ú¯Ø± Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ (Ø³Ù„Ø§Ù…/ØªØ´Ú©Ø±) Ø¨ÙˆØ¯ØŒ Ù‡ÛŒÚ† Ø§Ø¨Ø²Ø§Ø±ÛŒ ØµØ¯Ø§ Ù†Ø²Ù†.
    system_prompt = f"""You are a smart assistant for "{STORE_NAME}" ({store_context}).

OBJECTIVE: Decide if the user's request requires a database search (Tools) or is just general chat.

RULES:
1. If user asks about products, prices, availability, or specs -> Call the appropriate TOOL.
2. If user engages in general chat (Hello, Thanks, How are you) -> DO NOT call any tools. Just output an empty response to pass control.
3. You represent "{STORE_NAME}".
"""
    messages = [SystemMessage(content=system_prompt)] + trimmed_msgs

    # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ù„Ù‚Ù‡ ØªÚ©Ø±Ø§Ø±
    if state.get("retry_count", 0) >= 2:
        log_warning("Retry limit reached. Passing directly.")
        response = AIMessage(content="")
    else:
        if products_tool and articles_tool:
            response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)
        else:
            response = llm.invoke(messages)

    # Ø§Ú¯Ø± Ø§Ø¨Ø²Ø§Ø± ØµØ¯Ø§ Ø²Ø¯ØŒ Ù…Ø³ÛŒØ± Ú¯Ø±Ø§Ù Ø¨Ù‡ Ø³Ù…Øª Retrieve Ù…ÛŒâ€ŒØ±ÙˆØ¯
    if hasattr(response, "tool_calls") and len(response.tool_calls) > 0:
        return {"messages": [response]}
    
    # Ø§Ú¯Ø± Ø§Ø¨Ø²Ø§Ø± ØµØ¯Ø§ Ù†Ø²Ø¯ØŒ Ù¾ÛŒØ§Ù… Ø®Ø§Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ ØªØ§ Ù†ÙˆØ¯ generate_answer Ù¾Ø§Ø³Ø® Ø¯Ù‡Ø¯
    return {"messages": []}


def grade_documents(state: AgentState) -> Literal["generate_answer", "rewrite_question"]:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø¯Ø§Ø±Ú© Ø¨Ø§ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±.
    """
    log_step("GRADE", "Grading documents...")

    current_retry = state.get("retry_count", 0)
    if current_retry >= 1:
        return "generate_answer"

    tool_msgs = [msg for msg in state["messages"] if hasattr(msg, "type") and msg.type == "tool"]
    if not tool_msgs:
        return "rewrite_question"

    llm = gpt_4o_mini()
    question = state["messages"][0].content
    
    # [Ù…Ù‡Ù…] Ú©Ø§Ù†ØªÚ©Ø³Øª Ø²ÛŒØ§Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù† Ù‡Ù…Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª
    context_preview = "\n".join([msg.content for msg in tool_msgs])

    # English Prompt
    # ØªØ±Ø¬Ù…Ù‡: Ø³ÙˆØ§Ù„ Ùˆ Ù…ØªÙ† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±Ø§ Ø¨Ø¨ÛŒÙ†. Ø¢ÛŒØ§ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ù…ÙÛŒØ¯ Ø§Ø³ØªØŸ Ø¨Ù„Ù‡/Ø®ÛŒØ±
    grade_prompt = f"""User Question: {question}
Retrieved Context: {context_preview}

Is this context relevant and useful to answer the user's question? 
Reply ONLY 'yes' or 'no'."""

    response = llm.invoke([{"role": "user", "content": grade_prompt}])

    if "yes" in response.content.lower():
        return "generate_answer"
    else:
        return "rewrite_question"


def rewrite_question(state: AgentState):
    """
    Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ±.
    """
    log_step("REWRITE", "Rewriting query...")
    new_count = state.get("retry_count", 0) + 1
    llm = gpt_4o_mini()

    messages = state["messages"]
    last_human = next((m for m in reversed(messages) if isinstance(m, HumanMessage)), messages[-1])
    original_q = last_human.content

    # English Prompt
    # ØªØ±Ø¬Ù…Ù‡: Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ù‡ØªØ± Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†.
    msg = f"""Rewrite this question to be a better semantic search query for a product database.
Output ONLY the rewritten query, no explanations.
Original: {original_q}"""

    response = llm.invoke(msg)
    logger.info(f"Original: {original_q} -> Rewritten: {response.content}")

    return {
        "messages": [HumanMessage(content=response.content)],
        "retry_count": new_count,
    }


def generate_answer(state: AgentState):
    """
    Ù…ØºØ² Ù…ØªÙÚ©Ø± Ú¯Ø±Ø§Ù: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ (JSON).
    ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ/ØµÙˆØªÛŒ/ØªØ±Ú©ÛŒØ¨ÛŒ.
    """
    log_step("ANSWER", "Generating smart response...")
    
    llm = gpt_4o_mini().with_structured_output(ResponseDecision)

    # ÛŒØ§ÙØªÙ† Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ
    question = "Unknown"
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            question = msg.content
            break

    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ØªÚ©Ø³Øª (Ø§ÙØ²Ø§ÛŒØ´ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ±)
    tool_contents = [msg.content for msg in state["messages"] if hasattr(msg, "type") and msg.type == "tool"]
    full_context = "\n\n".join(tool_contents)[:15000] # Safe limit

    # English Prompt with Logic Rules
    # ØªØ±Ø¬Ù…Ù‡ Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡: ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù‡Ø³ØªÛŒ. Ø¨Ù‡ TTS Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±ÛŒ.
    # Ù‚ÙˆØ§Ù†ÛŒÙ†: Ø¨Ø±Ø§ÛŒ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ø² 'visual_text' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†. Ø¨Ø±Ø§ÛŒ ÙˆÛŒØ³ Ø§Ø² 'spoken_text' Ø®Ù„Ø§ØµÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
    # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ: Ú†Øª Ø¹Ù…ÙˆÙ…ÛŒ -> ÙÙ‚Ø· ÙˆÛŒØ³. Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„ -> ØªØ±Ú©ÛŒØ¨ÛŒ. Ø¯Ø±Ø®ÙˆØ§Ø³Øª ÙˆÛŒØ³ -> ÙÙ‚Ø· ÙˆÛŒØ³.
     # [ØªØºÛŒÛŒØ± Ø§Ø³Ø§Ø³ÛŒ]: Ù¾Ø±Ø§Ù…Ù¾Øª Ø¶Ø¯ ØªÙˆÙ‡Ù… (Anti-Hallucination Prompt)
    answer_prompt = f"""You are a smart, multimodal assistant for "{STORE_NAME}".
You have access to a TTS engine. NEVER say "I cannot send voice".

User Question: {question}
Context Info: {full_context}

CRITICAL RULES (ANTI-HALLUCINATION):
1. **CHECK EXISTENCE:** You must ONLY provide price/stock for products explicitly listed in 'Context Info'.
2. **NO INVENTION:** If the user asks for 'Product A', and 'Context Info' only contains 'Product B':
   - You MUST say: "Product A is currently not available in our store."
   - Then you can suggest 'Product B' from the context as an alternative.
   - **NEVER** generate specs, price, or "Available" status for 'Product A' if it is not in the context.
3. **EXACT MATCH:** Be careful with model numbers (e.g., 'Pro', 'X6' vs 'X7'). If the exact model is missing, it is NOT available.

DECISION RULES (JSON Output):
1. **visual_text**: Content shown in chat bubble.
2. **spoken_text**: Content read aloud (Short summary).
3. **action**: 'text_only', 'voice_only', or 'hybrid'.

STRATEGY:
- **Product Found:** Show details from context. Action: 'hybrid'.
- **Product NOT Found (But similars exist):** Say "We don't have [Requested Model], but we have these similar models:" -> Show similar models from context. Action: 'hybrid'.
- **General Chat:** Warm greeting. Action: 'voice_only'.

Language: Persian (Farsi).
"""

    response: ResponseDecision = llm.invoke([{"role": "user", "content": answer_prompt}])
    
    visual = response.visual_text
    spoken = response.spoken_text
    
    # --- Logic Matrix: Check User Permission ---
    user_allows_tts = state.get("enable_tts", False)
    final_audio_script = None
    
    if not user_allows_tts:
        # Ú©Ø§Ø±Ø¨Ø± ØµØ¯Ø§ Ø±Ø§ Ø¨Ø³ØªÙ‡ Ø§Ø³Øª
        final_audio_script = None
        # Ø§Ú¯Ø± Ù…Ø¯Ù„ ÙÙ‚Ø· ØµØ¯Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ Ù…ØªÙ† Ø¢Ù† Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¨Ø¯Ù‡
        if not visual and spoken:
            visual = spoken 
    else:
        # Ú©Ø§Ø±Ø¨Ø± ØµØ¯Ø§ Ø±Ø§ Ø¨Ø§Ø² Ú¯Ø°Ø§Ø´ØªÙ‡
        final_audio_script = spoken
        # Ø§Ú¯Ø± Ù…Ø¯Ù„ ÙÙ‚Ø· ØµØ¯Ø§ Ø®ÙˆØ§Ø³ØªÙ‡ØŒ ÛŒÚ© Ù…ØªÙ† Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ú¯Ø°Ø§Ø±
        if not visual:
            visual = "ğŸ”Š (Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ)"

    return {
        "messages": [AIMessage(content=visual)],
        "audio_script": final_audio_script,
        "retry_count": 0
    }


def generate_audio_output(state: AgentState):
    """
    ØªÙˆÙ„ÛŒØ¯ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø§Ø² Ø±ÙˆÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡.
    """
    script = state.get("audio_script")
    
    if not script:
        return {"audio_output_path": None}

    log_step("TTS", f"ğŸ”Š Generating audio ({len(script)} chars)...")

    audio_path = text_to_speech(
        text=script,
        model="gemini-2.5-flash-preview-tts",
        add_emotion=True,
    )
    return {"audio_output_path": audio_path}


def cleanup_node(state: AgentState):
    """
    Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ (Cleanup): Ø­Ø°Ù Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¨Ø²Ø§Ø±.
    Ø§ÛŒÙ† Ù†ÙˆØ¯ Ø¨Ø§Ø¹Ø« Ú©Ø§Ù‡Ø´ Ø´Ø¯ÛŒØ¯ Ù…ØµØ±Ù ØªÙˆÚ©Ù† Ø¯Ø± Ø¯ÙˆØ±Ù‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    """
    messages = state["messages"]
    delete_ops = []
    
    for msg in messages:
        # 1. Ø­Ø°Ù Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø¨Ø²Ø§Ø±
        if hasattr(msg, "type") and msg.type == "tool":
            delete_ops.append(RemoveMessage(id=msg.id))
            
        # 2. Ø­Ø°Ù Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø¨Ø²Ø§Ø±
        if isinstance(msg, AIMessage) and hasattr(msg, "tool_calls") and len(msg.tool_calls) > 0:
            delete_ops.append(RemoveMessage(id=msg.id))
            
    return {"messages": delete_ops}


# ============================================
# Graph Construction
# ============================================
def create_agent_graph(p_tool, a_tool):
    """
    Ø³Ø§Ø®Øª Ú¯Ø±Ø§Ù Ù†Ù‡Ø§ÛŒÛŒ LangGraph.
    Ù…Ø³ÛŒØ±: Start -> Check Audio -> Decision -> [Retrieve/Answer] -> ... -> Cleanup -> End
    """
    global products_tool, articles_tool
    products_tool = p_tool
    articles_tool = a_tool

    workflow = StateGraph(AgentState)

    # --- Nodes ---
    workflow.add_node("check_audio", check_audio_input)
    workflow.add_node("generate_query_or_respond", generate_query_or_respond)
    workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
    workflow.add_node("rewrite_question", rewrite_question)
    workflow.add_node("generate_answer", generate_answer)
    workflow.add_node("audio_output", generate_audio_output)
    workflow.add_node("cleanup_node", cleanup_node) # Ù†ÙˆØ¯ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ

    # --- Edges ---
    workflow.add_edge(START, "check_audio")
    workflow.add_edge("check_audio", "generate_query_or_respond")

    # ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ (Ø§Ø¨Ø²Ø§Ø± ÛŒØ§ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…)
    workflow.add_conditional_edges(
        "generate_query_or_respond",
        custom_router,
        {
            "retrieve": "retrieve",
            "generate_answer": "generate_answer",
        }
    )

    workflow.add_conditional_edges("retrieve", grade_documents)

    # Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ -> ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§ ÛŒØ§ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
    workflow.add_conditional_edges(
        "generate_answer",
        route_after_answer,
        {
            "audio_output": "audio_output", 
            "cleanup_node": "cleanup_node"
        }
    )

    workflow.add_edge("audio_output", "cleanup_node")
    workflow.add_edge("rewrite_question", "generate_query_or_respond")
    workflow.add_edge("cleanup_node", END) # Ù¾Ø§ÛŒØ§Ù† Ú¯Ø±Ø§Ù Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø§Ø³Øª

    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)