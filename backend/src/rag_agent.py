"""
Store Assistant RAG Agent - Logic Core (Optimized)
Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù ØªÙˆÚ©Ù† Ùˆ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ù„Ù‚Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒâ€ŒÙ¾Ø§ÛŒØ§Ù†.
"""

import os
import base64
from typing import Literal, Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
    trim_messages,
    BaseMessage
)
from langchain_core.tools import tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field

try:
    from config import *
    from tts_handler import text_to_speech  # ğŸ†• Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†

except ImportError:
    from src.config import *
    from src.tts_handler import text_to_speech  # ğŸ†• Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†


# ============================================
# Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ
# ============================================
products_tool = None
articles_tool = None

# ============================================
# ØªØ¹Ø±ÛŒÙ State (Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡)
# ============================================
class AgentState(MessagesState):
    """State Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¯Ø±ÛŒØ§ÙØª ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ùˆ Ø´Ù…Ø§Ø±Ø´ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§"""
    audio_path: Optional[str] = None
    audio_output_path: Optional[str] = None  # ğŸ†• Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ ØµÙˆØªÛŒ
    enable_tts: bool = False  # ğŸ†• Ú©Ù†ØªØ±Ù„ ÙØ¹Ø§Ù„/ØºÛŒØ±ÙØ¹Ø§Ù„
    retry_count: int = 0

# ============================================
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ (Helper Functions)
# ============================================
def get_trimmed_history(messages: list[BaseMessage], max_tokens=2000):
    """
    ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø±Ø§ Ø¨Ù‡ Ø´Ø¯Øª Ú©ÙˆØªØ§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¯Ø± Ù‡Ø²ÛŒÙ†Ù‡ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø´ÙˆØ¯.
    ÙÙ‚Ø· Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø§Ù…Ù¾Øª + Ú†Ù†Ø¯ Ù¾ÛŒØ§Ù… Ø¢Ø®Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±Ø¯.
    """
    return trim_messages(
        messages,
        max_tokens=max_tokens,
        strategy="last",
        token_counter=len, # Ø´Ù…Ø§Ø±Ø´ Ø­Ø¯ÙˆØ¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…
        include_system=True,
        start_on="human"
    )

# ============================================
# Ø¨Ø®Ø´ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores
# ============================================
def load_vector_stores():
    log_step("LOAD", "Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores...")
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL
    )
    products_store = Chroma(
        collection_name=PRODUCTS_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(PRODUCTS_CHROMA_DIR),
    )
    articles_store = Chroma(
        collection_name=ARTICLES_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(ARTICLES_CHROMA_DIR),
    )
    log_success("Vector stores Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    return products_store, articles_store

# ============================================
# Ø¨Ø®Ø´ 2: Ø³Ø§Ø®Øª Retriever Tools
# ============================================
def create_retriever_tools(products_store, articles_store):
    # k=2 Ú©Ø±Ø¯ÛŒÙ… Ú©Ù‡ ØªÙˆÚ©Ù† Ú©Ù…ØªØ±ÛŒ Ù…ØµØ±Ù Ø¨Ø´Ù‡ (Ù‚Ø¨Ù„Ø§ 3 Ø¨ÙˆØ¯)
    products_retriever = products_store.as_retriever(search_kwargs={"k": 2})
    articles_retriever = articles_store.as_retriever(search_kwargs={"k": 2})

    @tool
    def products_retriever_tool(query: str):
        """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª (Ù…ÙˆØ¨Ø§ÛŒÙ„ØŒ Ù„Ù¾ØªØ§Ù¾ Ùˆ...). Ù‚ÛŒÙ…Øª Ùˆ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        return products_retriever.invoke(query)

    @tool
    def articles_retriever_tool(query: str):
        """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø®Ø±ÛŒØ¯."""
        return articles_retriever.invoke(query)

    products_retriever_tool.name = "products_retriever"
    articles_retriever_tool.name = "articles_retriever"

    return products_retriever_tool, articles_retriever_tool

# ============================================
# Ø¨Ø®Ø´ 3: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ
# ============================================
class GradeDocuments(BaseModel):
    binary_score: str = Field(description="'yes' or 'no'")

def gpt_4o_mini():
    return ChatOpenAI(
        model=CHAT_GPT_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL, temperature=0
    )

def gemini_2_flash():
    return ChatGoogleGenerativeAI(
        model=CHAT_GEMINI_MODEL,
        google_api_key=API_KEY,
        transport="rest",
        client_options={"api_endpoint": GOOGLE_BASE_URL},
        temperature=0.7,
    )

# ============================================
# Ø¨Ø®Ø´ 4: Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª
# ============================================
def transcribe_audio_file(file_path: str) -> str:
    if not file_path or not os.path.exists(file_path):
        return ""
    try:
        llm = gemini_2_flash()
        mime_type = "audio/mp3"
        if file_path.endswith(".ogg"): mime_type = "audio/ogg"
        elif file_path.endswith(".wav"): mime_type = "audio/wav"
        elif file_path.endswith(".webm"): mime_type = "audio/webm"

        with open(file_path, "rb") as audio_file:
            audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

        # Ù¾Ø±Ø§Ù…Ù¾Øª Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ ØªÙˆÚ©Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¬Ù…ÛŒÙ†Ø§ÛŒ
        strict_prompt = "ÙÙ‚Ø· Ù…ØªÙ† Ø§ÛŒÙ† ØµÙˆØª Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ (Transcription). Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÙ‡."
        
        message = HumanMessage(
            content=[
                {"type": "text", "text": strict_prompt},
                {"type": "media", "mime_type": mime_type, "data": audio_b64},
            ]
        )
        logger.info(f"{Colors.CYAN}ğŸ¤ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§...{Colors.END}")
        response = llm.invoke([message])
        return response.content.strip()
    except Exception as e:
        log_error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§: {e}")
        return ""

def check_audio_input(state: AgentState):
    audio_path = state.get("audio_path")
    if audio_path and os.path.exists(audio_path):
        transcribed_text = transcribe_audio_file(audio_path)
        if transcribed_text:
            return {"messages": [HumanMessage(content=transcribed_text)], "audio_path": None}
        else:
            return {
                "messages": [HumanMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ ØµØ¯Ø§ ÙˆØ§Ø¶Ø­ Ù†Ø¨ÙˆØ¯.")],
                "audio_path": None
            }    
    return {}

# ============================================
# Ø¨Ø®Ø´ 5: Agent Nodes (Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡)
# ============================================

def generate_query_or_respond(state: AgentState):
    """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ: Ø¬Ø³ØªØ¬Ùˆ ÛŒØ§ Ù¾Ø§Ø³Ø®"""
    log_step("QUERY", "ØªØ­Ù„ÛŒÙ„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª...")
    
    # Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ù‡Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±
    # (Ø§Ú¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ù…Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§Ø´Ù‡ØŒ ÛŒØ¹Ù†ÛŒ Ø´Ø±ÙˆØ¹ Ø³ÛŒÚ©Ù„ Ø¬Ø¯ÛŒØ¯Ù‡)
    if isinstance(state["messages"][-1], HumanMessage):
        # Ø§Ù…Ø§ Ú†ÙˆÙ† State Ø§ÛŒÙ…ÛŒÙˆØªØ¨Ù„ Ù†ÛŒØ³ØªØŒ Ø§ÛŒÙ†Ø¬Ø§ ÙÙ‚Ø· Ù¾Ø§Ø³ Ù…ÛŒØ¯ÛŒÙ…ØŒ Ø±ÛŒØ³Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ÛŒØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ØªØ± Ø¨Ø§Ø´Ù‡
        # ÙØ¹Ù„Ø§ ÙØ±Ø¶ Ù…ÛŒÚ©Ù†ÛŒÙ… Ø§Ú¯Ø± human message Ø¯ÛŒØ¯ÛŒÙ… ÛŒØ¹Ù†ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø­Ø±Ù Ø²Ø¯Ù‡
        pass 

    has_user = any(isinstance(msg, HumanMessage) for msg in state["messages"])
    if not has_user:
        return {"messages": [AIMessage(content="Ù¾ÛŒØ§Ù…ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")]}

    llm = gpt_4o_mini()
    
    # Ù¾Ø±Ø§Ù…Ù¾Øª ÙØ´Ø±Ø¯Ù‡â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ ØªÙˆÚ©Ù†
    system_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù‡Ø³ØªÛŒ.
ÙˆØ¸Ø§ÛŒÙ: Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø­ØµÙˆÙ„Ø§ØªØŒ Ù‚ÛŒÙ…Øª Ùˆ Ù…ÙˆØ¬ÙˆØ¯ÛŒ.
Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§: products_retriever, articles_retriever.
Ù‚ÙˆØ§Ù†ÛŒÙ†:
1. ÙÙ‚Ø· Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ú¯ÛŒØ±.
2. Ø§Ú¯Ø± Ø¯Ø± Ø§Ø¨Ø²Ø§Ø± Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ú¯Ùˆ "Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±ÛŒÙ…" (Ø¯Ø±ÙˆØº Ù†Ú¯Ùˆ).
3. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø¹Ù…ÙˆÙ…ÛŒ Ø¨ÙˆØ¯ØŒ Ø®ÙˆØ¯Øª Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡."""

    # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø´Ø¯ÛŒØ¯ Ø±ÙˆÛŒ ØªØ§Ø±ÛŒØ®Ú†Ù‡ (ÙÙ‚Ø· 4-5 Ù¾ÛŒØ§Ù… Ø¢Ø®Ø±)
    trimmed_msgs = get_trimmed_history(state["messages"], max_tokens=2000)
    messages = [SystemMessage(content=system_prompt)] + trimmed_msgs

    # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø²ÛŒØ§Ø¯ Ø´Ø¯Ù‡ØŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒØ¨Ù†Ø¯ÛŒÙ… Ú©Ù‡ Ø¯ÛŒÚ¯Ù‡ Ø³Ø±Ú† Ù†Ú©Ù†Ù‡
    if state.get("retry_count", 0) >= 2:
        log_warning("ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´ Ø²ÛŒØ§Ø¯ Ø´Ø¯. Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø²Ø§Ø±.")
        response = llm.invoke(messages) # Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø²Ø§Ø±
    else:
        if products_tool and articles_tool:
            response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)
        else:
            response = llm.invoke(messages)

    return {"messages": [response]}


def grade_documents(state: AgentState) -> Literal["generate_answer", "rewrite_question"]:
    """Ú©ÛŒÙÛŒØª Ø³Ù†Ø¬ÛŒ Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø­Ù„Ù‚Ù‡"""
    log_step("GRADE", "Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ø§Ø±Ú©...")
    
    # 1. Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø¨ÛŒØ´ØªØ± Ø§Ø² 1 Ø¨Ø§Ø± Ø´Ø¯Ù‡ØŒ Ø¯ÛŒÚ¯Ù‡ Ø³Ø®Øª Ù†Ú¯ÛŒØ± Ùˆ Ø¨Ø±Ùˆ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡
    # (Ø­ØªÛŒ Ø§Ú¯Ø± Ù…Ø¯Ø§Ø±Ú© Ø¹Ø§Ù„ÛŒ Ù†ÛŒØ³ØªØŒ Ø¨Ù‡ØªØ± Ø§Ø² Ù‡ÛŒÚ†ÛŒÙ‡ ÛŒØ§ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ú¯Ù‡ Ù†Ø¯Ø§Ø±Ù…)
    current_retry = state.get("retry_count", 0)
    if current_retry >= 1:
        log_warning(f"ØªÙ„Ø§Ø´ {current_retry}: Ø¹Ø¨ÙˆØ± Ø§Ø² Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±ÛŒ.")
        return "generate_answer"

    tool_msgs = [msg for msg in state["messages"] if hasattr(msg, 'type') and msg.type == 'tool']
    if not tool_msgs:
        return "rewrite_question"

    llm = gpt_4o_mini()
    question = state["messages"][0].content
    
    # ÙÙ‚Ø· 1000 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„ Ù…Ø¯Ø§Ø±Ú© Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ú†Ú© Ú©Ø±Ø¯Ù† Ø¨ÙØ±Ø³Øª (ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ)
    context_preview = "\n".join([msg.content[:1000] for msg in tool_msgs])

    grade_prompt = f"""Ø³ÙˆØ§Ù„: {question}
Ù…Ø¯Ø§Ø±Ú©: {context_preview}
Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…Ø¯Ø§Ø±Ú© Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ø±Ø¨Ø· Ø¯Ø§Ø±Ù†Ø¯ØŸ (yes/no)"""
    
    response = llm.invoke([{"role": "user", "content": grade_prompt}])
    
    if "yes" in response.content.lower():
        return "generate_answer"
    else:
        return "rewrite_question"


def rewrite_question(state: AgentState):
    """Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ (Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡)"""
    log_step("REWRITE", "ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯...")
    
    # Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡
    new_count = state.get("retry_count", 0) + 1
    
    llm = gpt_4o_mini()
    original_q = state["messages"][0].content
    
    msg = f"Ø³ÙˆØ§Ù„ '{original_q}' Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ± Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù† (ÙÙ‚Ø· Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯)."
    response = llm.invoke(msg)
    
    logger.info(f"{Colors.GREEN}Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯ ({new_count}): {response.content}{Colors.END}")
    
    return {
        "messages": [HumanMessage(content=response.content)],
        "retry_count": new_count # Ø¢Ù¾Ø¯ÛŒØª Ø§Ø³ØªÛŒØª
    }


def generate_answer(state: AgentState):
    """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ú©Ø§Ù†ØªÚ©Ø³Øª Ù…Ø­Ø¯ÙˆØ¯"""
    log_step("ANSWER", "ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®...")
    llm = gpt_4o_mini()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ (Ù†Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡â€ŒÙ‡Ø§)
    # Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø§ÙˆÙ„ÛŒÙ† HumanMessage Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒÙ‡ØŒ ÛŒØ§ Ø¢Ø®Ø±ÛŒÙ† Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¨Ø²Ø§Ø±
    question = "Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±"
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            question = msg.content
            break

    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ù…Ø­Ø¯ÙˆØ¯Ø³Ø§Ø²ÛŒ Ù…Ø¯Ø§Ø±Ú©
    tool_contents = []
    for msg in state["messages"]:
        if hasattr(msg, "type") and msg.type == "tool":
            # ÙÙ‚Ø· 500 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø² Ù‡Ø± Ù…Ø¯Ø±Ú© Ø±Ùˆ Ø¨Ø±Ø¯Ø§Ø± (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ù†ÙØ¬Ø§Ø± ØªÙˆÚ©Ù†)
            # Ø§Ú¯Ø± Ù…Ø­ØµÙˆÙ„Ù‡ØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù… Ø§ÙˆÙ„Ø´Ù‡.
            tool_contents.append(msg.content[:800]) 

    # Ú©Ù„ Ú©Ø§Ù†ØªÚ©Ø³Øª Ø±Ùˆ Ù‡Ù… Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù† Ø¨Ù‡ 3000 Ú©Ø§Ø±Ø§Ú©ØªØ±
    full_context = "\n\n".join(tool_contents)[:3000]
    
    logger.info(f"{Colors.CYAN}Ø·ÙˆÙ„ Ú©Ø§Ù†ØªÚ©Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ: {len(full_context)} Ú©Ø§Ø±Ø§Ú©ØªØ±{Colors.END}")

    answer_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± {STORE_NAME} Ù‡Ø³ØªÛŒ.
Ø³ÙˆØ§Ù„: {question}
Ø§Ø·Ù„Ø§Ø¹Ø§Øª:
{full_context}

Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„:
1. ÙÙ‚Ø· Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ù„Ø§ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡.
2. Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†ÛŒØ³ØªØŒ Ø¨Ú¯Ùˆ "Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†Ø¯Ø§Ø±Ù…".
3. Ø®Ù„Ø§ØµÙ‡ Ùˆ Ù…ÙÛŒØ¯ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡.
4. Ø¨Ø§ Ù„Ø­Ù† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡ Ùˆ Ø³Ø¹ÛŒ Ú©Ù† Ø§Ø² (ØŒ) Ùˆ (.) Ùˆ Ø¹Ù„Ø§Ø¦Ù… Ø¯ÛŒÚ¯Ù‡ Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ """

    response = llm.invoke([{"role": "user", "content": answer_prompt}])
    
    # Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ø³Ø® Ø¯Ø§Ø¯Ù†ØŒ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ø±Ùˆ ØµÙØ± Ú©Ù† Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø¹Ø¯ÛŒ
    return {"messages": [response], "retry_count": 0}


def generate_audio_output(state: AgentState):
    """
    Ù†ÙˆØ¯ Ø®Ø±ÙˆØ¬ÛŒ: ØªØ¨Ø¯ÛŒÙ„ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ ØµÙˆØª
    ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ enable_tts=True Ø¨Ø§Ø´Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒØ´Ù‡
    """
    
    # Ú†Ú© Ú©Ø±Ø¯Ù† ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù† TTS
    if not state.get("enable_tts", False):
        log_step("TTS", "Ø®Ø±ÙˆØ¬ÛŒ ØµÙˆØªÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª")
        return {}
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ù¾Ø§Ø³Ø® AI
    last_ai_message = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, AIMessage):
            last_ai_message = msg
            break
    
    if not last_ai_message or not last_ai_message.content:
        log_warning("Ù¾ÛŒØ§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØµÙˆØª ÛŒØ§ÙØª Ù†Ø´Ø¯")
        return {}
    
    log_step("TTS", "ğŸ”Š Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ ØµÙˆØªÛŒ...")
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØµÙˆØª
    audio_path = text_to_speech(
        text=last_ai_message.content,
        model="gemini-2.5-flash-preview-tts",
        add_emotion=True  # Ù„Ø­Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡
    )
    
    if audio_path:
        return {"audio_output_path": audio_path}
    
    return {}


# ============================================
# Ø¨Ø®Ø´ 6: Ø³Ø§Ø®Øª Graph
# ============================================
def create_agent_graph(p_tool, a_tool):
    global products_tool, articles_tool
    products_tool = p_tool
    articles_tool = a_tool

    workflow = StateGraph(AgentState)
    
    workflow.add_node("check_audio", check_audio_input)
    workflow.add_node("generate_query_or_respond", generate_query_or_respond)
    workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
    workflow.add_node("rewrite_question", rewrite_question)
    workflow.add_node("generate_answer", generate_answer)
    workflow.add_node("audio_output", generate_audio_output)# ğŸ†• Ù†ÙˆØ¯ Ø¬Ø¯ÛŒØ¯ TTS

    workflow.add_edge(START, "check_audio")
    workflow.add_edge("check_audio", "generate_query_or_respond")
    
    # ğŸ”´ Ø§ØµÙ„Ø§Ø­ Ù…Ù‡Ù… Ø§ÛŒÙ†Ø¬Ø§Ø³Øª:
    # Ø§Ú¯Ø± Ø§Ø¨Ø²Ø§Ø± Ø®ÙˆØ§Ø³Øª -> Ø¨Ø±Ùˆ retrieve
    # Ø§Ú¯Ø± ØªÙ…Ø§Ù… Ø´Ø¯ (Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø§Ø¯) -> Ø¨Ø±Ùˆ audio_output (Ù†Ù‡ END)
    workflow.add_conditional_edges(
        "generate_query_or_respond", 
        tools_condition, 
        {"tools": "retrieve", END: "audio_output"} 
    )
    
    workflow.add_conditional_edges("retrieve", grade_documents)
    
    workflow.add_edge("generate_answer", "audio_output")
    workflow.add_edge("audio_output", END) # Ù¾Ø§ÛŒØ§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª
    workflow.add_edge("rewrite_question", "generate_query_or_respond")

    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# """
# Store Assistant RAG Agent - Logic Core
# Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ù…Ù†Ø·Ù‚ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ Ú¯Ø±Ø§Ù Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§Ø³Øª.
# Ù‡ÛŒÚ† UI ÛŒØ§ Ø³Ø±ÙˆØ±ÛŒ Ø¯Ø± Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø§Ø¬Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.
# """

# import os
# import base64
# from typing import Literal, Optional
# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_chroma import Chroma
# from langchain_core.messages import (
#     SystemMessage,
#     HumanMessage,
#     AIMessage,
#     trim_messages,
# )
# from langchain_core.tools import tool  # <--- Ø§ÛŒÙ† Ù…Ù‡Ù…Ù‡: Ø§ÛŒÙ…Ù¾ÙˆØ±Øª tool
# from langgraph.graph import StateGraph, MessagesState, START, END
# from langgraph.prebuilt import ToolNode, tools_condition
# from langgraph.checkpoint.memory import MemorySaver
# from pydantic import BaseModel, Field

# try:
#     from config import *
# except ImportError:
#     from src.config import *


# products_tool = None
# articles_tool = None
# # ============================================
# # ØªØ¹Ø±ÛŒÙ State
# # ============================================
# class AgentState(MessagesState):
#     audio_path: Optional[str] = None

# # ============================================
# # Ø¨Ø®Ø´ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores
# # ============================================
# def load_vector_stores():
#     log_step("LOAD", "Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores...")
#     embeddings = OpenAIEmbeddings(
#         model=EMBEDDING_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL
#     )
#     products_store = Chroma(
#         collection_name=PRODUCTS_COLLECTION,
#         embedding_function=embeddings,
#         persist_directory=str(PRODUCTS_CHROMA_DIR),
#     )
#     articles_store = Chroma(
#         collection_name=ARTICLES_COLLECTION,
#         embedding_function=embeddings,
#         persist_directory=str(ARTICLES_CHROMA_DIR),
#     )
#     log_success("Vector stores Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
#     return products_store, articles_store

# # ============================================
# # Ø¨Ø®Ø´ 2: Ø³Ø§Ø®Øª Retriever Tools
# # ============================================
# def create_retriever_tools(products_store, articles_store):
#     """Ø³Ø§Ø®Øª Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ (Ø±ÙˆØ´ ØµØ±ÛŒØ­ Ø¨Ø§ @tool Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø§Ø±ÙˆØ± TypeError)"""
    
#     # ØªØ¹Ø±ÛŒÙ Ø±ØªØ±ÛŒÙˆØ±Ù‡Ø§
#     products_retriever = products_store.as_retriever(search_kwargs={"k": RETRIEVAL_K})
#     articles_retriever = articles_store.as_retriever(search_kwargs={"k": RETRIEVAL_K})

#     # 1. ØªØ¹Ø±ÛŒÙ Ø§Ø¨Ø²Ø§Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ù‡ ØµÙˆØ±Øª ØªØ§Ø¨Ø¹ ØµØ±ÛŒØ­
#     @tool
#     def products_retriever_tool(query: str):
#         """Ø§Ø¨Ø²Ø§Ø± Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡. Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„Ø§ØªØŒ Ù‚ÛŒÙ…ØªØŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ùˆ Ù…Ø´Ø®ØµØ§Øª ÙÙ†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†."""
#         return products_retriever.invoke(query)

#     # 2. ØªØ¹Ø±ÛŒÙ Ø§Ø¨Ø²Ø§Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ù‡ ØµÙˆØ±Øª ØªØ§Ø¨Ø¹ ØµØ±ÛŒØ­
#     @tool
#     def articles_retriever_tool(query: str):
#         """Ø§Ø¨Ø²Ø§Ø± Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ø±Ø§Ù‡Ù†Ù…Ø§. Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø®Ø±ÛŒØ¯ØŒ Ù†Ú©Ø§Øª Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†."""
#         return articles_retriever.invoke(query)

#     # ØªÙ†Ø¸ÛŒÙ… Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ (Ø­ÛŒØ§ØªÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ)
#     products_retriever_tool.name = "products_retriever"
#     articles_retriever_tool.name = "articles_retriever"

#     return products_retriever_tool, articles_retriever_tool
# # ============================================
# # Ø¨Ø®Ø´ 3: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ
# # ============================================
# class GradeDocuments(BaseModel):
#     binary_score: str = Field(description="Ø§Ù…ØªÛŒØ§Ø² Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†: 'yes' ÛŒØ§ 'no'")

# def gpt_4o_mini():
#     return ChatOpenAI(
#         model=CHAT_GPT_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL, temperature=0
#     )

# def gemini_2_flash():
#     return ChatGoogleGenerativeAI(
#         model=CHAT_GEMINI_MODEL,
#         google_api_key=API_KEY,
#         transport="rest",
#         client_options={"api_endpoint": GOOGLE_BASE_URL},
#         temperature=0.7,
#     )

# # ============================================
# # Ø¨Ø®Ø´ 4: Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª (Voice Input)
# # ============================================


# def transcribe_audio_file(file_path: str) -> str:
#     """ØªØ¨Ø¯ÛŒÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Gemini"""

#     if not file_path or not os.path.exists(file_path):
#         return ""

#     try:
#         llm = gemini_2_flash()

#         # ØªØ´Ø®ÛŒØµ Mime Type
#         mime_type = "audio/mp3"
#         if file_path.endswith(".ogg"):
#             mime_type = "audio/ogg"
#         elif file_path.endswith(".wav"):
#             mime_type = "audio/wav"

#         # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Base64
#         with open(file_path, "rb") as audio_file:
#             audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

#         # Prompt Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±Ø§Ù†Ù‡
#         strict_prompt = """
#         ÙˆØ¸ÛŒÙÙ‡ ØªÙˆ ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· "Transcription" Ø§Ø³Øª.
#         1. Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ø± Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ´Ù†ÙˆÛŒ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³.
#         2. Ù‡ÛŒÚ† Ø¹Ø¨Ø§Ø±Øª Ø§Ø¶Ø§ÙÙ‡â€ŒØ§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù†.
#         3. Ù„Ø­Ù† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø±Ø§ Ø­ÙØ¸ Ú©Ù†.
#         4. ÙÙ‚Ø· Ù…ØªÙ† Ø®Ø§Ù„Øµ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.
#         """

#         # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù… Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ
#         message = HumanMessage(
#             content=[
#                 {"type": "text", "text": strict_prompt},
#                 {"type": "media", "mime_type": mime_type, "data": audio_b64},
#             ]
#         )

#         logger.info(f"{Colors.CYAN}ğŸ¤ Ø¯Ø± Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§ Ø¨Ù‡ Ù…ØªÙ†...{Colors.END}")
#         response = llm.invoke([message])

#         text = response.content.strip()
#         logger.info(f"{Colors.GREEN}âœ… Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {text}{Colors.END}")
#         return text

#     except Exception as e:
#         log_error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§: {e}")
#         return ""


# def check_audio_input(state: AgentState):
#     """Ù†ÙˆØ¯ ÙˆØ±ÙˆØ¯ÛŒ: Ø¨Ø±Ø±Ø³ÛŒ ØµØ¯Ø§ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ØªÙ†"""

#     audio_path = state.get("audio_path")

#     if audio_path and os.path.exists(audio_path):
#         log_step("AUDIO", "ğŸ¤ Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ...")

#         transcribed_text = transcribe_audio_file(audio_path)

#         if transcribed_text:
#             new_message = HumanMessage(content=transcribed_text)
#             return {"messages": [new_message], "audio_path": None}
#         else:
#             # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ Ù…ØªÙ†ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯
#             return {
#                 "messages": [HumanMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… ØµØ¯Ø§ÛŒ Ø´Ù…Ø§ Ø±Ø§ Ø¨Ø´Ù†ÙˆÙ…. Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")],
#                 "audio_path": None
#             }    

#     log_step("AUDIO", "Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ø§Ø³Øª (Ø¨Ø¯ÙˆÙ† ØµØ¯Ø§)")
#     return {}


# # ============================================
# # Ø¨Ø®Ø´ 5: Agent Nodes
# # ============================================


# def generate_query_or_respond(state: AgentState):
#     """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ: Ù†ÛŒØ§Ø² Ø¨Ù‡ RAG ÛŒØ§ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…"""

#     log_step("QUERY", "ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±...")

#     # 1. Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ: Ø¢ÛŒØ§ Ø§ØµÙ„Ø§Ù‹ Ø³ÙˆØ§Ù„ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
#     # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§Ú¯ "Ø§ÙˆÙ„ÛŒÙ† Ù¾ÛŒØ§Ù… ØµÙˆØªÛŒ" Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
#     has_user_message = any(isinstance(msg, HumanMessage) for msg in state["messages"])
    
#     if not has_user_message:
#         log_warning("Ù‡ÛŒÚ† Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ø§Ø² Ú©Ø§Ø±Ø¨Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ø´Ø§ÛŒØ¯ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯).")
#         return {
#             "messages": [
#                 AIMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ ØµØ¯Ø§ÛŒØªØ§Ù† Ø±Ø§ Ù†Ø´Ù†ÛŒØ¯Ù… ÛŒØ§ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ù…ØªÙ† Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.")
#             ]
#         }

#     # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆØ§Ù„ Ø¹Ø§Ø¯ÛŒ...
#     # llm = gemini_2_flash()
#     llm = gpt_4o_mini() # ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Tool Calling Ø§Ø² GPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

#     system_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù‡Ø³ØªÛŒ.

# ÙˆØ¸Ø§ÛŒÙ ØªÙˆ:
# - Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª (Ù‚ÛŒÙ…ØªØŒ Ù…Ø´Ø®ØµØ§ØªØŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ)
# - Ø§Ø±Ø§Ø¦Ù‡ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡ Ø®Ø±ÛŒØ¯
# - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…Ø®ØªÙ„Ù

# Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³:
# - products_retriever: Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
# - articles_retriever: Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§Ù‡Ø§ Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡

# Ù…Ù‡Ù…:
# - Ù‡ÛŒÚ†â€ŒÙˆÙ‚Øª Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¯Ø§Ø®Ù„ÛŒ Ø®ÙˆØ¯Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†
# - ÙÙ‚Ø· Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ú¯ÛŒØ±
# - Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†Ø¯Ø§Ø±ÛŒØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯Ùˆ
# - Ø¯Ø± Ø³ÙˆØ§Ù„Ø§Øª ØºÛŒØ±Ù…Ø±ØªØ¨Ø·ØŒ Ù…Ø³ÛŒØ± Ú¯ÙØªÚ¯Ùˆ Ø±Ø§ Ø¨Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ùˆ Ø®Ø¯Ù…Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù‡Ø¯Ø§ÛŒØª Ú©Ù†"""

#     # Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡
#     trimmed_messages = trim_messages(
#         state["messages"],
#         max_tokens=1000,
#         strategy="last",
#         token_counter=len,
#         include_system=True,
#     )

#     # Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ (Ø¨Ø§ SystemMessage Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø§ØµÙ„Ø§Ø­ Ú©Ø±Ø¯ÛŒÙ…)
#     messages = [SystemMessage(content=system_prompt)] + trimmed_messages

#     log_step("QUERY", "Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ RAG...")
    
#     # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø¯Ù„
#     response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)

#     # Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† ØªØµÙ…ÛŒÙ… Ù…Ø¯Ù„
#     if hasattr(response, "tool_calls") and response.tool_calls:
#         tool_names = [tc["name"] for tc in response.tool_calls]
#         log_step("QUERY", f"Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø¨Ø²Ø§Ø±: {', '.join(tool_names)}")
#     else:
#         log_step("QUERY", "Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø¯ÙˆÙ† RAG")

#     return {"messages": [response]}


# def grade_documents(
#     state: AgentState,
# ) -> Literal["generate_answer", "rewrite_question"]:
#     """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡"""

#     log_step("GRADE", "Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ù…Ø³ØªÙ†Ø¯Ø§Øª...")

#     llm = gpt_4o_mini()

#     question = None
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, HumanMessage):
#             question = msg.content
#             break

#     tool_contents = []
#     for msg in state["messages"]:
#         if hasattr(msg, "content") and hasattr(msg, "type"):
#             if msg.type == "tool":
#                 tool_contents.append(msg.content)

#     context = "\n\n".join(tool_contents) if tool_contents else ""

#     logger.info(f"{Colors.BLUE}ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len(tool_contents)}{Colors.END}")
#     logger.info(f"{Colors.BLUE}ğŸ“ Ø·ÙˆÙ„ context: {len(context)} Ú©Ø§Ø±Ø§Ú©ØªØ±{Colors.END}")

#     grade_prompt = f"""Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†.

# Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {question}

# Ù…Ø³ØªÙ†Ø¯Ø§Øª: {context}

# Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù†Ø¯?
# - Ø§Ú¯Ø± Ù…Ø±ØªØ¨Ø· Ùˆ Ù…ÙÛŒØ¯ Ù‡Ø³ØªÙ†Ø¯: yes
# - Ø§Ú¯Ø± Ù†Ø§Ù…Ø±ØªØ¨Ø· ÛŒØ§ Ù†Ø§Ú©Ø§ÙÛŒ Ù‡Ø³ØªÙ†Ø¯: no"""

#     response = llm.with_structured_output(GradeDocuments).invoke(
#         [{"role": "user", "content": grade_prompt}]
#     )

#     decision = response.binary_score

#     if decision == "yes":
#         log_success("Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±ØªØ¨Ø· Ø§Ø³Øª â†’ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®")
#         return "generate_answer"
#     else:
#         log_warning("Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù†Ø§Ù…Ø±ØªØ¨Ø· â†’ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„")
#         return "rewrite_question"


# def rewrite_question(state: AgentState):
#     """Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ±"""

#     log_step("REWRITE", "Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„...")

#     llm = gpt_4o_mini()

#     question = None
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, HumanMessage):
#             question = msg.content
#             break

#     logger.info(f"{Colors.YELLOW}â“ Ø³ÙˆØ§Ù„ Ù‚Ø¨Ù„ÛŒ: {question}{Colors.END}")

#     prompt = f"""Ø³ÙˆØ§Ù„ Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‡ ØªØ§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø¨Ø§Ø´Ø¯:

# Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ: {question}

# ÙÙ‚Ø· Ø³ÙˆØ§Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÛŒ."""

#     response = llm.invoke([{"role": "user", "content": prompt}])
#     new_question = response.content

#     logger.info(f"{Colors.GREEN}âœï¸  Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯: {new_question}{Colors.END}")

#     return {"messages": [HumanMessage(content=new_question)]}


# def generate_answer(state: AgentState):
#     """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ"""

#     log_step("ANSWER", "ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ...")

#     llm = gpt_4o_mini()

#     question = None
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, HumanMessage):
#             question = msg.content
#             break

#     tool_contents = []
#     for msg in state["messages"]:
#         if hasattr(msg, "type") and msg.type == "tool":
#             tool_contents.append(msg.content)

#     context = "\n\n".join(tool_contents)

#     logger.info(
#         f"{Colors.CYAN}ğŸ’¬ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§Ø³Ø§Ø³ {len(tool_contents)} Ù…Ø³ØªÙ†Ø¯{Colors.END}"
#     )

#     answer_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù‡Ø³ØªÛŒ.

# Ø¨Ø±Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡:

# Ø³ÙˆØ§Ù„: {question}

# Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯:
# {context}

# Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„:
# - ÙÙ‚Ø· Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
# - Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†ÛŒØ³ØªØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯Ùˆ
# - Ù¾Ø§Ø³Ø® Ø±Ø§ ÙˆØ§Ø¶Ø­ Ùˆ Ù…Ø®ØªØµØ± Ø¨Ù†ÙˆÛŒØ³ (3-5 Ø¬Ù…Ù„Ù‡)"""

#     response = llm.invoke([{"role": "user", "content": answer_prompt}])

#     answer_length = len(response.content)
#     logger.info(f"{Colors.GREEN}âœ… Ù¾Ø§Ø³Ø® ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯ ({answer_length} Ú©Ø§Ø±Ø§Ú©ØªØ±){Colors.END}")

#     return {"messages": [response]}
# # ============================================
# # Ø¨Ø®Ø´ 6: Ø³Ø§Ø®Øª Graph
# # ============================================
# def create_agent_graph(p_tool, a_tool):

#     # Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ
#     global products_tool, articles_tool
    
#     # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù†ÙˆØ¯Ù‡Ø§
#     products_tool = p_tool
#     articles_tool = a_tool

#     workflow = StateGraph(AgentState)
    
#     workflow.add_node("check_audio", check_audio_input)
#     workflow.add_node("generate_query_or_respond", generate_query_or_respond)
#     workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
#     workflow.add_node("rewrite_question", rewrite_question)
#     workflow.add_node("generate_answer", generate_answer)

#     workflow.add_edge(START, "check_audio")
#     workflow.add_edge("check_audio", "generate_query_or_respond")
    
#     workflow.add_conditional_edges(
#         "generate_query_or_respond", tools_condition, {"tools": "retrieve", END: END}
#     )
#     workflow.add_conditional_edges("retrieve", grade_documents)
#     workflow.add_edge("generate_answer", END)
#     workflow.add_edge("rewrite_question", "generate_query_or_respond")

#     memory = MemorySaver()
#     return workflow.compile(checkpointer=memory)

# ============================================
# Ø­Ø°Ù Ú©Ø§Ù…Ù„ Ø¨Ø®Ø´ Main Ùˆ Gradio
# ============================================
# Ù‚Ø¨Ù„Ø§Ù‹ Ø§ÛŒÙ†Ø¬Ø§ if __name__ == "__main__" Ø¯Ø§Ø´ØªÛŒÙ… Ú©Ù‡ gradio Ø±Ø§ Ù„Ø§Ù†Ú† Ù…ÛŒâ€ŒÚ©Ø±Ø¯.
# Ø§Ù„Ø§Ù† Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ÙÙ‚Ø· ØªÙˆØ§Ø¨Ø¹ Ø¨Ø§Ù„Ø§ Ø±Ø§ "ØªØ¹Ø±ÛŒÙ" Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ server.py Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ "ØµØ¯Ø§" Ù…ÛŒâ€ŒØ²Ù†Ø¯.