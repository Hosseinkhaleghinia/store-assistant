"""
Store Assistant RAG Agent - Core Logic
Optimized for reduced token consumption and loop prevention.
"""

import os
import base64
from typing import Literal, Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
    trim_messages,
    BaseMessage,
)
from langchain_core.tools import tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field

try:
    from config import *
    from tts_handler import text_to_speech
except ImportError:
    from src.config import *
    from src.tts_handler import text_to_speech


# ============================================
# Global Variables
# ============================================
products_tool = None
articles_tool = None


# ============================================
# State Definition
# ============================================
class AgentState(MessagesState):
    """State with audio input/output support and retry counter"""
    audio_path: Optional[str] = None
    audio_output_path: Optional[str] = None
    enable_tts: bool = False
    retry_count: int = 0


# ============================================
# Helper Functions
# ============================================
def _extract_store_context(store_name: str) -> str:
    """
    Extract store type/context from store name.
    Examples:
        "Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø§Ø³ØªÙ‚Ù„Ø§Ù„" -> "mobile phone and electronics"
        "Ù„Ø¨Ø§Ø³ Ù¾Ø±Ø³Ù¾ÙˆÙ„ÛŒØ³" -> "clothing and fashion"
        "Ú©ØªØ§Ø¨ Ø¢Ø²Ø§Ø¯ÛŒ" -> "book"
    """
    store_lower = store_name.lower()
    
    # Define keywords for different store types
    mobile_keywords = ["Ù…ÙˆØ¨Ø§ÛŒÙ„", "mobile", "Ú¯ÙˆØ´ÛŒ", "phone", "Ù„Ù¾ØªØ§Ù¾", "laptop", "ØªØ¨Ù„Øª", "tablet"]
    clothing_keywords = ["Ù„Ø¨Ø§Ø³", "Ù¾ÙˆØ´Ø§Ú©", "clothing", "fashion", "Ù…Ø¯"]
    book_keywords = ["Ú©ØªØ§Ø¨", "book", "Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡"]
    electronics_keywords = ["Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©", "electronic", "Ø¯ÛŒØ¬ÛŒØªØ§Ù„", "digital"]
    
    # Check for mobile/electronics store
    if any(keyword in store_lower for keyword in mobile_keywords):
        return "mobile phone, laptop, tablet and electronics"
    
    # Check for clothing store
    if any(keyword in store_lower for keyword in clothing_keywords):
        return "clothing and fashion"
    
    # Check for book store
    if any(keyword in store_lower for keyword in book_keywords):
        return "book and publication"
    
    # Check for general electronics
    if any(keyword in store_lower for keyword in electronics_keywords):
        return "electronics and technology"
    
    # Default: try to use the store name itself as context
    return f"{store_name} products"


def get_trimmed_history(messages: list[BaseMessage], max_tokens=2000):
    """
    Aggressively trim message history to save costs.
    Keeps only system prompt + last few messages.
    """
    return trim_messages(
        messages,
        max_tokens=max_tokens,
        strategy="last",
        token_counter=len,
        include_system=True,
        start_on="human",
    )


def custom_router(state):
    """
    Custom routing function that:
    1. Checks if tools are needed (via tools_condition)
    2. Routes to audio output if TTS is enabled
    3. Otherwise ends the conversation
    """
    decision = tools_condition(state)

    if decision == "tools":
        return "retrieve"

    if state.get("enable_tts", False):
        return "audio_output"

    return END


def route_after_answer(state):
    """Route to audio output if TTS is enabled, otherwise end"""
    if state.get("enable_tts", False):
        return "audio_output"
    return END


# ============================================
# Vector Store Initialization
# ============================================
def load_vector_stores():
    """Load and initialize Chroma vector stores for products and articles"""
    log_step("LOAD", "Loading vector stores...")
    
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL, 
        api_key=API_KEY, 
        base_url=OPENAI_BASE_URL
    )
    
    products_store = Chroma(
        collection_name=PRODUCTS_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(PRODUCTS_CHROMA_DIR),
    )
    
    articles_store = Chroma(
        collection_name=ARTICLES_COLLECTION,
        embedding_function=embeddings,
        persist_directory=str(ARTICLES_CHROMA_DIR),
    )
    
    log_success("Vector stores loaded successfully")
    return products_store, articles_store


# ============================================
# Retriever Tools
# ============================================
def create_retriever_tools(products_store, articles_store):
    """
    Create retriever tools for products and articles.
    k=2 for lower token consumption (previously k=3)
    """
    products_retriever = products_store.as_retriever(search_kwargs={"k": 2})
    articles_retriever = articles_store.as_retriever(search_kwargs={"k": 2})

    @tool
    def products_retriever_tool(query: str):
        """Search products database (mobile, laptop, etc.). Returns price and availability."""
        return products_retriever.invoke(query)

    @tool
    def articles_retriever_tool(query: str):
        """Search articles and buying guides."""
        return articles_retriever.invoke(query)

    products_retriever_tool.name = "products_retriever"
    articles_retriever_tool.name = "articles_retriever"

    return products_retriever_tool, articles_retriever_tool


# ============================================
# Language Models
# ============================================
class GradeDocuments(BaseModel):
    """Schema for document grading"""
    binary_score: str = Field(description="'yes' or 'no'")


def gpt_4o_mini():
    """Initialize GPT-4o-mini model"""
    return ChatOpenAI(
        model=CHAT_GPT_MODEL, 
        api_key=API_KEY, 
        base_url=OPENAI_BASE_URL, 
        temperature=0
    )


def gemini_2_flash():
    """Initialize Gemini 2 Flash model"""
    return ChatGoogleGenerativeAI(
        model=CHAT_GEMINI_MODEL,
        google_api_key=API_KEY,
        transport="rest",
        client_options={"api_endpoint": GOOGLE_BASE_URL},
        temperature=0.7,
    )


# ============================================
# Audio Processing
# ============================================
def transcribe_audio_file(file_path: str) -> str:
    """
    Transcribe audio file using Gemini with vision/audio capabilities.
    Supports: mp3, ogg, wav, webm
    """
    if not file_path or not os.path.exists(file_path):
        return ""
    
    try:
        llm = gemini_2_flash()
        
        # Determine MIME type
        mime_type = "audio/mp3"
        if file_path.endswith(".ogg"):
            mime_type = "audio/ogg"
        elif file_path.endswith(".wav"):
            mime_type = "audio/wav"
        elif file_path.endswith(".webm"):
            mime_type = "audio/webm"

        # Read and encode audio
        with open(file_path, "rb") as audio_file:
            audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

        # Concise prompt to reduce input tokens
        strict_prompt = "Transcribe this audio to text only. No additional explanation."

        message = HumanMessage(
            content=[
                {"type": "text", "text": strict_prompt},
                {"type": "media", "mime_type": mime_type, "data": audio_b64},
            ]
        )
        
        logger.info(f"{Colors.CYAN}ğŸ¤ Transcribing audio...{Colors.END}")
        response = llm.invoke([message])
        return response.content.strip()
        
    except Exception as e:
        log_error(f"Audio transcription error: {e}")
        return ""


def check_audio_input(state: AgentState):
    """
    Check for audio input and transcribe if present.
    First node in the graph.
    """
    audio_path = state.get("audio_path")
    
    if audio_path and os.path.exists(audio_path):
        transcribed_text = transcribe_audio_file(audio_path)
        
        if transcribed_text:
            return {
                "messages": [HumanMessage(content=transcribed_text)],
                "audio_path": None,
                "audio_output_path": None,
            }
        else:
            return {
                "messages": [HumanMessage(content="Sorry, audio was unclear.")],
                "audio_path": None,
                "audio_output_path": None, 
            }
    
    return {}


# ============================================
# Agent Nodes
# ============================================
def generate_query_or_respond(state: AgentState):
    """
    Main decision node: Determine whether to search or respond directly.
    Uses tools (products/articles retrievers) if needed.
    """
    log_step("QUERY", "Analyzing request...")

    # Check for user message
    has_user = any(isinstance(msg, HumanMessage) for msg in state["messages"])
    if not has_user:
        return {"messages": [AIMessage(content="No message received.")]}

    llm = gpt_4o_mini()

    # Extract store context from name (e.g., "Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø§Ø³ØªÙ‚Ù„Ø§Ù„" -> mobile store)
    store_context = _extract_store_context(STORE_NAME)

    # Enhanced system prompt with store context
    system_prompt = f"""You are an assistant for "{STORE_NAME}" - {store_context}.

IMPORTANT: Your store name is "{STORE_NAME}" and you should freely share this name when customers ask about it. This is public information and there's no reason to hide it.

Your role:
- Answer questions about our {store_context} products, prices, and availability
- Use products_retriever for product searches
- Use articles_retriever for guides and articles
- Always introduce yourself with the store name when appropriate

Rules:
1. ONLY use information from the retrieval tools for specific product details
2. If information is not in the tools, say "We don't have that information currently"
3. Never make up prices, availability, or product details
4. For general questions about {store_context} or the store itself, answer directly
5. Always maintain context that you work for "{STORE_NAME}" - a {store_context} store"""

    # Aggressive history trimming (only last 4-5 messages)
    trimmed_msgs = get_trimmed_history(state["messages"], max_tokens=2000)
    messages = [SystemMessage(content=system_prompt)] + trimmed_msgs

    # Prevent infinite loops: disable tools after 2 retries
    if state.get("retry_count", 0) >= 2:
        log_warning("Retry limit reached. Direct response without tools.")
        response = llm.invoke(messages)
    else:
        if products_tool and articles_tool:
            response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)
        else:
            response = llm.invoke(messages)

    return {"messages": [response]}


def grade_documents(
    state: AgentState,
) -> Literal["generate_answer", "rewrite_question"]:
    """
    Grade document relevance with loop protection.
    After 1 retry, proceed to answer even if documents aren't perfect.
    """
    log_step("GRADE", "Grading documents...")

    # Loop protection: after 1 retry, proceed to answer
    current_retry = state.get("retry_count", 0)
    if current_retry >= 1:
        log_warning(f"Retry {current_retry}: Skipping strict grading.")
        return "generate_answer"

    # Extract tool messages
    tool_msgs = [
        msg for msg in state["messages"] if hasattr(msg, "type") and msg.type == "tool"
    ]
    
    if not tool_msgs:
        return "rewrite_question"

    llm = gpt_4o_mini()
    
    # Find original question
    question = state["messages"][0].content

    # Preview first 1000 chars of context for grading (cost savings)
    context_preview = "\n".join([msg.content[:1000] for msg in tool_msgs])

    grade_prompt = f"""Question: {question}
Context: {context_preview}
Are these documents relevant to the question? (yes/no)"""

    response = llm.invoke([{"role": "user", "content": grade_prompt}])

    if "yes" in response.content.lower():
        return "generate_answer"
    else:
        return "rewrite_question"


def rewrite_question(state: AgentState):
    """
    Rewrite question for better retrieval.
    Increments retry counter to prevent loops.
    """
    log_step("REWRITE", "Rewriting query...")

    # Increment retry counter
    new_count = state.get("retry_count", 0) + 1

    llm = gpt_4o_mini()

    # Find last human message (original question)
    messages = state["messages"]
    last_human_message = next(
        (m for m in reversed(messages) if isinstance(m, HumanMessage)), None
    )

    if last_human_message:
        original_q = last_human_message.content
    else:
        original_q = messages[-1].content

    logger.info(f"Original Question: {original_q}")

    # Concise rewrite prompt
    msg = (
        f"Improve this question for better product database search. "
        f"Write only the improved question, no explanation.\n"
        f"Original: {original_q}"
    )

    response = llm.invoke(msg)

    logger.info(
        f"{Colors.GREEN}Rewritten question ({new_count}): {response.content}{Colors.END}"
    )

    return {
        "messages": [HumanMessage(content=response.content)],
        "retry_count": new_count,
    }


def generate_answer(state: AgentState):
    """
    Generate final answer with limited context to reduce token usage.
    Resets retry counter for next user query.
    """
    log_step("ANSWER", "Generating response...")
    
    llm = gpt_4o_mini()

    # Extract original user question
    question = "User question"
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            question = msg.content
            break

    # Collect and limit context (prevent token explosion)
    tool_contents = []
    for msg in state["messages"]:
        if hasattr(msg, "type") and msg.type == "tool":
            # Only take first 800 chars of each document
            tool_contents.append(msg.content[:800])

    # Limit total context to 3000 chars
    full_context = "\n\n".join(tool_contents)[:3000]

    logger.info(
        f"{Colors.CYAN}Final context length: {len(full_context)} chars{Colors.END}"
    )

    # Extract store context
    store_context = _extract_store_context(STORE_NAME)

    answer_prompt = f"""You are an assistant for "{STORE_NAME}" - a {store_context} store.

IMPORTANT: You work for "{STORE_NAME}" and should freely share this store name when asked. This is public information.

Question: {question}

Available Information:
{full_context}

Instructions:
1. Answer based ONLY on the information provided above for specific product details
2. If no relevant information is available, say "We don't have that information currently"
3. Be concise, helpful, and accurate
4. Use a conversational and friendly tone
5. You can freely mention that you work for "{STORE_NAME}" - a {store_context} store
6. Use proper punctuation (commas, periods, etc.)"""

    response = llm.invoke([{"role": "user", "content": answer_prompt}])

    # Reset retry counter for next question
    return {"messages": [response], "retry_count": 0}


def generate_audio_output(state: AgentState):
    """
    Audio output node: Convert final response to speech.
    Only runs if enable_tts=True.
    """
    if not state.get("enable_tts", False):
        log_step("TTS", "Audio output disabled")
        return {}

    # Find last AI message
    last_ai_message = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, AIMessage):
            last_ai_message = msg
            break

    if not last_ai_message or not last_ai_message.content:
        log_warning("No message to convert to audio")
        return {}

    log_step("TTS", "ğŸ”Š Generating audio output...")

    # Convert to speech
    audio_path = text_to_speech(
        text=last_ai_message.content,
        model="gemini-2.5-flash-preview-tts",
        add_emotion=True,
    )

    if audio_path:
        return {"audio_output_path": audio_path}

    return {}


# ============================================
# Graph Construction
# ============================================
def create_agent_graph(p_tool, a_tool):
    """
    Construct the LangGraph workflow.
    
    Flow:
    START -> check_audio -> generate_query_or_respond -> [retrieve OR audio_output OR END]
    retrieve -> grade_documents -> [generate_answer OR rewrite_question]
    generate_answer -> [audio_output OR END]
    rewrite_question -> generate_query_or_respond
    """
    global products_tool, articles_tool
    products_tool = p_tool
    articles_tool = a_tool

    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("check_audio", check_audio_input)
    workflow.add_node("generate_query_or_respond", generate_query_or_respond)
    workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
    workflow.add_node("rewrite_question", rewrite_question)
    workflow.add_node("generate_answer", generate_answer)
    workflow.add_node("audio_output", generate_audio_output)

    # Add edges
    workflow.add_edge(START, "check_audio")
    workflow.add_edge("check_audio", "generate_query_or_respond")

    # Custom router after query generation
    workflow.add_conditional_edges(
        "generate_query_or_respond",
        custom_router,
        {"retrieve": "retrieve", "audio_output": "audio_output", END: END},
    )

    workflow.add_conditional_edges("retrieve", grade_documents)

    # Conditional routing after answer
    workflow.add_conditional_edges(
        "generate_answer",
        route_after_answer,
        {"audio_output": "audio_output", END: END},
    )

    workflow.add_edge("audio_output", END)
    workflow.add_edge("rewrite_question", "generate_query_or_respond")

    # Compile with memory
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)


# """
# Store Assistant RAG Agent - Logic Core (Optimized)
# Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù ØªÙˆÚ©Ù† Ùˆ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ù„Ù‚Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒâ€ŒÙ¾Ø§ÛŒØ§Ù†.
# """

# import os
# import base64
# from typing import Literal, Optional
# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_chroma import Chroma
# from langchain_core.messages import (
#     SystemMessage,
#     HumanMessage,
#     AIMessage,
#     trim_messages,
#     BaseMessage,
# )
# from langchain_core.tools import tool
# from langgraph.graph import StateGraph, MessagesState, START, END
# from langgraph.prebuilt import ToolNode, tools_condition
# from langgraph.checkpoint.memory import MemorySaver
# from pydantic import BaseModel, Field

# try:
#     from config import *
#     from tts_handler import text_to_speech  # ğŸ†• Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†

# except ImportError:
#     from src.config import *
#     from src.tts_handler import text_to_speech  # ğŸ†• Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†


# # ============================================
# # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ
# # ============================================
# products_tool = None
# articles_tool = None


# # ============================================
# # ØªØ¹Ø±ÛŒÙ State (Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡)
# # ============================================
# class AgentState(MessagesState):
#     """State Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¯Ø±ÛŒØ§ÙØª ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ùˆ Ø´Ù…Ø§Ø±Ø´ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§"""

#     audio_path: Optional[str] = None
#     audio_output_path: Optional[str] = None  # ğŸ†• Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ ØµÙˆØªÛŒ
#     enable_tts: bool = False  # ğŸ†• Ú©Ù†ØªØ±Ù„ ÙØ¹Ø§Ù„/ØºÛŒØ±ÙØ¹Ø§Ù„
#     retry_count: int = 0


# # ============================================
# # ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ (Helper Functions)
# # ============================================
# def get_trimmed_history(messages: list[BaseMessage], max_tokens=2000):
#     """
#     ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø±Ø§ Ø¨Ù‡ Ø´Ø¯Øª Ú©ÙˆØªØ§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¯Ø± Ù‡Ø²ÛŒÙ†Ù‡ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø´ÙˆØ¯.
#     ÙÙ‚Ø· Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø§Ù…Ù¾Øª + Ú†Ù†Ø¯ Ù¾ÛŒØ§Ù… Ø¢Ø®Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±Ø¯.
#     """
#     return trim_messages(
#         messages,
#         max_tokens=max_tokens,
#         strategy="last",
#         token_counter=len,  # Ø´Ù…Ø§Ø±Ø´ Ø­Ø¯ÙˆØ¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…
#         include_system=True,
#         start_on="human",
#     )


# def custom_router(state):
#     # Û±. ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ tools_condition (Ø·Ø¨Ù‚ Ø®ÙˆØ§Ø³ØªÙ‡ Ø´Ù…Ø§)
#     # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¢ÛŒØ§ Ù…Ø¯Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø¨Ø²Ø§Ø± (Tool Call) Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
#     decision = tools_condition(state)

#     # Û². Ø§Ú¯Ø± ØªØµÙ…ÛŒÙ… "tools" Ø¨ÙˆØ¯ØŒ ÛŒØ¹Ù†ÛŒ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ù†ÙˆØ¯ retrieve Ø¨Ø±ÙˆÛŒÙ…
#     if decision == "tools":
#         return "retrieve"

#     # Û³. Ø§Ú¯Ø± ØªØµÙ…ÛŒÙ… END Ø¨ÙˆØ¯ (ÛŒØ¹Ù†ÛŒ Ù¾Ø§Ø³Ø® Ù…ØªÙ†ÛŒ Ø§Ø³Øª)ØŒ Ø­Ø§Ù„Ø§ Ø´Ø±Ø· TTS Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
#     if state.get("enable_tts", False):
#         return "audio_output"

#     # Û´. Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª Ù¾Ø§ÛŒØ§Ù†
#     return END


# def route_after_answer(state):
#     if state.get("enable_tts", False):
#         return "audio_output"
#     return END


# # ============================================
# # Ø¨Ø®Ø´ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores
# # ============================================
# def load_vector_stores():
#     log_step("LOAD", "Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Vector Stores...")
#     embeddings = OpenAIEmbeddings(
#         model=EMBEDDING_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL
#     )
#     products_store = Chroma(
#         collection_name=PRODUCTS_COLLECTION,
#         embedding_function=embeddings,
#         persist_directory=str(PRODUCTS_CHROMA_DIR),
#     )
#     articles_store = Chroma(
#         collection_name=ARTICLES_COLLECTION,
#         embedding_function=embeddings,
#         persist_directory=str(ARTICLES_CHROMA_DIR),
#     )
#     log_success("Vector stores Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
#     return products_store, articles_store


# # ============================================
# # Ø¨Ø®Ø´ 2: Ø³Ø§Ø®Øª Retriever Tools
# # ============================================
# def create_retriever_tools(products_store, articles_store):
#     # k=2 Ú©Ø±Ø¯ÛŒÙ… Ú©Ù‡ ØªÙˆÚ©Ù† Ú©Ù…ØªØ±ÛŒ Ù…ØµØ±Ù Ø¨Ø´Ù‡ (Ù‚Ø¨Ù„Ø§ 3 Ø¨ÙˆØ¯)
#     products_retriever = products_store.as_retriever(search_kwargs={"k": 2})
#     articles_retriever = articles_store.as_retriever(search_kwargs={"k": 2})

#     @tool
#     def products_retriever_tool(query: str):
#         """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª (Ù…ÙˆØ¨Ø§ÛŒÙ„ØŒ Ù„Ù¾ØªØ§Ù¾ Ùˆ...). Ù‚ÛŒÙ…Øª Ùˆ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
#         return products_retriever.invoke(query)

#     @tool
#     def articles_retriever_tool(query: str):
#         """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø®Ø±ÛŒØ¯."""
#         return articles_retriever.invoke(query)

#     products_retriever_tool.name = "products_retriever"
#     articles_retriever_tool.name = "articles_retriever"

#     return products_retriever_tool, articles_retriever_tool


# # ============================================
# # Ø¨Ø®Ø´ 3: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ
# # ============================================
# class GradeDocuments(BaseModel):
#     binary_score: str = Field(description="'yes' or 'no'")


# def gpt_4o_mini():
#     return ChatOpenAI(
#         model=CHAT_GPT_MODEL, api_key=API_KEY, base_url=OPENAI_BASE_URL, temperature=0
#     )


# def gemini_2_flash():
#     return ChatGoogleGenerativeAI(
#         model=CHAT_GEMINI_MODEL,
#         google_api_key=API_KEY,
#         transport="rest",
#         client_options={"api_endpoint": GOOGLE_BASE_URL},
#         temperature=0.7,
#     )


# # ============================================
# # Ø¨Ø®Ø´ 4: Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª
# # ============================================
# def transcribe_audio_file(file_path: str) -> str:
#     if not file_path or not os.path.exists(file_path):
#         return ""
#     try:
#         llm = gemini_2_flash()
#         mime_type = "audio/mp3"
#         if file_path.endswith(".ogg"):
#             mime_type = "audio/ogg"
#         elif file_path.endswith(".wav"):
#             mime_type = "audio/wav"
#         elif file_path.endswith(".webm"):
#             mime_type = "audio/webm"

#         with open(file_path, "rb") as audio_file:
#             audio_b64 = base64.b64encode(audio_file.read()).decode("utf-8")

#         # Ù¾Ø±Ø§Ù…Ù¾Øª Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ ØªÙˆÚ©Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¬Ù…ÛŒÙ†Ø§ÛŒ
#         strict_prompt = (
#             "ÙÙ‚Ø· Ù…ØªÙ† Ø§ÛŒÙ† ØµÙˆØª Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ (Transcription). Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÙ‡."
#         )

#         message = HumanMessage(
#             content=[
#                 {"type": "text", "text": strict_prompt},
#                 {"type": "media", "mime_type": mime_type, "data": audio_b64},
#             ]
#         )
#         logger.info(f"{Colors.CYAN}ğŸ¤ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§...{Colors.END}")
#         response = llm.invoke([message])
#         return response.content.strip()
#     except Exception as e:
#         log_error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§: {e}")
#         return ""


# def check_audio_input(state: AgentState):
#     audio_path = state.get("audio_path")
#     if audio_path and os.path.exists(audio_path):
#         transcribed_text = transcribe_audio_file(audio_path)
#         if transcribed_text:
#             return {
#                 "messages": [HumanMessage(content=transcribed_text)],
#                 "audio_path": None,
#                 "audio_output_path": None,
#             }
#         else:
#             return {
#                 "messages": [HumanMessage(content="Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ ØµØ¯Ø§ ÙˆØ§Ø¶Ø­ Ù†Ø¨ÙˆØ¯.")],
#                 "audio_path": None,
#                 "audio_output_path": None, 
#             }
#     return {}


# # ============================================
# # Ø¨Ø®Ø´ 5: Agent Nodes (Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡)
# # ============================================


# def generate_query_or_respond(state: AgentState):
#     """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ: Ø¬Ø³ØªØ¬Ùˆ ÛŒØ§ Ù¾Ø§Ø³Ø®"""
#     log_step("QUERY", "ØªØ­Ù„ÛŒÙ„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª...")

#     # Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ù‡Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±
#     # (Ø§Ú¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ù…Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§Ø´Ù‡ØŒ ÛŒØ¹Ù†ÛŒ Ø´Ø±ÙˆØ¹ Ø³ÛŒÚ©Ù„ Ø¬Ø¯ÛŒØ¯Ù‡)
#     if isinstance(state["messages"][-1], HumanMessage):
#         # Ø§Ù…Ø§ Ú†ÙˆÙ† State Ø§ÛŒÙ…ÛŒÙˆØªØ¨Ù„ Ù†ÛŒØ³ØªØŒ Ø§ÛŒÙ†Ø¬Ø§ ÙÙ‚Ø· Ù¾Ø§Ø³ Ù…ÛŒØ¯ÛŒÙ…ØŒ Ø±ÛŒØ³Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ÛŒØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ØªØ± Ø¨Ø§Ø´Ù‡
#         # ÙØ¹Ù„Ø§ ÙØ±Ø¶ Ù…ÛŒÚ©Ù†ÛŒÙ… Ø§Ú¯Ø± human message Ø¯ÛŒØ¯ÛŒÙ… ÛŒØ¹Ù†ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø­Ø±Ù Ø²Ø¯Ù‡
#         pass

#     has_user = any(isinstance(msg, HumanMessage) for msg in state["messages"])
#     if not has_user:
#         return {"messages": [AIMessage(content="Ù¾ÛŒØ§Ù…ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")]}

#     llm = gpt_4o_mini()

#     # Ù¾Ø±Ø§Ù…Ù¾Øª ÙØ´Ø±Ø¯Ù‡â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ ØªÙˆÚ©Ù†
#     system_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡ {STORE_NAME} Ù‡Ø³ØªÛŒ.
# ÙˆØ¸Ø§ÛŒÙ: Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø­ØµÙˆÙ„Ø§ØªØŒ Ù‚ÛŒÙ…Øª Ùˆ Ù…ÙˆØ¬ÙˆØ¯ÛŒ.
# Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§: products_retriever, articles_retriever.
# Ù‚ÙˆØ§Ù†ÛŒÙ†:
# 1. ÙÙ‚Ø· Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ú¯ÛŒØ±.
# 2. Ø§Ú¯Ø± Ø¯Ø± Ø§Ø¨Ø²Ø§Ø± Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ú¯Ùˆ "Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±ÛŒÙ…" (Ø¯Ø±ÙˆØº Ù†Ú¯Ùˆ).
# 3. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø¹Ù…ÙˆÙ…ÛŒ Ø¨ÙˆØ¯ØŒ Ø®ÙˆØ¯Øª Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡."""

#     # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø´Ø¯ÛŒØ¯ Ø±ÙˆÛŒ ØªØ§Ø±ÛŒØ®Ú†Ù‡ (ÙÙ‚Ø· 4-5 Ù¾ÛŒØ§Ù… Ø¢Ø®Ø±)
#     trimmed_msgs = get_trimmed_history(state["messages"], max_tokens=2000)
#     messages = [SystemMessage(content=system_prompt)] + trimmed_msgs

#     # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø²ÛŒØ§Ø¯ Ø´Ø¯Ù‡ØŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒØ¨Ù†Ø¯ÛŒÙ… Ú©Ù‡ Ø¯ÛŒÚ¯Ù‡ Ø³Ø±Ú† Ù†Ú©Ù†Ù‡
#     if state.get("retry_count", 0) >= 2:
#         log_warning("ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´ Ø²ÛŒØ§Ø¯ Ø´Ø¯. Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø²Ø§Ø±.")
#         response = llm.invoke(messages)  # Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø²Ø§Ø±
#     else:
#         if products_tool and articles_tool:
#             response = llm.bind_tools([products_tool, articles_tool]).invoke(messages)
#         else:
#             response = llm.invoke(messages)

#     return {"messages": [response]}


# def grade_documents(
#     state: AgentState,
# ) -> Literal["generate_answer", "rewrite_question"]:
#     """Ú©ÛŒÙÛŒØª Ø³Ù†Ø¬ÛŒ Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø­Ù„Ù‚Ù‡"""
#     log_step("GRADE", "Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ø§Ø±Ú©...")

#     # 1. Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø¨ÛŒØ´ØªØ± Ø§Ø² 1 Ø¨Ø§Ø± Ø´Ø¯Ù‡ØŒ Ø¯ÛŒÚ¯Ù‡ Ø³Ø®Øª Ù†Ú¯ÛŒØ± Ùˆ Ø¨Ø±Ùˆ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡
#     # (Ø­ØªÛŒ Ø§Ú¯Ø± Ù…Ø¯Ø§Ø±Ú© Ø¹Ø§Ù„ÛŒ Ù†ÛŒØ³ØªØŒ Ø¨Ù‡ØªØ± Ø§Ø² Ù‡ÛŒÚ†ÛŒÙ‡ ÛŒØ§ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ú¯Ù‡ Ù†Ø¯Ø§Ø±Ù…)
#     current_retry = state.get("retry_count", 0)
#     if current_retry >= 1:
#         log_warning(f"ØªÙ„Ø§Ø´ {current_retry}: Ø¹Ø¨ÙˆØ± Ø§Ø² Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±ÛŒ.")
#         return "generate_answer"

#     tool_msgs = [
#         msg for msg in state["messages"] if hasattr(msg, "type") and msg.type == "tool"
#     ]
#     if not tool_msgs:
#         return "rewrite_question"

#     llm = gpt_4o_mini()
#     question = state["messages"][0].content

#     # ÙÙ‚Ø· 1000 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„ Ù…Ø¯Ø§Ø±Ú© Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ú†Ú© Ú©Ø±Ø¯Ù† Ø¨ÙØ±Ø³Øª (ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ)
#     context_preview = "\n".join([msg.content[:1000] for msg in tool_msgs])

#     grade_prompt = f"""Ø³ÙˆØ§Ù„: {question}
# Ù…Ø¯Ø§Ø±Ú©: {context_preview}
# Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…Ø¯Ø§Ø±Ú© Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ø±Ø¨Ø· Ø¯Ø§Ø±Ù†Ø¯ØŸ (yes/no)"""

#     response = llm.invoke([{"role": "user", "content": grade_prompt}])

#     if "yes" in response.content.lower():
#         return "generate_answer"
#     else:
#         return "rewrite_question"


# def rewrite_question(state: AgentState):
#     """Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ (Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡) Ø¨Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±"""
#     log_step("REWRITE", "ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯...")

#     # Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡
#     new_count = state.get("retry_count", 0) + 1

#     llm = gpt_4o_mini()

#     # --- [Ø§ØµÙ„Ø§Ø­ Ù…Ù‡Ù…]: Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± ---
#     # Ù„ÛŒØ³Øª Ø±Ø§ Ù…Ø¹Ú©ÙˆØ³ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ùˆ Ø§ÙˆÙ„ÛŒÙ† Ù¾ÛŒØ§Ù…ÛŒ Ú©Ù‡ Ø§Ø² Ù†ÙˆØ¹ HumanMessage Ø¨Ø§Ø´Ø¯ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
#     messages = state["messages"]
#     last_human_message = next(
#         (m for m in reversed(messages) if isinstance(m, HumanMessage)), None
#     )

#     if last_human_message:
#         original_q = last_human_message.content
#     else:
#         # Ø§Ú¯Ø± Ø¨Ù‡ Ù‡Ø± Ø¯Ù„ÛŒÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ (Ú©Ù‡ Ø¨Ø¹ÛŒØ¯ Ø§Ø³Øª)ØŒ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ù„ÛŒØ³Øª Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±
#         original_q = messages[-1].content

#     # Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø³ÙˆØ§Ù„ Ø¯Ø±Ø³Øª Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
#     logger.info(f"Original Question Found: {original_q}")

#     # Ù¾Ø±Ø§Ù…Ù¾Øª Ø±Ø§ Ú©Ù…ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¨Ø¯Ø§Ù†Ø¯ Ù‡Ø¯Ù Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ± Ø§Ø³Øª
#     msg = (
#         f"Ø³ÙˆØ§Ù„ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø¯Ù‡ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†. "
#         f"ÙÙ‚Ø· Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø¶Ø§ÙÙ‡.\n"
#         f"Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ: {original_q}"
#     )

#     response = llm.invoke(msg)

#     logger.info(
#         f"{Colors.GREEN}Ø³ÙˆØ§Ù„ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ ({new_count}): {response.content}{Colors.END}"
#     )

#     return {
#         # Ø§ÛŒÙ† Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù„ÛŒØ³Øª Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§ÛŒØ¬Ù†Øª ÙÚ©Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¯Ø³ØªÙˆØ± Ø¬Ø¯ÛŒØ¯ÛŒ Ø§Ø³Øª
#         "messages": [HumanMessage(content=response.content)],
#         "retry_count": new_count,
#     }


# def generate_answer(state: AgentState):
#     """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ú©Ø§Ù†ØªÚ©Ø³Øª Ù…Ø­Ø¯ÙˆØ¯"""
#     log_step("ANSWER", "ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®...")
#     llm = gpt_4o_mini()

#     # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ (Ù†Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡â€ŒÙ‡Ø§)
#     # Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø§ÙˆÙ„ÛŒÙ† HumanMessage Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒÙ‡ØŒ ÛŒØ§ Ø¢Ø®Ø±ÛŒÙ† Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¨Ø²Ø§Ø±
#     question = "Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±"
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, HumanMessage):
#             question = msg.content
#             break

#     # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ù…Ø­Ø¯ÙˆØ¯Ø³Ø§Ø²ÛŒ Ù…Ø¯Ø§Ø±Ú©
#     tool_contents = []
#     for msg in state["messages"]:
#         if hasattr(msg, "type") and msg.type == "tool":
#             # ÙÙ‚Ø· 500 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø² Ù‡Ø± Ù…Ø¯Ø±Ú© Ø±Ùˆ Ø¨Ø±Ø¯Ø§Ø± (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ù†ÙØ¬Ø§Ø± ØªÙˆÚ©Ù†)
#             # Ø§Ú¯Ø± Ù…Ø­ØµÙˆÙ„Ù‡ØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù… Ø§ÙˆÙ„Ø´Ù‡.
#             tool_contents.append(msg.content[:800])

#     # Ú©Ù„ Ú©Ø§Ù†ØªÚ©Ø³Øª Ø±Ùˆ Ù‡Ù… Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù† Ø¨Ù‡ 3000 Ú©Ø§Ø±Ø§Ú©ØªØ±
#     full_context = "\n\n".join(tool_contents)[:3000]

#     logger.info(
#         f"{Colors.CYAN}Ø·ÙˆÙ„ Ú©Ø§Ù†ØªÚ©Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ: {len(full_context)} Ú©Ø§Ø±Ø§Ú©ØªØ±{Colors.END}"
#     )

#     answer_prompt = f"""ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± {STORE_NAME} Ù‡Ø³ØªÛŒ.
# Ø³ÙˆØ§Ù„: {question}
# Ø§Ø·Ù„Ø§Ø¹Ø§Øª:
# {full_context}

# Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„:
# 1. ÙÙ‚Ø· Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ù„Ø§ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡.
# 2. Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†ÛŒØ³ØªØŒ Ø¨Ú¯Ùˆ "Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†Ø¯Ø§Ø±Ù…".
# 3. Ø®Ù„Ø§ØµÙ‡ Ùˆ Ù…ÙÛŒØ¯ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡.
# 4. Ø¨Ø§ Ù„Ø­Ù† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡ Ùˆ Ø³Ø¹ÛŒ Ú©Ù† Ø§Ø² (ØŒ) Ùˆ (.) Ùˆ Ø¹Ù„Ø§Ø¦Ù… Ø¯ÛŒÚ¯Ù‡ Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ """

#     response = llm.invoke([{"role": "user", "content": answer_prompt}])

#     # Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ø³Ø® Ø¯Ø§Ø¯Ù†ØŒ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ø±Ùˆ ØµÙØ± Ú©Ù† Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø¹Ø¯ÛŒ
#     return {"messages": [response], "retry_count": 0}


# def generate_audio_output(state: AgentState):
#     """
#     Ù†ÙˆØ¯ Ø®Ø±ÙˆØ¬ÛŒ: ØªØ¨Ø¯ÛŒÙ„ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ ØµÙˆØª
#     ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ enable_tts=True Ø¨Ø§Ø´Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒØ´Ù‡
#     """

#     # Ú†Ú© Ú©Ø±Ø¯Ù† ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù† TTS
#     if not state.get("enable_tts", False):
#         log_step("TTS", "Ø®Ø±ÙˆØ¬ÛŒ ØµÙˆØªÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª")
#         return {}

#     # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ù¾Ø§Ø³Ø® AI
#     last_ai_message = None
#     for msg in reversed(state["messages"]):
#         if isinstance(msg, AIMessage):
#             last_ai_message = msg
#             break

#     if not last_ai_message or not last_ai_message.content:
#         log_warning("Ù¾ÛŒØ§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØµÙˆØª ÛŒØ§ÙØª Ù†Ø´Ø¯")
#         return {}

#     log_step("TTS", "ğŸ”Š Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ ØµÙˆØªÛŒ...")

#     # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØµÙˆØª
#     audio_path = text_to_speech(
#         text=last_ai_message.content,
#         model="gemini-2.5-flash-preview-tts",
#         add_emotion=True,  # Ù„Ø­Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡
#     )

#     if audio_path:
#         return {"audio_output_path": audio_path}

#     return {}


# # ============================================
# # Ø¨Ø®Ø´ 6: Ø³Ø§Ø®Øª Graph
# # ============================================
# def create_agent_graph(p_tool, a_tool):
#     global products_tool, articles_tool
#     products_tool = p_tool
#     articles_tool = a_tool

#     workflow = StateGraph(AgentState)

#     # --- ØªØ¹Ø±ÛŒÙ Ù†ÙˆØ¯Ù‡Ø§ (ØªØºÛŒÛŒØ±ÛŒ Ù†Ú©Ø±Ø¯Ù†Ø¯) ---
#     workflow.add_node("check_audio", check_audio_input)
#     workflow.add_node("generate_query_or_respond", generate_query_or_respond)
#     workflow.add_node("retrieve", ToolNode([products_tool, articles_tool]))
#     workflow.add_node("rewrite_question", rewrite_question)
#     workflow.add_node("generate_answer", generate_answer)
#     workflow.add_node("audio_output", generate_audio_output)

#     # --- ØªØ¹Ø±ÛŒÙ ÛŒØ§Ù„â€ŒÙ‡Ø§ ---
#     workflow.add_edge(START, "check_audio")
#     workflow.add_edge("check_audio", "generate_query_or_respond")

#     # [Ø¨Ø®Ø´ Ù…Ù‡Ù… Û±] Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ ÙˆØ§Ø³Ø· Ú©Ù‡ tools_condition Ø±Ø§ Ø¯Ø± Ø¯Ù„ Ø®ÙˆØ¯ Ø¯Ø§Ø±Ø¯
#     workflow.add_conditional_edges(
#         "generate_query_or_respond",
#         custom_router,
#         # ØªØ¹ÛŒÛŒÙ† Ù…Ù‚ØµØ¯Ù‡Ø§ÛŒ Ù…Ù…Ú©Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø®Ø±ÙˆØ¬ÛŒ ØªØ§Ø¨Ø¹ custom_router
#         {"retrieve": "retrieve", "audio_output": "audio_output", END: END},
#     )

#     workflow.add_conditional_edges("retrieve", grade_documents)

#     # [Ø¨Ø®Ø´ Ù…Ù‡Ù… Û²] Ø´Ø±Ø·ÛŒ Ú©Ø±Ø¯Ù† Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² generate_answer
#     workflow.add_conditional_edges(
#         "generate_answer",
#         route_after_answer,
#         {"audio_output": "audio_output", END: END},
#     )

#     workflow.add_edge("audio_output", END)
#     workflow.add_edge("rewrite_question", "generate_query_or_respond")

#     memory = MemorySaver()
#     return workflow.compile(checkpointer=memory)
